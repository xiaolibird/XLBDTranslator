"""
Gemini ç¿»è¯‘å®¢æˆ·ç«¯
ä½¿ç”¨ tenacity è¿›è¡Œé‡è¯•ç®¡ç†
"""
import asyncio
import base64
import json
import time
import re
import mimetypes
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor
from urllib import request, error

from google import genai
from google.genai import types
from google.genai import errors as genai_errors
from google.api_core.exceptions import GoogleAPICallError, ResourceExhausted, ServiceUnavailable, ClientError, DeadlineExceeded
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

from ..core.schema import Settings, ContentSegment, TranslationMap, SegmentList
from ..core.exceptions import (
    APIError, APIRateLimitError, APITimeoutError, APIAuthenticationError,
    JSONParseError, TranslationError
)
from .base import BaseTranslator, BaseAsyncTranslator
from .support import CachePersistenceManager, PromptManager
from ..utils.logger import get_logger

logger = get_logger(__name__)

# ========================================================================
# Gemini ç¿»è¯‘å®¢æˆ·ç«¯
# ========================================================================

class GeminiTranslator(BaseTranslator):
    """Gemini ç¿»è¯‘å®¢æˆ·ç«¯ã€‚
    
    ç»§æ‰¿è‡ª BaseTranslatorï¼Œå®ç° Google Gemini çš„å…·ä½“ç¿»è¯‘é€»è¾‘ã€‚
    """

    def __init__(
        self, 
        settings: Settings, 
        cache_manager: Optional[CachePersistenceManager] = None
    ):
        """
        Args:
            settings: å…¨å±€è®¾ç½®å¯¹è±¡ï¼ˆåŒ…å«document_pathç”¨äºè®¡ç®—hashï¼‰
            cache_manager: ç¼“å­˜æŒä¹…åŒ–ç®¡ç†å™¨ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚æœæä¾›åˆ™ä½¿ç”¨ï¼Œå¦åˆ™è‡ªåŠ¨åˆ›å»º
        """
        # è°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°
        super().__init__(settings)
        
        self.generation_config = {}
        self.cache_refs: Dict[str, str] = {}
        self._async_translator = None  # æ‡’åŠ è½½å¼‚æ­¥ç¿»è¯‘å™¨
        self._client: Optional[genai.Client] = None
        self._base_generation_config: Optional[types.GenerateContentConfig] = None
        
        # åˆå§‹åŒ– Prompt ç®¡ç†å™¨
        self.prompt_manager = PromptManager(settings)
        
        # åˆå§‹åŒ–ç¼“å­˜æŒä¹…åŒ–ç®¡ç†å™¨ï¼ˆä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ï¼Œå¦åˆ™æ ¹æ®doc_hashåˆ›å»ºï¼‰
        self.cache_persistence = cache_manager
        if self.cache_persistence is None and settings.processing.enable_gemini_caching and self.doc_hash:
            self.cache_persistence = CachePersistenceManager(settings)

        # é…ç½® API
        self._configure_api()

        # åˆå§‹åŒ–æ¨¡å‹ï¼ˆæ–° SDKï¼šClient + GenerateContentConfigï¼›é€šè¿‡é€‚é…å™¨ä¿ç•™æ—§è°ƒç”¨å½¢æ€ï¼‰
        self.model = self._create_model()
        
        # æ³¨æ„ï¼šåˆå§‹åŒ–æ—¶ä¸åˆ›å»ºç¼“å­˜
        # ç¼“å­˜åˆ›å»ºåˆ†ä¸¤é˜¶æ®µï¼š
        # 1. é¢„ç¿»è¯‘é˜¶æ®µï¼šè°ƒç”¨ create_base_cache() åˆ›å»ºåŸºç¡€ç¼“å­˜ï¼ˆæ—  glossaryã€æ—  modeï¼‰
        # 2. æ­£å¼ç¿»è¯‘é˜¶æ®µï¼šè°ƒç”¨ create_full_cache() åˆ›å»ºå®Œæ•´ç¼“å­˜ï¼ˆå« glossary å’Œ modeï¼‰
        logger.info("ğŸ”§ GeminiTranslator åˆå§‹åŒ–å®Œæˆï¼ˆå»¶è¿Ÿç¼“å­˜åˆ›å»ºï¼‰")
    
    def create_base_cache(self) -> Optional[str]:
        """
        åˆ›å»ºåŸºç¡€ç¼“å­˜ï¼ˆç”¨äºé¢„ç¿»è¯‘é˜¶æ®µï¼‰
        
        åªåŒ…å« system_instruction + text_translation_prompt
        ä¸åŒ…å« glossary å’Œ mode
        
        Returns:
            ç¼“å­˜åç§°ï¼Œå¦‚æœå¤±è´¥è¿”å› None
        """
        if not self.settings.processing.enable_gemini_caching or not self.cache_persistence:
            logger.info("â„¹ï¸ Gemini ç¼“å­˜æœªå¯ç”¨ï¼Œè·³è¿‡åŸºç¡€ç¼“å­˜åˆ›å»º")
            return None
        
        # ç”ŸæˆåŸºç¡€ system instructionï¼ˆæ—  modeã€æ—  glossaryï¼‰
        system_instruction = self.prompt_manager.get_system_instruction(
            use_vision=self.settings.processing.use_vision_mode,
            include_mode=False,
            include_glossary=False
        )
        
        cache_name = self.cache_persistence.get_or_create_system_cache(
            system_instruction=system_instruction,
            model_name=self.settings.api.gemini_model,
            display_name="base_pretranslate"
        )
        
        if cache_name:
            self.cache_refs['base'] = cache_name
            logger.info(f"âœ… åŸºç¡€ç¼“å­˜å·²å°±ç»ªï¼ˆé¢„ç¿»è¯‘ç”¨ï¼‰: {cache_name[:50]}...")
        
        return cache_name
    
    def create_full_cache(self, glossary: Optional[Dict[str, str]] = None) -> Optional[str]:
        """
        åˆ›å»ºå®Œæ•´ç¼“å­˜ï¼ˆç”¨äºæ­£å¼ç¿»è¯‘é˜¶æ®µï¼‰
        
        åŒ…å« system_instruction + text_translation_prompt + mode + glossary
        
        Args:
            glossary: æœ¯è¯­è¡¨å­—å…¸
            
        Returns:
            ç¼“å­˜åç§°ï¼Œå¦‚æœå¤±è´¥è¿”å› None
        """
        if not self.settings.processing.enable_gemini_caching or not self.cache_persistence:
            logger.info("â„¹ï¸ Gemini ç¼“å­˜æœªå¯ç”¨ï¼Œè·³è¿‡å®Œæ•´ç¼“å­˜åˆ›å»º")
            return None
        
        # æ ¼å¼åŒ–æœ¯è¯­è¡¨
        glossary_text = ""
        if glossary:
            glossary_text = "\n".join([
                f"- **{k}**: {v}" 
                for k, v in glossary.items()
            ])
        
        # ç”Ÿæˆå®Œæ•´ system instructionï¼ˆå« mode å’Œ glossaryï¼‰
        system_instruction = self.prompt_manager.get_system_instruction(
            use_vision=self.settings.processing.use_vision_mode,
            include_mode=True,
            include_glossary=bool(glossary),
            glossary_text=glossary_text
        )
        
        mode_name = getattr(self.settings.processing.translation_mode_entity, 'name', 'Default')
        glossary_count = len(glossary) if glossary else 0
        
        cache_name = self.cache_persistence.get_or_create_system_cache(
            system_instruction=system_instruction,
            model_name=self.settings.api.gemini_model,
            display_name=f"full_{mode_name}_g{glossary_count}"
        )
        
        if cache_name:
            self.cache_refs['system'] = cache_name  # æ­£å¼ç¿»è¯‘ä½¿ç”¨ 'system' key
            logger.info(f"âœ… å®Œæ•´ç¼“å­˜å·²å°±ç»ªï¼ˆæ­£å¼ç¿»è¯‘ç”¨ï¼‰: {cache_name[:50]}...")
            logger.info(f"   - ç¿»è¯‘æ¨¡å¼: {mode_name}")
            logger.info(f"   - æœ¯è¯­è¡¨: {glossary_count} æ¡")
        
        return cache_name
    
    def use_base_cache(self) -> bool:
        """åˆ‡æ¢åˆ°ä½¿ç”¨åŸºç¡€ç¼“å­˜ï¼ˆé¢„ç¿»è¯‘é˜¶æ®µï¼‰"""
        if 'base' in self.cache_refs:
            self.cache_refs['system'] = self.cache_refs['base']
            return True
        return False
    
    @property
    def async_translator(self):
        """æ‡’åŠ è½½å¼‚æ­¥ç¿»è¯‘å™¨"""
        if self._async_translator is None:
            self._async_translator = AsyncGeminiTranslator(self)
        return self._async_translator
    

    def _configure_api(self):
        """é…ç½® Gemini API"""
        try:
            # Gemini Developer API
            self._client = genai.Client(api_key=self.settings.api.gemini_api_key)
        except Exception as e:
            raise APIAuthenticationError(
                "Failed to configure Gemini API. Check your API key.",
                context={"error": str(e)}
            )

    def _create_model(self):
        """åˆ›å»º Gemini æ¨¡å‹å®ä¾‹ï¼ˆæ–° SDKï¼šä»…å‡†å¤‡ base configï¼Œå¹¶è¿”å›é€‚é…å™¨ï¼‰"""

        if self._client is None:
            raise APIAuthenticationError("Gemini client is not configured")

        safety_settings = [
            types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE"),
            types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
            types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
            types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
        ]

        # ä» settings è¯»å–ç”Ÿæˆå‚æ•°ï¼ˆè€Œä¸æ˜¯ç¡¬ç¼–ç ï¼‰
        self.generation_config = {
            "temperature": self.settings.processing.temperature,
            "top_p": self.settings.processing.top_p,
            "response_mime_type": "application/json",
            "max_output_tokens": self.settings.processing.max_output_tokens,
        }
        
        # å¯é€‰å‚æ•°ï¼štop_kï¼ˆå¦‚æœè®¾ç½®äº†æ‰æ·»åŠ ï¼‰
        if self.settings.processing.top_k is not None:
            self.generation_config["top_k"] = self.settings.processing.top_k
        
        logger.debug(f"ğŸ”§ API ç”Ÿæˆå‚æ•°: temperature={self.generation_config['temperature']}, "
                    f"top_p={self.generation_config['top_p']}, "
                    f"max_output_tokens={self.generation_config['max_output_tokens']}")

        # æ ¹æ®processingæ¨¡å¼é€‰æ‹©å¯¹åº”çš„system instructionï¼ˆåŒ…å«promptå›ºå®šéƒ¨åˆ†ï¼‰
        use_vision = self.settings.processing.use_vision_mode
        system_instruction = self.prompt_manager.get_system_instruction(use_vision=use_vision)

        self._base_generation_config = types.GenerateContentConfig(
            system_instruction=system_instruction,
            safety_settings=safety_settings,
            **self.generation_config,
        )

    def _generate_content(self, contents: Any, generation_config: Optional[Dict[str, Any]] = None, use_cache: bool = True, purpose: str = "API Call") -> Any:
        """ç»Ÿä¸€çš„å†…å®¹ç”Ÿæˆæ–¹æ³•ï¼Œå¤„ç†ç¼“å­˜é€»è¾‘
        
        Args:
            contents: è¦å‘é€çš„å†…å®¹
            generation_config: ç”Ÿæˆé…ç½®è¦†ç›–ï¼ˆå­—å…¸æ ¼å¼ï¼‰
            use_cache: æ˜¯å¦å°è¯•ä½¿ç”¨ç³»ç»Ÿç¼“å­˜
            purpose: è°ƒç”¨ç›®çš„ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            
        Returns:
            APIå“åº”å¯¹è±¡
        """
        # æ„å»ºé…ç½®
        config_update = generation_config or {}
        config = self._base_generation_config.model_copy(update=config_update)
        
        # å¤„ç†ç¼“å­˜
        cache_name = self.cache_refs.get("system") if use_cache else None
        if cache_name:
            config = config.model_copy(update={
                "cached_content": cache_name,
                "system_instruction": None,
                "tools": None,
                "tool_config": None,
            })
        
        try:
            response = self._client.models.generate_content(
                model=self.settings.api.gemini_model,
                contents=contents,
                config=config,
            )
            if cache_name:
                logger.debug(f"ğŸ”„ {purpose} ä½¿ç”¨ Gemini Cache: {cache_name[:30]}...")
            # Validate response structure to avoid downstream NoneType subscripts
            if not response:
                logger.error(f"âŒ {purpose} returned empty response object")
                raise APIError(f"Empty response from model for {purpose}", context={"response": repr(response)})

            candidates = getattr(response, 'candidates', None)
            # Check for prompt_feedback block reasons (e.g., prohibited content)
            prompt_fb = getattr(response, 'prompt_feedback', None)
            if prompt_fb is not None and getattr(prompt_fb, 'block_reason', None):
                block_reason = getattr(prompt_fb, 'block_reason')
                logger.error(f"âŒ {purpose} blocked by model: {block_reason}")
                raise APIError(f"Model blocked content for {purpose}", context={"block_reason": str(block_reason), "response": repr(response)})

            if not candidates or candidates[0] is None:
                logger.error(f"âŒ {purpose} response has no candidates: {repr(response)}")
                raise APIError(f"Model response missing candidates for {purpose}", context={"response": repr(response)})

            return response
        except Exception as e:
            # ç¼“å­˜å¤±è´¥æ—¶é™çº§
            if cache_name:
                logger.warning(f"âš ï¸  {purpose} ç¼“å­˜ä½¿ç”¨å¤±è´¥ï¼Œé™çº§ä¸ºæ™®é€šè°ƒç”¨: {e}")
                config_no_cache = config.model_copy(update={
                    "cached_content": None,
                    "system_instruction": self._base_generation_config.system_instruction,
                })
                response2 = self._client.models.generate_content(
                    model=self.settings.api.gemini_model,
                    contents=contents,
                    config=config_no_cache,
                )

                # åŒæ ·éªŒè¯å¤‡ç”¨å“åº”
                if not response2:
                    logger.error(f"âŒ {purpose} fallback returned empty response object")
                    raise APIError(f"Empty fallback response from model for {purpose}", context={"response": repr(response2)})

                # Check fallback prompt feedback as well
                prompt_fb2 = getattr(response2, 'prompt_feedback', None)
                if prompt_fb2 is not None and getattr(prompt_fb2, 'block_reason', None):
                    block_reason2 = getattr(prompt_fb2, 'block_reason')
                    logger.error(f"âŒ {purpose} fallback blocked by model: {block_reason2}")
                    raise APIError(f"Fallback model blocked content for {purpose}", context={"block_reason": str(block_reason2), "response": repr(response2)})

                candidates2 = getattr(response2, 'candidates', None)
                if not candidates2 or candidates2[0] is None:
                    logger.error(f"âŒ {purpose} fallback response has no candidates: {repr(response2)}")
                    raise APIError(f"Fallback model response missing candidates for {purpose}", context={"response": repr(response2)})

                return response2
            raise
    


    def translate_batch(
        self,
        segments: SegmentList,
        context: str = "",
        glossary: Optional[Dict[str, str]] = None
    ) -> List[str]:
        """
        æ ¸å¿ƒç¿»è¯‘æ–¹æ³•
        æ ¹æ®å†…å®¹ç±»å‹è‡ªåŠ¨åˆ†æµåˆ°å¯¹åº”å¤„ç†é€»è¾‘
        """
        if not segments:
            return []

        # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡
        has_image = any(seg.content_type == "image" for seg in segments)

        if has_image:
            return self._translate_vision_batch(segments, context, glossary)
        else:
            return self._translate_text_batch(segments, context, glossary)

    def translate_titles(self, titles: List[str]) -> TranslationMap:
        """ç¿»è¯‘æ ‡é¢˜åˆ—è¡¨"""
        if not titles:
            return {}

        input_json_str = json.dumps(titles, ensure_ascii=False)
        original_prompt = self.prompt_manager.format_title_prompt(input_json_str)

        try:
            response = self._generate_content(
                contents=original_prompt,
                generation_config=self.generation_config,
                use_cache=True,
                purpose="Title Translation"
            )
            raw_text = response.candidates[0].content.parts[0].text
            # è§£æå“åº”ï¼Œå¹¶å¤„ç†è‡ªæˆ‘ä¿®æ­£
            parsed_data = self._handle_json_response_with_correction(
                raw_text,
                original_prompt,
                is_title_translation=True
            )

            # å½’ä¸€åŒ–å¤„ç†
            if isinstance(parsed_data, dict):
                return {str(k): str(v) for k, v in parsed_data.items() if isinstance(v, str)}
            elif isinstance(parsed_data, list) and parsed_data:
                result = {}
                for item in parsed_data:
                    if isinstance(item, dict):
                        for k, v in item.items():
                            if k != 'id':  # è·³è¿‡ id å­—æ®µ
                                result[str(k)] = str(v)
                return result

            return {}

        except Exception as e:
            logger.error(f"Title translation failed even after correction attempts: {e}")
            return {}

    def extract_glossary(self, segments: SegmentList) -> Dict[str, str]:
        """ä»å·²ç¿»è¯‘çš„ç‰‡æ®µä¸­è‡ªåŠ¨æå–æœ¯è¯­è¡¨"""
        logger.info("ğŸ§  æ­£åœ¨æå–æœ¯è¯­è¡¨ä»¥å¢å¼ºåç»­ç¿»è¯‘...")
        if not segments:
            logger.warning("   - æ— å†…å®¹å¯ä¾›æå–æœ¯è¯­è¡¨ã€‚")
            return {}

        # å‡†å¤‡ç”¨äºåˆ†æçš„æ–‡æœ¬
        text_to_analyze = []
        for seg in segments:
            if seg.is_translated:
                text_to_analyze.append(f"Original: {seg.original_text}\nTranslated: {seg.translated_text}\n---")
        
        if not text_to_analyze:
            logger.warning("   - æä¾›çš„ç‰‡æ®µå‡æœªç¿»è¯‘ï¼Œæ— æ³•æå–æœ¯è¯­ã€‚")
            return {}

        content_sample = "\n".join(text_to_analyze)
        
        # æ„å»º Prompt
        original_prompt = f"""
        You are an expert linguist and terminologist.
        Analyze the following pairs of original and translated text. Identify all key, recurring, or specialized terms (like names, places, philosophical concepts, technical jargon) and create a definitive glossary.

        RULES:
        1. The output MUST be a flat JSON object.
        2. Keys are the original English terms.
        3. Values are their corresponding Chinese translations found in the text.
        4. Focus on nouns and proper nouns.
        5. Be precise. The goal is to enforce consistency.

        Example Output Format:
        {{
            "Slavoj Å½iÅ¾ek": "æ–¯æ‹‰æ²ƒçƒ­Â·é½æ³½å…‹",
            "the Real": "å®åœ¨ç•Œ",
            "Objet petit a": "å®¢ä½“å° a"
        }}

        Text to Analyze:
        <text>
        {content_sample[:8000]}
        </text>

        Return ONLY the JSON object.
        """

        try:
            if self._client is None:
                raise APIAuthenticationError("Gemini client is not configured")

            # ä½¿ç”¨ settings ä¸­çš„å‚æ•°ï¼Œç¡®ä¿ä¸€è‡´æ€§
            extraction_config = types.GenerateContentConfig(
                response_mime_type="application/json",
                temperature=self.settings.processing.temperature,
                max_output_tokens=self.settings.processing.max_output_tokens,
            )

            # Use centralized _generate_content to benefit from response validation and cache fallback
            response = self._generate_content(
                contents=original_prompt,
                generation_config={
                    "response_mime_type": "application/json",
                    "temperature": self.settings.processing.temperature,
                    "max_output_tokens": self.settings.processing.max_output_tokens,
                },
                use_cache=True,
                purpose="Glossary Extraction"
            )

            # Extract text safely (validated by _generate_content)
            raw_text = response.candidates[0].content.parts[0].text
            # å¤„ç†è‡ªæˆ‘ä¿®æ­£
            parsed_glossary = self._handle_json_response_with_correction(
                raw_text, 
                original_prompt, 
                is_glossary_extraction=True
            )
            
            # å½’ä¸€åŒ–ä¸åŒå¯èƒ½çš„æ¨¡å‹è¾“å‡ºæ ¼å¼ä¸ºå¹³å¦çš„ {str: str} å½¢å¼
            final_glossary: Dict[str, str] = {}

            if isinstance(parsed_glossary, dict):
                for k, v in parsed_glossary.items():
                    try:
                        if k and v:
                            final_glossary[str(k).strip()] = str(v).strip()
                    except TypeError:
                        # è·³è¿‡ä¸å¯å“ˆå¸Œæˆ–éæ ‡é‡çš„é”®
                        continue

            elif isinstance(parsed_glossary, list):
                for item in parsed_glossary:
                    if isinstance(item, dict):
                        for k, v in item.items():
                            if k and v:
                                final_glossary[str(k).strip()] = str(v).strip()
                    elif isinstance(item, (list, tuple)) and len(item) == 2:
                        k, v = item
                        if k and v:
                            final_glossary[str(k).strip()] = str(v).strip()

            # æ—¥å¿—å’Œè¿”å›
            if final_glossary:
                logger.info(f"   - âœ… æˆåŠŸæå– {len(final_glossary)} ä¸ªæœ¯è¯­ã€‚")
                for k, v in list(final_glossary.items())[:5]:
                    logger.info(f"     - '{k}' -> '{v}'")
                if len(final_glossary) > 5:
                    logger.info("     - ... (æ›´å¤šæœ¯è¯­)")
                return final_glossary

            logger.warning(f"   - âš ï¸ æœ¯è¯­æå–æœªèƒ½äº§ç”Ÿæœ‰æ•ˆå­—å…¸ã€‚åŸå§‹å“åº”ç±»å‹: {type(parsed_glossary)}. åŸå§‹å“åº”ç‰‡æ®µ: {raw_text[:200]}")
            return {}
        except Exception as e:
            logger.error(f"   - âŒ æå–æœ¯è¯­è¡¨æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return {}

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=30),
        retry=retry_if_exception_type((APIError, GoogleAPICallError)),
        reraise=True
    )
    def _translate_text_batch(
        self,
        segments: SegmentList,
        context: str,
        glossary: Optional[Dict[str, str]] = None
    ) -> List[str]:
        """æ–‡æœ¬æ‰¹é‡ç¿»è¯‘ï¼ˆå¸¦é‡è¯•ï¼‰"""
        # æ„å»ºè¾“å…¥æ•°æ®
        input_data = [
            {"id": seg.segment_id, "text": seg.original_text}
            for seg in segments
        ]
        input_json = json.dumps(input_data, ensure_ascii=False)

        # æˆªå–ä¸Šä¸‹æ–‡
        safe_context = context[-self.settings.processing.max_context_length:] if context else ""

        # æ ¼å¼åŒ–æœ¯è¯­è¡¨ï¼ˆä»…åœ¨éç¼“å­˜æ¨¡å¼ä¸‹æ‰åœ¨ user message ä¸­åŒ…å«ï¼‰
        # æ­£å¼ç¿»è¯‘é˜¶æ®µ glossary å·²åœ¨ system instruction ç¼“å­˜ä¸­ï¼Œè¿™é‡Œä¸éœ€è¦å†ä¼ 
        glossary_text = ""
        if glossary and not self.settings.processing.enable_gemini_caching:
            # éç¼“å­˜æ¨¡å¼ï¼šåœ¨ user message ä¸­åŒ…å« glossary
            glossary_text = "\n".join([f"- **{k}**: Must be translated as **{v}**" for k, v in glossary.items()])

        # æ ¼å¼åŒ–æç¤ºï¼ˆå›ºå®šéƒ¨åˆ†å·²åœ¨system instructionï¼Œä»…å¡«å……åŠ¨æ€å˜é‡ï¼‰
        original_prompt = self.prompt_manager.format_text_prompt(
            context=safe_context,
            input_json=input_json,
            glossary=glossary_text
        )

        response = self._generate_content(
            contents=original_prompt,
            generation_config=self.generation_config,
            use_cache=True,
            purpose="Text Translation"
        )
        try:
            raw_text = response.candidates[0].content.parts[0].text
        except Exception:
            # If the response was blocked/malformed, return failed markers for this batch
            logger.error("âŒ Text Translation response invalid or blocked; marking batch as failed")
            return ["[Failed: Blocked or invalid response]" for _ in segments]
        
        # è§£æå“åº”ï¼Œä¼ é€’æœŸæœ›çš„ ID åˆ—è¡¨ä»¥ä¾¿æ£€æµ‹ç¼ºå¤±çš„ç¿»è¯‘
        input_ids = [s.segment_id for s in segments]
        output_list = self._handle_json_response_with_correction(
            raw_text, 
            original_prompt, 
            is_text_translation=True,
            expected_ids=input_ids
        )

        # æ˜ å°„ç»“æœ
        output_map = {
            int(item['id']): str(item.get('translation', ''))
            for item in output_list
            if 'id' in item and str(item['id']).isdigit()
        }

        # ç”Ÿæˆæœ€ç»ˆç»“æœ
        results = []
        for uid in input_ids:
            results.append(output_map.get(uid, "[Failed: Missing translation]"))

        return results

    def _translate_vision_batch(
        self,
        segments: SegmentList,
        context: str,
        glossary: Optional[Dict[str, str]] = None
    ) -> List[str]:
        """è§†è§‰æ‰¹é‡ç¿»è¯‘ï¼ˆä¸²è¡Œå¤„ç†ï¼‰"""
        results = []
        current_context = context[-self.settings.processing.max_context_length:] if context else ""

        for seg in segments:
            try:
                if seg.content_type == "image" and seg.image_path:
                    translation = self._call_vision_api(
                        seg.image_path,
                        current_context
                    )
                    time.sleep(self.settings.processing.vision_rate_limit_delay)
                else:
                    # é™çº§å¤„ç†æ–‡æœ¬
                    fallback_result = self._translate_text_batch([seg], current_context, glossary)
                    translation = fallback_result[0] if fallback_result else "[Fallback Failed]"

                results.append(translation)

                # æ›´æ–°ä¸Šä¸‹æ–‡
                current_context += f"\n{translation}"
                if len(current_context) > self.settings.processing.max_context_length:
                    current_context = current_context[-self.settings.processing.max_context_length:]

            except Exception as e:
                logger.error(f"âŒ Visionç¿»è¯‘å¤±è´¥ (segment {seg.segment_id}): {e}")
                results.append(f"[Failed: {str(e)}]")
                continue

        return results

    def _call_vision_api(self, img_path: str, context: str) -> str:
        """è°ƒç”¨è§†è§‰ APIï¼ˆæ”¯æŒ Gemini Cachingï¼‰"""
        try:
            # ä½¿ç”¨ prompt_manager æ ¼å¼åŒ–æç¤º
            original_prompt = self.prompt_manager.format_vision_prompt(context)

            mime_type, _ = mimetypes.guess_type(img_path)
            mime_type = mime_type or "image/png"
            with open(img_path, "rb") as f:
                image_bytes = f.read()

            image_part = types.Part.from_bytes(data=image_bytes, mime_type=mime_type)

            vision_config = {
                "temperature": self.generation_config["temperature"],
                "top_p": self.generation_config["top_p"],
                "max_output_tokens": self.generation_config["max_output_tokens"],
                "response_mime_type": "application/json",
            }

            response = self._generate_content(
                contents=[original_prompt, image_part],
                generation_config=vision_config,
                use_cache=True,
                purpose="Vision Translation"
            )

            raw_text = (response.text or "").strip()

            # è§£æ JSON å¹¶æå– "translation" å­—æ®µï¼Œå¤„ç†è‡ªæˆ‘ä¿®æ­£
            parsed_json = self._handle_json_response_with_correction(
                raw_text,
                original_prompt,
                is_vision_translation=True,
                image_part=image_part,
            )

            if isinstance(parsed_json, dict) and "translation" in parsed_json:
                return parsed_json["translation"]

            logger.error(
                "âŒ Vision API did not return valid JSON with a 'translation' key even after correction. "
                f"Got: {raw_text[:200]}"
            )
            return "[Failed: Invalid JSON Response]"

        except Exception as e:
            logger.error(f"âŒ Vision APIè°ƒç”¨å¤±è´¥ for {img_path}: {e}")
            return f"[Failed: {str(e)}]"

    def _handle_json_response_with_correction(
        self,
        raw_text: str,
        original_prompt: str,
        is_title_translation: bool = False,
        is_glossary_extraction: bool = False,
        is_text_translation: bool = False,
        is_vision_translation: bool = False,
        image_part: Optional[Any] = None,
        expected_ids: Optional[List[int]] = None
    ) -> Any:
        """
        å¤„ç† JSON å“åº”ï¼ˆç®€åŒ–ç‰ˆï¼ŒåºŸé™¤ LLM è‡ªæˆ‘ä¿®æ­£ï¼Œä¼˜å…ˆä¿å­˜æˆåŠŸéƒ¨åˆ†ï¼‰
        
        çº é”™æµç¨‹ï¼š
        1. æ ‡å‡† JSON è§£æ
        2. æ­£åˆ™è¡¨è¾¾å¼å…œåº•è§£æï¼ˆå°½å¯èƒ½æå–æˆåŠŸçš„ç¿»è¯‘ï¼‰
        3. å¯¹äºç¼ºå¤±çš„ segmentï¼Œæ ‡è®°ä¸ºå¤±è´¥ï¼ˆä¸å†è°ƒç”¨ LLM ä¿®æ­£ï¼‰
        
        Args:
            expected_ids: æœŸæœ›çš„ segment ID åˆ—è¡¨ï¼ˆç”¨äºæ£€æµ‹ç¼ºå¤±çš„ç¿»è¯‘ï¼‰
        """
        # ========== é˜¶æ®µ1ï¼šæ ‡å‡†JSONè§£æ ==========
        try:
            parsed_data = self._repair_json_content(raw_text)
            logger.debug("âœ… æ ‡å‡†JSONè§£ææˆåŠŸ")
            return parsed_data
        except JSONParseError as e:
            logger.debug(f"âš ï¸ æ ‡å‡†JSONè§£æå¤±è´¥: {e}")
        
        # ========== é˜¶æ®µ2ï¼šæ­£åˆ™è¡¨è¾¾å¼å…œåº•è§£æ ==========
        try:
            if is_text_translation:
                fallback_result = self._regex_fallback(raw_text)
                if fallback_result and len(fallback_result) > 0:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯çœŸæ­£çš„ç¿»è¯‘å¤±è´¥æ ‡ç­¾
                    first_trans = fallback_result[0].get("translation", "")
                    if not first_trans.startswith("[Failed") and not first_trans.startswith("[Translation Failed"):
                        extracted_count = len(fallback_result)
                        logger.info(f"âœ… æ­£åˆ™è¡¨è¾¾å¼è§£ææˆåŠŸï¼Œæå– {extracted_count} æ¡ç¿»è¯‘")
                        
                        # å¦‚æœæä¾›äº†æœŸæœ›çš„ ID åˆ—è¡¨ï¼Œæ£€æŸ¥ç¼ºå¤±çš„ç¿»è¯‘
                        if expected_ids:
                            extracted_ids = {item.get("id") for item in fallback_result}
                            missing_ids = [eid for eid in expected_ids if eid not in extracted_ids]
                            
                            if missing_ids:
                                logger.warning(f"âš ï¸ {len(missing_ids)} ä¸ª segment ç¿»è¯‘ç¼ºå¤±: {missing_ids[:5]}{'...' if len(missing_ids) > 5 else ''}")
                                # ä¸ºç¼ºå¤±çš„ ID æ·»åŠ å¤±è´¥æ ‡è®°
                                for mid in missing_ids:
                                    fallback_result.append({
                                        "id": mid,
                                        "translation": "[Failed: Missing in response]"
                                    })
                        
                        return fallback_result
                        
            elif is_title_translation or is_glossary_extraction:
                fallback_result = self._regex_fallback_for_dict_like(raw_text)
                if fallback_result:
                    logger.info(f"âœ… æ­£åˆ™è¡¨è¾¾å¼è§£ææˆåŠŸï¼ˆå­—å…¸æ ¼å¼ï¼‰ï¼Œæå– {len(fallback_result)} é¡¹")
                    return fallback_result
                    
        except Exception as e:
            logger.debug(f"âš ï¸ æ­£åˆ™è¡¨è¾¾å¼è§£æå¤±è´¥: {e}")
        
        # ========== æœ€ç»ˆå…œåº•ï¼šè¿”å›é”™è¯¯æ ‡è®° ==========
        logger.error(f"âŒ JSON è§£æå¤±è´¥ï¼ˆæ ‡å‡†JSON + æ­£åˆ™å‡å¤±è´¥ï¼‰ï¼ŒåŸå§‹å“åº”é•¿åº¦: {len(raw_text)}")
        logger.debug(f"   åŸå§‹å“åº”æœ«å°¾: {raw_text[-200:] if len(raw_text) > 200 else raw_text}")
        
        if is_text_translation:
            # å¦‚æœæä¾›äº†æœŸæœ›çš„ ID åˆ—è¡¨ï¼Œä¸ºæ‰€æœ‰ ID è¿”å›å¤±è´¥æ ‡è®°
            if expected_ids:
                return [{"id": eid, "translation": "[Failed: JSON Parse Error]"} for eid in expected_ids]
            return [{"id": 1, "translation": "[Failed: JSON Parse Error]"}]
        elif is_title_translation or is_glossary_extraction:
            return {}
        elif is_vision_translation:
            return {}
        
        return None

    def _parse_json_response(self, text: str) -> List[Dict[str, Any]]:
        """è§£ææ–‡æœ¬ç¿»è¯‘çš„ JSON å“åº”ï¼Œæ”¯æŒå¤šç§æ ¼å¼"""
        try:
            # è°ƒç”¨æ–°çš„å¤„ç†å‡½æ•°ï¼Œå¹¶æ˜ç¡®è¿™æ˜¯æ–‡æœ¬ç¿»è¯‘åœºæ™¯
            # æ³¨æ„: è¿™é‡Œ original_prompt ä¼ é€’ç©ºå­—ç¬¦ä¸²ï¼Œå› ä¸º _parse_json_response ä¹‹å‰æ²¡æœ‰ç›´æ¥çš„ prompt ä¿¡æ¯ã€‚
            # åªæœ‰å½“åŸå§‹ API è°ƒç”¨å¤±è´¥æ—¶ï¼Œ_handle_json_response_with_correction æ‰ä¼šä½¿ç”¨åˆ° original_prompt è¿›è¡Œè‡ªæˆ‘ä¿®æ­£ã€‚
            # å¦‚æœæ˜¯ _parse_json_response ç‹¬ç«‹è°ƒç”¨ï¼ˆä¾‹å¦‚ä»ç¼“å­˜è¯»å–ï¼‰ï¼Œé‚£ä¹ˆ original_prompt ç¡®å®æ˜¯æœªçŸ¥çš„ã€‚
            result = self._handle_json_response_with_correction(text, "", is_text_translation=True)
            if isinstance(result, list):
                return result
            elif isinstance(result, dict) and 'translations' in result:
                return result['translations']
            else:
                return []
        except Exception as e:
            logger.error(f"âŒ æœ€ç»ˆJSONè§£æå¤±è´¥ï¼ŒåŒ…æ‹¬ä¿®æ­£å’Œæ­£åˆ™å›é€€: {e}")
            return []

    def _repair_json_content(self, text: str) -> Any:
        """ä¿®å¤ JSON å­—ç¬¦ä¸² (åªè¿›è¡Œä»£ç å—å»é™¤ï¼Œä¸è¿›è¡Œé«˜çº§å­—ç¬¦ä¸²ä¿®å¤)"""
        # å»é™¤ Markdown ä»£ç å—
        pattern = r'^```(?:json)?\s*(.*)\s*```$'
        match = re.search(pattern, text, re.DOTALL)
        if match:
            text = match.group(1)

        try:
            return json.loads(text)
        except json.JSONDecodeError as e:
            # ä¸å†è¿›è¡Œå†…éƒ¨é«˜çº§ä¿®å¤ï¼Œç›´æ¥æŠ›å‡ºï¼Œç”±ä¸Šå±‚å¤„ç†è‡ªæˆ‘ä¿®æ­£
            raise JSONParseError(f"Initial JSON parse failed: {e}")

    def _regex_fallback(self, text: str) -> List[Dict[str, Any]]:
        """æ­£åˆ™è¡¨è¾¾å¼å…œåº•è§£æï¼ˆæ”¯æŒæˆªæ–­æ¢å¤ï¼‰"""
        logger.info("ğŸ”„ Using regex fallback for JSON parsing...")

        # æ£€æµ‹æ˜¯å¦è¢«æˆªæ–­ï¼ˆæœ«å°¾æ²¡æœ‰ ] æˆ–æœ€åä¸€ä¸ªå¯¹è±¡ä¸å®Œæ•´ï¼‰
        is_truncated = not text.rstrip().endswith(']')
        if is_truncated:
            logger.warning("âš ï¸ Detected incomplete JSON (missing closing bracket or truncated content)")

        # ç­–ç•¥1: æ ‡å‡†JSONæ ¼å¼ï¼ˆå®Œæ•´å¯¹è±¡ï¼‰
        pattern = r'"id":\s*(\d+),\s*"translation":\s*"((?:[^"\\]|\\.)*)"\s*\}'
        matches = re.findall(pattern, text, re.DOTALL)

        if not matches:
            # ç­–ç•¥2: å®½æ¾åŒ¹é…ï¼ˆå…è®¸ç¼ºå°‘ç»“æŸæ‹¬å·ï¼‰
            pattern_loose = r'"id":\s*(\d+),\s*"translation":\s*"((?:[^"\\]|\\.)*?)"'
            matches = re.findall(pattern_loose, text, re.DOTALL)

        if not matches:
            # ç­–ç•¥3: å•å¼•å·æ ¼å¼
            pattern_sq = r"'id':\s*(\d+),\s*'translation':\s*'((?:[^'\\]|\\.)*)'"
            matches = re.findall(pattern_sq, text, re.DOTALL)

        if not matches:
            # ç­–ç•¥4: æåº¦å®½æ¾ï¼ˆå¤„ç†æˆªæ–­æƒ…å†µï¼‰- æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„ id/translation å¯¹
            pattern_ultra = r'"id"\s*:\s*(\d+)[^}]*"translation"\s*:\s*"([^"]*?)(?:"|$)'
            matches = re.findall(pattern_ultra, text, re.DOTALL)

        if not matches:
            logger.error(f"âŒ Regex fallback failed completely. Text length: {len(text)}, Last 200 chars: {repr(text[-200:])}")
            logger.warning("âš ï¸ Returning translation failure tag to ensure output file integrity.")
            return [{"id": 1, "translation": "[Translation Failed - JSON Parse Error]"}]

        logger.info(f"âœ… Regex extracted {len(matches)} segments" + (" (from truncated JSON)" if is_truncated else ""))
        
        result = []
        for mid, mtext in matches:
            # æ¸…ç†è½¬ä¹‰å­—ç¬¦
            cleaned_text = mtext.replace('\\"', '"').replace("\\'", "'").replace('\\n', '\n')
            # æ£€æµ‹æœ€åä¸€ä¸ªå¯¹è±¡æ˜¯å¦è¢«æˆªæ–­
            if is_truncated and (mid, mtext) == matches[-1]:
                # æ£€æŸ¥æ˜¯å¦åœ¨å¥å­ä¸­é—´æˆªæ–­ï¼ˆæ²¡æœ‰æ ‡ç‚¹ç¬¦å·ç»“å°¾ï¼‰
                if cleaned_text and not cleaned_text.rstrip().endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?', 'ã€', '"', ')', 'ï¼‰')):
                    logger.warning(f"âš ï¸ Segment {mid} appears truncated (no sentence-ending punctuation), marking as incomplete")
                    cleaned_text += "[...ç¿»è¯‘è¢«æˆªæ–­]"
            result.append({"id": int(mid), "translation": cleaned_text})
        
        return result

    def _regex_fallback_for_dict_like(self, text: str) -> Optional[Dict[str, str]]:
        """æ­£åˆ™è¡¨è¾¾å¼å…œåº•è§£æï¼ˆå­—å…¸æ ¼å¼ï¼Œç”¨äº title translation å’Œ glossary extractionï¼‰
        
        ç›®æ ‡æ ¼å¼ç¤ºä¾‹ï¼š
        {"Chapter 1": "ç¬¬ä¸€ç« ", "Introduction": "ç®€ä»‹"}
        æˆ–
        {"æœ¯è¯­A": "ç¿»è¯‘A", "æœ¯è¯­B": "ç¿»è¯‘B"}
        """
        logger.info("ğŸ”„ Using regex fallback for dict-like JSON parsing...")
        
        result = {}
        
        # ç­–ç•¥1: æ ‡å‡† JSON é”®å€¼å¯¹æ ¼å¼
        pattern = r'"([^"]+)"\s*:\s*"([^"]*)"'
        matches = re.findall(pattern, text, re.DOTALL)
        
        if matches:
            for key, value in matches:
                # è·³è¿‡å¯èƒ½çš„å…ƒæ•°æ®å­—æ®µ
                if key.lower() in ('id', 'type', 'status', 'error'):
                    continue
                # æ¸…ç†è½¬ä¹‰å­—ç¬¦
                cleaned_key = key.replace('\\"', '"').replace("\\'", "'").replace('\\n', '\n')
                cleaned_value = value.replace('\\"', '"').replace("\\'", "'").replace('\\n', '\n')
                result[cleaned_key] = cleaned_value
        
        if not result:
            # ç­–ç•¥2: å•å¼•å·æ ¼å¼
            pattern_sq = r"'([^']+)'\s*:\s*'([^']*)'"
            matches = re.findall(pattern_sq, text, re.DOTALL)
            for key, value in matches:
                if key.lower() in ('id', 'type', 'status', 'error'):
                    continue
                cleaned_key = key.replace("\\'", "'").replace('\\n', '\n')
                cleaned_value = value.replace("\\'", "'").replace('\\n', '\n')
                result[cleaned_key] = cleaned_value
        
        if result:
            logger.info(f"âœ… Regex extracted {len(result)} key-value pairs (dict format)")
            return result
        else:
            logger.error(f"âŒ Regex fallback for dict-like failed. Text length: {len(text)}")
            return None


# ========================================================================
# Gemini å¼‚æ­¥ç¿»è¯‘å®¢æˆ·ç«¯
# ========================================================================

class AsyncGeminiTranslator(BaseAsyncTranslator):
    """å¼‚æ­¥ Gemini ç¿»è¯‘å®¢æˆ·ç«¯ï¼Œæ”¯æŒå¹¶å‘æ‰¹é‡ç¿»è¯‘ã€‚
    
    ç»§æ‰¿è‡ª BaseAsyncTranslatorï¼Œå®ç° Gemini çš„å¼‚æ­¥ç¿»è¯‘é€»è¾‘ã€‚
    æ”¯æŒä¸Šä¸‹æ–‡ç®¡ç†å™¨è‡ªåŠ¨èµ„æºæ¸…ç†ã€‚
    """

    def __init__(self, base_translator: GeminiTranslator):
        """
        Args:
            base_translator: åŸºç¡€çš„ GeminiTranslator å®ä¾‹ï¼Œç”¨äºå¤ç”¨é…ç½®å’ŒåŒæ­¥æ–¹æ³•
        """
        # è°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°
        super().__init__(base_translator)
        
        self.generation_config = base_translator.generation_config
        self.cache_refs = base_translator.cache_refs
        self.prompt_manager = base_translator.prompt_manager  # å¤ç”¨ prompt_manager
        
        # ä» settings è·å–çº¿ç¨‹æ± å¤§å°ï¼Œé»˜è®¤ 10
        max_workers = getattr(base_translator.settings.processing, 'async_max_workers', 10)
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        
        # ä» settings è·å–è§†è§‰ API ä¿¡å·é‡ï¼Œé»˜è®¤ 3
        self.vision_semaphore_limit = getattr(base_translator.settings.processing, 'vision_max_concurrent', 3)
        
        # ä» settings è·å–è¶…æ—¶é…ç½®ï¼Œé»˜è®¤ 300 ç§’
        self.async_timeout = getattr(base_translator.settings.processing, 'async_batch_timeout', 300)
        
        logger.debug(f"ğŸ”§ AsyncGeminiTranslator initialized: workers={max_workers}, vision_sem={self.vision_semaphore_limit}, timeout={self.async_timeout}s")
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡ºï¼Œè‡ªåŠ¨æ¸…ç†èµ„æº"""
        self.cleanup()
        return False
    
    def __enter__(self):
        """åŒæ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£ï¼ˆå…¼å®¹æ€§ï¼‰"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """åŒæ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡º"""
        self.cleanup()
        return False
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç¡®ä¿èµ„æºæ¸…ç†"""
        try:
            if hasattr(self, 'executor') and self.executor is not None:
                self.cleanup()
        except Exception:
            pass
    
    async def translate_text_batch_async(
        self,
        segments: SegmentList,
        context: str,
        glossary: Optional[Dict[str, str]] = None
    ) -> List[str]:
        """
        å¼‚æ­¥æ‰¹é‡ç¿»è¯‘æ–‡æœ¬segmentï¼ˆç®€åŒ–ç‰ˆï¼Œä¸åŒæ­¥æ¨¡å¼é€»è¾‘å®Œå…¨ä¸€è‡´ï¼‰
        
        æ¶æ„è®¾è®¡ï¼ˆV4ï¼šä¸åŒæ­¥æ¨¡å¼ç»Ÿä¸€ï¼Œä¸€ä¸ª batch = ä¸€æ¬¡ API è°ƒç”¨ï¼‰ï¼š
        
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ åŒæ­¥/å¼‚æ­¥æ¨¡å¼ç»Ÿä¸€æµç¨‹:                                           â”‚
        â”‚                                                                 â”‚
        â”‚ 1. æ•´ä¸ª batch çš„ segments æ‰“åŒ…æˆ JSON æ•°ç»„                       â”‚
        â”‚    [{"id": 1, "original": "..."}, {"id": 2, "original": "..."}] â”‚
        â”‚                                                                 â”‚
        â”‚ 2. ä¸€æ¬¡ API è°ƒç”¨ç¿»è¯‘æ•´ä¸ª batch                                   â”‚
        â”‚                                                                 â”‚
        â”‚ 3. LLM è¿”å›å¯¹åº”çš„ç¿»è¯‘ç»“æœæ•°ç»„                                    â”‚
        â”‚    [{"id": 1, "translation": "..."}, {"id": 2, ...}]            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        
        å¹¶å‘æ§åˆ¶åœ¨ workflow å±‚é€šè¿‡ Semaphore å®ç°ï¼Œengine å±‚åªè´Ÿè´£å•æ¬¡ç¿»è¯‘ã€‚
        
        Args:
            segments: å¾…ç¿»è¯‘çš„ segment åˆ—è¡¨ï¼ˆä¸€ä¸ª batchï¼‰
            context: ç¿»è¯‘ä¸Šä¸‹æ–‡ï¼ˆbatch ä¹‹å‰çš„åŸæ–‡ï¼Œç”± workflow å±‚æä¾›ï¼‰
            glossary: æœ¯è¯­è¡¨ï¼ˆç¼“å­˜æ¨¡å¼ä¸‹ä¼šè¢«å¿½ç•¥ï¼‰
        
        Returns:
            ç¿»è¯‘ç»“æœåˆ—è¡¨
        """
        if not segments:
            return []
        
        logger.info(f"ğŸš€ å¼‚æ­¥ç¿»è¯‘ {len(segments)} ä¸ªæ–‡æœ¬æ®µ...")
        
        # ========== ä¸åŒæ­¥æ¨¡å¼å®Œå…¨ä¸€è‡´çš„æ•°æ®å‡†å¤‡ ==========
        
        # æˆªå–ä¸Šä¸‹æ–‡
        safe_context = context[-self.settings.processing.max_context_length:] if context else ""
        
        # å‡†å¤‡è¾“å…¥æ•°æ®ï¼ˆä¸åŒæ­¥æ¨¡å¼å®Œå…¨ä¸€è‡´ï¼‰
        input_data = [
            {"id": seg.segment_id, "original": seg.original_text}
            for seg in segments
        ]
        input_json = json.dumps(input_data, ensure_ascii=False)
        
        # æ ¼å¼åŒ–æœ¯è¯­è¡¨ï¼ˆä»…åœ¨éç¼“å­˜æ¨¡å¼ä¸‹ä½¿ç”¨ï¼‰
        glossary_text = ""
        if glossary and not self.settings.processing.enable_gemini_caching:
            glossary_text = "\n".join([f"- **{k}**: Must be translated as **{v}**" for k, v in glossary.items()])
        
        # æ ¼å¼åŒ– Promptï¼ˆä¸åŒæ­¥æ¨¡å¼å®Œå…¨ä¸€è‡´ï¼‰
        original_prompt = self.prompt_manager.format_text_prompt(
            context=safe_context,
            input_json=input_json,
            glossary=glossary_text
        )
        
        # ========== å¼‚æ­¥æ‰§è¡Œ API è°ƒç”¨ ==========
        
        # è·å–å½“å‰äº‹ä»¶å¾ªç¯
        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            loop = asyncio.get_event_loop()
        
        def _call_with_cache():
            return self.base._generate_content(
                contents=original_prompt,
                generation_config=self.generation_config,
                use_cache=True,
                purpose="Async Text Translation"
            )
        
        # é‡è¯•é€»è¾‘ï¼ˆä¸åŒæ­¥æ¨¡å¼çš„ @retry è£…é¥°å™¨æ•ˆæœä¸€è‡´ï¼‰
        retry_count = 2
        last_error = None
        input_ids = [s.segment_id for s in segments]
        
        for attempt in range(retry_count + 1):
            try:
                # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥çš„ API è°ƒç”¨
                response = await asyncio.wait_for(
                    loop.run_in_executor(self.executor, _call_with_cache),
                    timeout=self.async_timeout
                )
                
                raw_text = response.candidates[0].content.parts[0].text
                
                # è§£æå“åº”ï¼ˆå¤ç”¨åŒæ­¥æ–¹æ³•ï¼Œä¼ é€’æœŸæœ›çš„ ID åˆ—è¡¨ï¼‰
                output_list = self.base._handle_json_response_with_correction(
                    raw_text,
                    original_prompt,
                    is_text_translation=True,
                    expected_ids=input_ids
                )
                
                # æ˜ å°„ç»“æœï¼ˆä¸åŒæ­¥æ¨¡å¼å®Œå…¨ä¸€è‡´ï¼‰
                output_map = {
                    int(item['id']): str(item.get('translation', ''))
                    for item in output_list
                    if 'id' in item and str(item['id']).isdigit()
                }
                
                # ç”Ÿæˆæœ€ç»ˆç»“æœ
                results = [output_map.get(uid, "[Failed: Missing translation]") for uid in input_ids]
                
                success_count = len([r for r in results if not r.startswith('[Failed')])
                logger.info(f"âœ… å¼‚æ­¥ç¿»è¯‘å®Œæˆï¼ŒæˆåŠŸ {success_count}/{len(segments)}")
                
                return results
            
            except asyncio.TimeoutError:
                last_error = f"Timeout after {self.async_timeout}s"
                logger.error(f"âŒ å¼‚æ­¥ç¿»è¯‘è¶…æ—¶ï¼ˆ{self.async_timeout}sï¼‰")
                break  # è¶…æ—¶ä¸é‡è¯•
            
            except Exception as e:
                last_error = e
                if attempt < retry_count:
                    wait_time = 2 ** attempt
                    logger.warning(f"âš ï¸ ç¿»è¯‘å¤±è´¥ï¼ˆå°è¯• {attempt + 1}/{retry_count + 1}ï¼‰ï¼Œ{wait_time}s åé‡è¯•: {e}")
                    await asyncio.sleep(wait_time)
                else:
                    logger.error(f"âŒ ç¿»è¯‘å¤±è´¥ï¼Œå·²ç”¨å°½æ‰€æœ‰é‡è¯•: {e}")
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å›å¤±è´¥æ ‡è®°
        return [f"[Failed: {str(last_error)}]"] * len(segments)

    async def translate_vision_batch_async(
        self,
        segments: SegmentList,
        context: str,
        glossary: Optional[Dict[str, str]] = None
    ) -> List[str]:
        """
        å¼‚æ­¥æ‰¹é‡ç¿»è¯‘åŒ…å«å›¾åƒçš„segment
        
        ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°ï¼Œé¿å…è§¦å‘ Gemini é€Ÿç‡é™åˆ¶
        """
        if not segments:
            return []
        
        logger.info(f"ğŸ–¼ï¸ ä½¿ç”¨å¼‚æ­¥æ¨¡å¼ç¿»è¯‘ {len(segments)} ä¸ªè§†è§‰æ®µï¼ˆå¹¶å‘é™åˆ¶: {self.vision_semaphore_limit}ï¼‰...")
        
        # åˆ›å»ºä¿¡å·é‡ï¼Œé™åˆ¶å¹¶å‘è§†è§‰ API è°ƒç”¨æ•°ï¼ˆä»é…ç½®è¯»å–ï¼‰
        semaphore = asyncio.Semaphore(self.vision_semaphore_limit)
        
        # åˆ›å»ºç¿»è¯‘ä»»åŠ¡
        tasks = []
        for seg in segments:
            if seg.content_type == "image" and seg.image_path:
                task = self._call_vision_api_async(
                    seg.image_path,
                    context,
                    semaphore
                )
            else:
                # æ–‡æœ¬é™çº§å¤„ç†
                task = self._translate_text_fallback_async(seg, context, glossary)
            
            tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰ç¿»è¯‘å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†å¼‚å¸¸ç»“æœ
        final_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"âŒ è§†è§‰ç¿»è¯‘å¤±è´¥ (segment {segments[i].segment_id}): {result}")
                final_results.append(f"[Failed: {str(result)}]")
            else:
                final_results.append(result)
        
        logger.info(f"âœ… å¼‚æ­¥è§†è§‰ç¿»è¯‘å®Œæˆ")
        return final_results

    async def _call_vision_api_async(
        self,
        img_path: str,
        context: str,
        semaphore: asyncio.Semaphore,
        retry_count: int = 2
    ) -> str:
        """å¼‚æ­¥è°ƒç”¨è§†è§‰ APIï¼Œä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘ï¼Œæ”¯æŒé‡è¯•"""
        
        async with semaphore:  # é™åˆ¶å¹¶å‘æ•°
            # è·å–å½“å‰äº‹ä»¶å¾ªç¯ï¼ˆå®‰å…¨æ–¹å¼ï¼‰
            try:
                loop = asyncio.get_running_loop()
            except RuntimeError:
                loop = asyncio.get_event_loop()
            
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œ I/O ç»‘å®šçš„å›¾åƒå¤„ç†
            def _process_vision():
                # ç›´æ¥è°ƒç”¨åŸºç¡€ç¿»è¯‘å™¨çš„è§†è§‰APIæ–¹æ³•
                return self.base._call_vision_api(img_path, context)
            
            # é‡è¯•é€»è¾‘
            last_error = None
            for attempt in range(retry_count + 1):
                try:
                    result = await loop.run_in_executor(self.executor, _process_vision)
                    
                    # æ·»åŠ å»¶è¿Ÿé¿å…é€Ÿç‡é™åˆ¶
                    await asyncio.sleep(self.settings.processing.vision_rate_limit_delay)
                    
                    if attempt > 0:
                        logger.info(f"âœ… è§†è§‰ API é‡è¯•æˆåŠŸï¼ˆç¬¬ {attempt + 1} æ¬¡å°è¯•ï¼‰: {img_path}")
                    
                    return result
                
                except Exception as e:
                    last_error = e
                    if attempt < retry_count:
                        wait_time = 2 ** attempt
                        logger.warning(f"âš ï¸ è§†è§‰ API å¤±è´¥ï¼ˆå°è¯• {attempt + 1}/{retry_count + 1}ï¼‰ï¼Œ{wait_time}s åé‡è¯•: {img_path}")
                        await asyncio.sleep(wait_time)
                    else:
                        logger.error(f"âŒ è§†è§‰ API å¤±è´¥ï¼Œå·²ç”¨å°½æ‰€æœ‰é‡è¯•: {img_path}")
            
            return f"[Failed: {str(last_error)}]"

    async def _translate_text_fallback_async(
        self,
        segment: ContentSegment,
        context: str,
        glossary: Optional[Dict[str, str]]
    ) -> str:
        """å¼‚æ­¥æ–‡æœ¬é™çº§å¤„ç†"""
        # è·å–å½“å‰äº‹ä»¶å¾ªç¯ï¼ˆå®‰å…¨æ–¹å¼ï¼‰
        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            loop = asyncio.get_event_loop()
        
        def _sync_fallback():
            results = self.base._translate_text_batch(
                [segment],
                context,
                glossary
            )
            return results[0] if results else "[Fallback Failed]"
        
        return await loop.run_in_executor(self.executor, _sync_fallback)

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'executor') and self.executor is not None:
            try:
                self.executor.shutdown(wait=True)
                self.executor = None  # æ ‡è®°ä¸ºå·²æ¸…ç†
                logger.info("ğŸ§¹ å¼‚æ­¥ç¿»è¯‘å™¨å·²æ¸…ç†èµ„æº")
            except Exception as e:
                logger.debug(f"æ¸…ç† executor æ—¶å‡ºç°è­¦å‘Š: {e}")


# ========================================================================
# OpenAI-compatible (DeepSeek) ç¿»è¯‘å®¢æˆ·ç«¯
# ========================================================================


class OpenAICompatibleTranslator(BaseTranslator):
    """OpenAI-compatible ç¿»è¯‘å®¢æˆ·ç«¯ï¼ˆDeepSeek API å…¼å®¹ OpenAI æ ¼å¼ï¼‰ã€‚

    DeepSeek docs:
      - base_url: https://api.deepseek.com (or https://api.deepseek.com/v1)
      - endpoint: POST /chat/completions

    Env vars are provided via Settings.api:
      - API_OPENAI_API_KEY / OPENAI_API_KEY / DEEPSEEK_API_KEY
      - OPENAI_BASE_URL / DEEPSEEK_BASE_URL
      - OPENAI_MODEL / DEEPSEEK_MODEL
    """

    def __init__(self, settings: Settings):
        super().__init__(settings)
        self.prompt_manager = PromptManager(settings)
        self._async_translator: Optional[AsyncOpenAICompatibleTranslator] = None

        self.api_key: Optional[str] = settings.api.openai_api_key
        self.base_url: str = settings.api.openai_base_url
        self.model: str = settings.api.openai_model
        
        # éªŒè¯å’Œä¿®å¤ base_url é…ç½®
        self.base_url = self._validate_and_fix_base_url(self.base_url)
        
        # è‡ªåŠ¨æ£€æµ‹æ˜¯å¦ä¸ºæœ¬åœ°æœåŠ¡ï¼ˆOllamaï¼‰æˆ– DeepSeek API
        # é€‚é… M2 Pro 16GB ç¡¬ä»¶ç¯å¢ƒï¼Œæœ¬åœ°æ¨¡å¼éœ€è¦ç‰¹æ®Šå¤„ç†
        self.is_local: bool = self._detect_local_service(self.base_url)
        self.is_deepseek: bool = self._detect_deepseek_api(self.base_url)
        
        # DeepSeek é•¿æ–‡æœ¬æ¨¡å¼ï¼šè‡ªåŠ¨åˆ‡æ¢ä¸ºå• message æ¨¡å¼ï¼ˆsystem + instruction + mode + context + promptï¼‰
        # åŸå› ï¼šDeepSeek å¯¹é•¿ä¸Šä¸‹æ–‡æ”¯æŒæ›´å¥½ï¼Œä¸” system message å¯èƒ½å½±å“æ€§èƒ½
        self.use_long_text_mode: bool = self.is_deepseek
        
        if self.is_local:
            logger.info("ğŸ  æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å¼ï¼ˆOllamaï¼‰")
            logger.info(f"   - æœåŠ¡åœ°å€: {self.base_url}")
            logger.info(f"   - æ¨¡å‹: {self.model}")
            logger.info("   - å·²å¯ç”¨ M2 Pro ä¼˜åŒ–ï¼šnum_ctx=8192, num_thread=10")
        elif self.is_deepseek:
            logger.info("ğŸš€ æ£€æµ‹åˆ° DeepSeek API")
            logger.info(f"   - API åœ°å€: {self.base_url}")
            logger.info(f"   - æ¨¡å‹: {self.model}")
            logger.info("   - å·²å¯ç”¨é•¿æ–‡æœ¬æ¨¡å¼ï¼ˆæ‰€æœ‰å†…å®¹åˆå¹¶ä¸ºå•ä¸ª user messageï¼‰")
        else:
            logger.info("â˜ï¸  æ£€æµ‹åˆ°äº‘ç«¯æ¨¡å¼ï¼ˆOpenAIï¼‰")
            logger.info(f"   - API åœ°å€: {self.base_url}")
            logger.info(f"   - æ¨¡å‹: {self.model}")

        if not self.api_key:
            raise APIAuthenticationError(
                "OpenAI-compatible API key is missing. Set API_OPENAI_API_KEY (or OPENAI_API_KEY/DEEPSEEK_API_KEY).",
                context={"setting": "API_OPENAI_API_KEY"},
            )
    
    def _validate_and_fix_base_url(self, base_url: str) -> str:
        """éªŒè¯å¹¶ä¿®å¤ base_url é…ç½®
        
        Args:
            base_url: åŸå§‹çš„ base_url é…ç½®
            
        Returns:
            ä¿®å¤åçš„ base_url
        """
        if not base_url:
            raise ValueError("OPENAI_BASE_URL ä¸èƒ½ä¸ºç©º")
            
        base = base_url.strip()
        
        # å¦‚æœå·²ç»æ˜¯å®Œæ•´çš„ URLï¼Œç›´æ¥è¿”å›
        if base.startswith(('http://', 'https://')):
            return base
            
        # å¤„ç†å¸¸è§çš„é”™è¯¯é…ç½®
        if 'deepseek' in base.lower():
            # DeepSeek å¸¸è§é”™è¯¯é…ç½®
            logger.warning(f"âš ï¸ æ£€æµ‹åˆ°ä¸å®Œæ•´çš„ DeepSeek URL é…ç½®: '{base}'")
            logger.warning("   è‡ªåŠ¨ä¿®å¤ä¸º: https://api.deepseek.com")
            return 'https://api.deepseek.com'
        elif 'localhost' in base or '127.0.0.1' in base:
            # æœ¬åœ°æœåŠ¡
            if not base.startswith('http://'):
                fixed_url = f'http://{base}'
                logger.warning(f"âš ï¸ æœ¬åœ°æœåŠ¡ URL ç¼ºå°‘åè®®: '{base}' -> '{fixed_url}'")
                return fixed_url
            return base
        else:
            # å…¶ä»–äº‘ç«¯æœåŠ¡ï¼Œå‡è®¾æ˜¯åŸŸåï¼Œæ·»åŠ  https://
            fixed_url = f'https://{base}'
            logger.warning(f"âš ï¸ æ£€æµ‹åˆ°ä¸å®Œæ•´çš„ URL é…ç½®: '{base}' -> '{fixed_url}'")
            logger.warning("   å¦‚æœè¿™æ˜¯é”™è¯¯çš„ï¼Œè¯·åœ¨é…ç½®ä¸­æä¾›å®Œæ•´çš„ URLï¼ˆåŒ…å« http:// æˆ– https://ï¼‰")
            return fixed_url

    def _detect_local_service(self, base_url: str) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºæœ¬åœ°æœåŠ¡ï¼ˆOllamaï¼‰
        
        Args:
            base_url: API åŸºç¡€ URL
            
        Returns:
            True å¦‚æœæ˜¯æœ¬åœ°æœåŠ¡ï¼ˆåŒ…å« localhost æˆ– 127.0.0.1ï¼‰
        """
        if not base_url:
            return False
        url_lower = base_url.lower()
        return 'localhost' in url_lower or '127.0.0.1' in url_lower
    
    def _detect_deepseek_api(self, base_url: str) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸º DeepSeek API
        
        Args:
            base_url: API åŸºç¡€ URL
            
        Returns:
            True å¦‚æœæ˜¯ DeepSeek APIï¼ˆåŒ…å« api.deepseek.comï¼‰
        """
        if not base_url:
            return False
        url_lower = base_url.lower()
        return 'deepseek.com' in url_lower or 'deepseek' in url_lower

    @property
    def async_translator(self) -> Optional['AsyncOpenAICompatibleTranslator']:
        # æœ¬åœ°æ¨¡å¼å¼ºåˆ¶åŒæ­¥ç¿»è¯‘ï¼Œé™ä½åŠŸè€—å’Œå†…å­˜å‹åŠ›
        if self.is_local:
            logger.debug("ğŸ”’ æœ¬åœ°æ¨¡å¼ç¦ç”¨å¼‚æ­¥ç¿»è¯‘ï¼ˆé™ä½åŠŸè€—ï¼‰")
            return None
        
        if self._async_translator is None:
            self._async_translator = AsyncOpenAICompatibleTranslator(self)
        return self._async_translator

    def translate_batch(
        self,
        segments: SegmentList,
        context: str = "",
        glossary: Optional[Dict[str, str]] = None,
    ) -> List[str]:
        if not segments:
            return []

        has_image = any(seg.content_type == "image" for seg in segments)
        if has_image:
            return self._translate_vision_batch(segments, context)
        return self._translate_text_batch(segments, context, glossary)

    def translate_titles(self, titles: List[str]) -> TranslationMap:
        if not titles:
            return {}

        input_json_str = json.dumps(titles, ensure_ascii=False)
        original_prompt = self.prompt_manager.format_title_prompt(input_json_str)

        raw_text = self._chat_completions(
            system_instruction=self.prompt_manager.get_system_instruction(use_vision=False),
            user_content=original_prompt,
        )

        parsed_data = self._handle_json_response_with_repair(
            raw_text=raw_text,
            original_prompt=original_prompt,
            is_dict_like=True,
        )

        if isinstance(parsed_data, dict):
            return {str(k): str(v) for k, v in parsed_data.items() if isinstance(v, str)}

        if isinstance(parsed_data, list) and parsed_data:
            result: Dict[str, str] = {}
            for item in parsed_data:
                if isinstance(item, dict):
                    for k, v in item.items():
                        if k != 'id':
                            result[str(k)] = str(v)
            return result

        return {}

    def extract_glossary(self, segments: SegmentList) -> Dict[str, str]:
        if not segments:
            return {}

        text_to_analyze: List[str] = []
        for seg in segments:
            if seg.is_translated:
                text_to_analyze.append(
                    f"Original: {seg.original_text}\nTranslated: {seg.translated_text}\n---"
                )

        if not text_to_analyze:
            return {}

        content_sample = "\n".join(text_to_analyze)
        original_prompt = f"""
You are an expert linguist and terminologist.
Analyze the following pairs of original and translated text. Identify all key, recurring, or specialized terms (like names, places, philosophical concepts, technical jargon) and create a definitive glossary.

RULES:
1. The output MUST be a flat JSON object.
2. Keys are the original English terms.
3. Values are their corresponding Chinese translations found in the text.
4. Focus on nouns and proper nouns.
5. Be precise. The goal is to enforce consistency.

Text to Analyze:
<text>
{content_sample[:8000]}
</text>

Return ONLY the JSON object.
""".strip()

        raw_text = self._chat_completions(
            system_instruction="You output JSON only.",
            user_content=original_prompt,
        )

        parsed = self._handle_json_response_with_repair(
            raw_text=raw_text,
            original_prompt=original_prompt,
            is_dict_like=True,
        )

        final_glossary: Dict[str, str] = {}
        if isinstance(parsed, dict):
            for k, v in parsed.items():
                if k and v:
                    final_glossary[str(k).strip()] = str(v).strip()
        return final_glossary

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=20),
        retry=retry_if_exception_type((APIError,)),
        reraise=True,
    )
    def _translate_text_batch(
        self,
        segments: SegmentList,
        context: str,
        glossary: Optional[Dict[str, str]] = None,
    ) -> List[str]:
        input_data = [{"id": seg.segment_id, "text": seg.original_text} for seg in segments]
        input_json = json.dumps(input_data, ensure_ascii=False)

        safe_context = context[-self.settings.processing.max_context_length:] if context else "No Context"

        glossary_text = "N/A"
        if glossary:
            glossary_text = "\n".join(
                [f"- **{k}**: Must be translated as **{v}**" for k, v in glossary.items()]
            )

        # DeepSeek é•¿æ–‡æœ¬æ¨¡å¼ï¼šå°† system instruction åµŒå…¥åˆ° user content ä¸­
        if self.use_long_text_mode:
            system_instruction = self.prompt_manager.get_system_instruction(use_vision=False)
            user_prompt = self.prompt_manager.format_text_prompt(
                context=safe_context,
                input_json=input_json,
                glossary=glossary_text,
            )
            # å°† system instruction å’Œ user prompt åˆå¹¶ä¸ºå®Œæ•´çš„é•¿æ–‡æœ¬ prompt
            combined_prompt = f"{system_instruction}\n\n{'='*80}\n\n{user_prompt}"
            
            raw_text = self._chat_completions(
                system_instruction="",  # é•¿æ–‡æœ¬æ¨¡å¼ä¸‹ system_instruction ä¸ºç©º
                user_content=combined_prompt,
            )
        else:
            # æ ‡å‡†æ¨¡å¼ï¼šsystem å’Œ user åˆ†ç¦»
            original_prompt = self.prompt_manager.format_text_prompt(
                context=safe_context,
                input_json=input_json,
                glossary=glossary_text,
            )

            raw_text = self._chat_completions(
                system_instruction=self.prompt_manager.get_system_instruction(use_vision=False),
                user_content=original_prompt,
            )

        # è§£æå“åº”ï¼Œä¼ é€’æœŸæœ›çš„ ID åˆ—è¡¨ä»¥ä¾¿æ£€æµ‹ç¼ºå¤±çš„ç¿»è¯‘
        input_ids = [s.segment_id for s in segments]
        output_list = self._handle_json_response_with_repair(
            raw_text=raw_text,
            original_prompt=combined_prompt if self.use_long_text_mode else original_prompt,
            is_text_translation=True,
            expected_ids=input_ids,
        )

        output_map = {
            int(item['id']): str(item.get('translation', ''))
            for item in output_list
            if isinstance(item, dict) and 'id' in item and str(item['id']).isdigit()
        }
        return [output_map.get(uid, "[Failed: Missing translation]") for uid in input_ids]

    def _translate_vision_batch(self, segments: SegmentList, context: str) -> List[str]:
        results: List[str] = []
        current_context = context[-self.settings.processing.max_context_length:] if context else ""

        for seg in segments:
            if seg.content_type == "image" and seg.image_path:
                results.append(self._call_vision_api(seg.image_path, current_context))
            else:
                fallback = self._translate_text_batch([seg], current_context, glossary=None)
                results.append(fallback[0] if fallback else "[Fallback Failed]")

            current_context += f"\n{results[-1]}"
            if len(current_context) > self.settings.processing.max_context_length:
                current_context = current_context[-self.settings.processing.max_context_length:]

        return results

    def _call_vision_api(self, img_path: str, context: str) -> str:
        original_prompt = self.prompt_manager.format_vision_prompt(context)

        try:
            with open(img_path, 'rb') as f:
                b64 = base64.b64encode(f.read()).decode('ascii')
            image_url = f"data:image/png;base64,{b64}"

            raw_text = self._chat_completions(
                system_instruction=self.prompt_manager.get_system_instruction(use_vision=True),
                user_content=[
                    {"type": "text", "text": original_prompt},
                    {"type": "image_url", "image_url": {"url": image_url}},
                ],
            )

            parsed = self._handle_json_response_with_repair(
                raw_text=raw_text,
                original_prompt=original_prompt,
                is_dict_like=True,
            )
            if isinstance(parsed, dict) and 'translation' in parsed:
                return str(parsed['translation'])
            return "[Failed: Invalid JSON Response]"
        except Exception as e:
            logger.error(f"âŒ OpenAI-compatible Vision API è°ƒç”¨å¤±è´¥ for {img_path}: {e}")
            return f"[Failed: {str(e)}]"

    def _build_chat_completions_url(self) -> str:
        """æ„å»º Chat Completions API URL
        
        é’ˆå¯¹æœ¬åœ° Ollama æœåŠ¡çš„è·¯å¾„ä¿®å¤é€»è¾‘ï¼š
        - æœ¬åœ°æ¨¡å¼ï¼šå¼ºåˆ¶ä½¿ç”¨ http://127.0.0.1:11434/v1/chat/completions
        - äº‘ç«¯æ¨¡å¼ï¼ˆDeepSeekï¼‰ï¼šä¿æŒåŸé€»è¾‘
        """
        base = (self.base_url or '').rstrip('/')
        
        # æœ¬åœ°æ¨¡å¼ï¼šå¼ºåˆ¶ä½¿ç”¨ 127.0.0.1:11434/v1/chat/completionsï¼ˆOllama æ ‡å‡†æ¥å£ï¼‰
        if self.is_local:
            # ç»Ÿä¸€ä½¿ç”¨ 127.0.0.1 è€Œé localhostï¼Œé¿å… DNS è§£æé—®é¢˜
            return 'http://127.0.0.1:11434/v1/chat/completions'
        
        # äº‘ç«¯æ¨¡å¼ï¼šç¡®ä¿ base_url æ˜¯å®Œæ•´çš„ URL
        if not base.startswith(('http://', 'https://')):
            # å¦‚æœä¸æ˜¯å®Œæ•´çš„ URLï¼Œå°è¯•ä¿®å¤å¸¸è§çš„é”™è¯¯é…ç½®
            if 'deepseek' in base.lower():
                # DeepSeek å¸¸è§é”™è¯¯é…ç½®ä¿®å¤
                logger.warning(f"âš ï¸ æ£€æµ‹åˆ°ä¸å®Œæ•´çš„ DeepSeek URL é…ç½®: '{base}'ï¼Œè‡ªåŠ¨ä¿®å¤ä¸ºæ ‡å‡† URL")
                return 'https://api.deepseek.com/v1/chat/completions'
            else:
                # å…¶ä»–æœåŠ¡ï¼Œå°è¯•æ·»åŠ  https:// å‰ç¼€
                logger.warning(f"âš ï¸ æ£€æµ‹åˆ°ä¸å®Œæ•´çš„ URL é…ç½®: '{base}'ï¼Œå°è¯•æ·»åŠ  https:// å‰ç¼€")
                base = f'https://{base}'
        
        # äº‘ç«¯æ¨¡å¼ï¼šDeepSeek æ”¯æŒä¸¤ç§æ ¼å¼
        # https://api.deepseek.com/chat/completions æˆ–
        # https://api.deepseek.com/v1/chat/completions
        if base.endswith('/v1'):
            return base + '/chat/completions'
        return base + '/chat/completions'

    def _chat_completions(self, system_instruction: str, user_content: Any) -> str:
        """è°ƒç”¨ Chat Completions API
        
        æœ¬åœ°æ¨¡å¼ä¼˜åŒ–ï¼ˆM2 Pro 16GBï¼‰ï¼š
        - å¼ºåˆ¶ 120s è¶…æ—¶ï¼ˆæœ¬åœ°æ¨ç†è¾ƒæ…¢ï¼‰
        - æ³¨å…¥ options å­—æ®µï¼šnum_ctx=8192ï¼ˆé•¿æ–‡æœ¬è®°å¿†ï¼‰ï¼Œnum_thread=10ï¼ˆé€‚é… M2 Pro æ ¸å¿ƒæ•°ï¼‰
        
        DeepSeek é•¿æ–‡æœ¬æ¨¡å¼ï¼š
        - æ‰€æœ‰å†…å®¹å·²é¢„å…ˆåˆå¹¶åˆ° user_content ä¸­ï¼Œsystem_instruction ä¸ºç©º
        - æ ¼å¼ï¼šå®Œæ•´çš„é•¿æ–‡æœ¬ prompt åŒ…å« system instruction + mode + context + input
        - åŸå› ï¼šDeepSeek å¯¹é•¿ä¸Šä¸‹æ–‡æ”¯æŒæ›´å¥½ï¼Œä¸”é¿å… system message é™åˆ¶
        """
        # DeepSeek é•¿æ–‡æœ¬æ¨¡å¼ï¼šæ‰€æœ‰å†…å®¹å·²é¢„å…ˆåˆå¹¶ï¼Œæ— éœ€é¢å¤–å¤„ç†
        if self.use_long_text_mode:
            payload = {
                "model": self.model,
                "messages": [
                    {"role": "user", "content": user_content},
                ],
                "temperature": 0.2,
                "stream": False,
            }
            logger.debug("ğŸ“ ä½¿ç”¨é•¿æ–‡æœ¬æ¨¡å¼ï¼ˆæ‰€æœ‰å†…å®¹é¢„å…ˆåˆå¹¶ï¼‰")
        else:
            # æ ‡å‡†æ¨¡å¼ï¼šsystem + user åˆ†ç¦»
            payload = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": system_instruction},
                    {"role": "user", "content": user_content},
                ],
                "temperature": 0.2,
                "stream": False,
            }
        
        # è®°å½•å‘é€ç»™APIçš„æ–‡æœ¬é•¿åº¦
        total_text_length = 0
        for message in payload["messages"]:
            content = message["content"]
            if isinstance(content, str):
                total_text_length += len(content)
            elif isinstance(content, list):
                # å¤„ç†å¤šæ¨¡æ€å†…å®¹
                for item in content:
                    if isinstance(item, dict) and "text" in item:
                        total_text_length += len(item["text"])
        
        logger.info(f"ğŸ“¤ å‘é€APIè¯·æ±‚ - æ–‡æœ¬æ€»é•¿åº¦: {total_text_length} å­—ç¬¦")
        if self.use_long_text_mode:
            logger.debug("   ğŸ“Š é•¿æ–‡æœ¬æ¨¡å¼: System Instruction + åˆ†éš”ç¬¦ + Mode + Glossary + Context + Input JSON")
        else:
            logger.debug("   ğŸ“Š æ ‡å‡†æ¨¡å¼: System Instruction + User Content")
        
        # æœ¬åœ°æ¨¡å¼ï¼šæ³¨å…¥ Ollama ä¸“ç”¨å‚æ•°ï¼Œé’ˆå¯¹ M2 Pro 16GB ä¼˜åŒ–
        if self.is_local:
            payload["options"] = {
                "num_ctx": 1024,      # è¿›ä¸€æ­¥é™ä½ï¼šä» 2048 åˆ° 1024ï¼ˆé€‚åº”è¶…é•¿ä¸Šä¸‹æ–‡ï¼‰
                "num_thread": 1,      # è¿›ä¸€æ­¥é™ä½ï¼šä» 2 åˆ° 1ï¼ˆå•çº¿ç¨‹ï¼Œæä½å†…å­˜å‹åŠ›ï¼‰
            }
            logger.debug("ğŸ”§ æœ¬åœ°æ¨¡å¼ payload å·²æ³¨å…¥ options: num_ctx=1024, num_thread=1")

        url = self._build_chat_completions_url()
        data = json.dumps(payload).encode('utf-8')
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {self.api_key}',
        }
        
        # åŠ¨æ€è¶…æ—¶ï¼šæœ¬åœ°æ¨¡å¼å¼ºåˆ¶ 120sï¼Œäº‘ç«¯æ¨¡å¼æ ¹æ®æœåŠ¡è°ƒæ•´
        if self.is_deepseek:
            timeout = 120  # DeepSeekå“åº”è¾ƒæ…¢ï¼Œå¢åŠ åˆ°120ç§’
            logger.debug(f"â±ï¸  DeepSeekæ¨¡å¼è¶…æ—¶è®¾ç½®: {timeout}s")
        elif self.is_local:
            timeout = 120 if self.is_local else self.settings.processing.request_timeout
            if self.is_local:
                logger.debug(f"â±ï¸  æœ¬åœ°æ¨¡å¼è¶…æ—¶è®¾ç½®: {timeout}s")
        else:
            timeout = self.settings.processing.request_timeout

        req = request.Request(url, data=data, headers=headers, method='POST')
        try:
            with request.urlopen(req, timeout=timeout) as resp:
                resp_text = resp.read().decode('utf-8')
        except error.HTTPError as e:
            body = e.read().decode('utf-8', errors='replace') if hasattr(e, 'read') else ''
            raise APIError(f"OpenAI-compatible HTTPError: {e.code} {e.reason} {body[:200]}")
        except error.URLError as e:
            raise APITimeoutError(f"OpenAI-compatible request failed: {e}")
        except TimeoutError as e:
            raise APITimeoutError(f"OpenAI-compatible request timeout: {e}")

        try:
            parsed = json.loads(resp_text)
            content = parsed['choices'][0]['message']['content']
            if not isinstance(content, str):
                return json.dumps(content, ensure_ascii=False)
            return content.strip()
        except Exception as e:
            raise APIError(f"OpenAI-compatible response parse failed: {e}")

    def _strip_code_fences(self, text: str) -> str:
        pattern = r'^```(?:json)?\s*(.*)\s*```$'
        match = re.search(pattern, text, re.DOTALL)
        return match.group(1) if match else text

    def _handle_json_response_with_repair(
        self,
        raw_text: str,
        original_prompt: str,
        *,
        is_text_translation: bool = False,
        is_dict_like: bool = False,
        expected_ids: Optional[List[int]] = None,
    ) -> Any:
        """
        å¤„ç† JSON å“åº”ï¼ˆç®€åŒ–ç‰ˆï¼Œä¸ Gemini translator é€»è¾‘ä¸€è‡´ï¼‰
        
        çº é”™æµç¨‹ï¼š
        1. æ ‡å‡† JSON è§£æ
        2. æ­£åˆ™è¡¨è¾¾å¼å…œåº•è§£æï¼ˆå°½å¯èƒ½æå–æˆåŠŸçš„ç¿»è¯‘ï¼‰
        3. å¯¹äºç¼ºå¤±çš„ segmentï¼Œæ ‡è®°ä¸ºå¤±è´¥ï¼ˆä¸å†è°ƒç”¨ LLM ä¿®æ­£ï¼‰
        
        Args:
            expected_ids: æœŸæœ›çš„ segment ID åˆ—è¡¨ï¼ˆç”¨äºæ£€æµ‹ç¼ºå¤±çš„ç¿»è¯‘ï¼‰
        """
        # ========== é˜¶æ®µ1ï¼šæ ‡å‡†JSONè§£æ ==========
        try:
            cleaned = self._strip_code_fences(raw_text)
            return json.loads(cleaned)
        except json.JSONDecodeError as e:
            logger.debug(f"âš ï¸ æ ‡å‡†JSONè§£æå¤±è´¥: {e}")
        
        # ========== é˜¶æ®µ2ï¼šæ­£åˆ™è¡¨è¾¾å¼å…œåº•è§£æ ==========
        if is_text_translation:
            try:
                fallback = self._regex_fallback_for_list(raw_text)
                if fallback and len(fallback) > 0:
                    extracted_count = len(fallback)
                    logger.info(f"âœ… æ­£åˆ™è¡¨è¾¾å¼è§£ææˆåŠŸï¼Œæå– {extracted_count} æ¡ç¿»è¯‘")
                    
                    # å¦‚æœæä¾›äº†æœŸæœ›çš„ ID åˆ—è¡¨ï¼Œæ£€æŸ¥ç¼ºå¤±çš„ç¿»è¯‘
                    if expected_ids:
                        extracted_ids = {item.get("id") for item in fallback}
                        missing_ids = [eid for eid in expected_ids if eid not in extracted_ids]
                        
                        if missing_ids:
                            logger.warning(f"âš ï¸ {len(missing_ids)} ä¸ª segment ç¿»è¯‘ç¼ºå¤±: {missing_ids[:5]}{'...' if len(missing_ids) > 5 else ''}")
                            # ä¸ºç¼ºå¤±çš„ ID æ·»åŠ å¤±è´¥æ ‡è®°
                            for mid in missing_ids:
                                fallback.append({
                                    "id": mid,
                                    "translation": "[Failed: Missing in response]"
                                })
                    
                    return fallback
            except Exception as e:
                logger.debug(f"âš ï¸ æ­£åˆ™è¡¨è¾¾å¼è§£æå¤±è´¥: {e}")
        
        # ========== æœ€ç»ˆå…œåº•ï¼šè¿”å›é”™è¯¯æ ‡è®° ==========
        logger.error(f"âŒ JSON è§£æå¤±è´¥ï¼ˆæ ‡å‡†JSON + æ­£åˆ™å‡å¤±è´¥ï¼‰ï¼ŒåŸå§‹å“åº”é•¿åº¦: {len(raw_text)}")
        
        if is_text_translation:
            if expected_ids:
                return [{"id": eid, "translation": "[Failed: JSON Parse Error]"} for eid in expected_ids]
            return [{"id": 1, "translation": "[Failed: JSON Parse Error]"}]
        if is_dict_like:
            return {}
        raise JSONParseError("Failed to parse JSON")

    def _regex_fallback_for_list(self, text: str) -> List[Dict[str, Any]]:
        """æ­£åˆ™è¡¨è¾¾å¼å…œåº•è§£æï¼ˆä¸ Gemini translator çš„ _regex_fallback é€»è¾‘ä¸€è‡´ï¼‰"""
        logger.info("ğŸ”„ Using regex fallback for JSON parsing...")
        
        # æ£€æµ‹æ˜¯å¦è¢«æˆªæ–­
        is_truncated = not text.rstrip().endswith(']')
        if is_truncated:
            logger.warning("âš ï¸ Detected incomplete JSON (missing closing bracket)")
        
        # ç­–ç•¥1: æ ‡å‡†JSONæ ¼å¼
        pattern = r'"id":\s*(\d+),\s*"translation":\s*"((?:[^"\\]|\\.)*)"\s*\}'
        matches = re.findall(pattern, text, re.DOTALL)
        
        if not matches:
            # ç­–ç•¥2: å®½æ¾åŒ¹é…
            pattern_loose = r'"id":\s*(\d+),\s*"translation":\s*"((?:[^"\\]|\\.)*?)"'
            matches = re.findall(pattern_loose, text, re.DOTALL)
        
        if not matches:
            return []
        
        logger.info(f"âœ… Regex extracted {len(matches)} segments" + (" (from truncated JSON)" if is_truncated else ""))
        
        result = []
        for mid, mtext in matches:
            cleaned_text = mtext.replace('\\"', '"').replace("\\'", "'").replace('\\n', '\n')
            # æ£€æµ‹æœ€åä¸€ä¸ªå¯¹è±¡æ˜¯å¦è¢«æˆªæ–­
            if is_truncated and (mid, mtext) == matches[-1]:
                if cleaned_text and not cleaned_text.rstrip().endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?', 'ã€', '"', ')', 'ï¼‰')):
                    cleaned_text += "[...ç¿»è¯‘è¢«æˆªæ–­]"
            result.append({"id": int(mid), "translation": cleaned_text})
        
        return result

# ========================================================================
# OpenAI-compatible (DeepSeek) å¼‚æ­¥ç¿»è¯‘å®¢æˆ·ç«¯
# ========================================================================
class AsyncOpenAICompatibleTranslator(BaseAsyncTranslator):
    """å¼‚æ­¥ OpenAI-compatible ç¿»è¯‘å™¨ï¼ˆçº¿ç¨‹æ± åŒ…è£…ï¼ŒåŒæ­¥HTTPè¯·æ±‚å¹¶å‘æ‰§è¡Œï¼‰ã€‚
    
    å¹¶å‘æ§åˆ¶ç­–ç•¥ï¼ˆM2 Pro 16GB ä¼˜åŒ–ï¼‰ï¼š
    - æœ¬åœ°æ¨¡å¼ï¼ˆOllamaï¼‰ï¼šmax_workers=2ï¼Œé˜²æ­¢ 16GB ç»Ÿä¸€å†…å­˜æº¢å‡º
    - äº‘ç«¯æ¨¡å¼ï¼ˆDeepSeekï¼‰ï¼šmax_workers=10ï¼Œå……åˆ†åˆ©ç”¨ç½‘ç»œå¹¶å‘
    æ”¯æŒä¸Šä¸‹æ–‡ç®¡ç†å™¨è‡ªåŠ¨èµ„æºæ¸…ç†ã€‚
    """

    def __init__(self, base_translator: OpenAICompatibleTranslator):
        super().__init__(base_translator)
        
        # åŠ¨æ€å¹¶å‘æ§åˆ¶ï¼šæ ¹æ®æœåŠ¡ç±»å‹è°ƒæ•´å¹¶å‘æ•°
        if base_translator.is_local:
            # æœ¬åœ°æ¨¡å¼ï¼š2 å¹¶å‘ï¼ˆM2 Pro 16GB é™åˆ¶ï¼Œé¿å…æ˜¾å­˜æº¢å‡ºï¼‰
            max_workers = 2
        elif base_translator.is_deepseek:
            # DeepSeekï¼š3 å¹¶å‘ï¼ˆé¿å…è§¦å‘é€Ÿç‡é™åˆ¶ï¼ŒDeepSeekå“åº”è¾ƒæ…¢ï¼‰
            max_workers = 3
        else:
            # å…¶ä»–äº‘ç«¯æ¨¡å¼ï¼š10 å¹¶å‘ï¼ˆç½‘ç»œ I/O å¯†é›†ï¼Œå¯ä»¥é«˜å¹¶å‘ï¼‰
            max_workers = 10
            
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self._max_workers = max_workers  # ä¿å­˜ç”¨äºæ—¥å¿—
        
        # æ—¥å¿—è¾“å‡ºå½“å‰å¹¶å‘æ¨¡å¼
        if base_translator.is_local:
            logger.info("ğŸ”’ å¼‚æ­¥ç¿»è¯‘å™¨å·²åˆå§‹åŒ–ï¼ˆæœ¬åœ°æ¨¡å¼ï¼‰")
            logger.info("   - å¹¶å‘æ•°: 2ï¼ˆM2 Pro 16GB å†…å­˜ä¿æŠ¤ï¼‰")
            logger.info("   - åŸå› : é˜²æ­¢æœ¬åœ°æ¨¡å‹å¹¶å‘å¯¼è‡´ç»Ÿä¸€å†…å­˜æº¢å‡º")
        elif base_translator.is_deepseek:
            logger.info("ğŸš€ å¼‚æ­¥ç¿»è¯‘å™¨å·²åˆå§‹åŒ–ï¼ˆDeepSeekæ¨¡å¼ï¼‰")
            logger.info("   - å¹¶å‘æ•°: 3ï¼ˆé€Ÿç‡é™åˆ¶ä¿æŠ¤ï¼‰")
            logger.info("   - åŸå› : DeepSeekå“åº”è¾ƒæ…¢ï¼Œé¿å…è§¦å‘APIé™æµ")
        else:
            logger.info("ğŸš€ å¼‚æ­¥ç¿»è¯‘å™¨å·²åˆå§‹åŒ–ï¼ˆäº‘ç«¯æ¨¡å¼ï¼‰")
            logger.info("   - å¹¶å‘æ•°: 10ï¼ˆç½‘ç»œ I/O ä¼˜åŒ–ï¼‰")
            logger.info("   - é€‚ç”¨äº: OpenAI ç­‰äº‘ç«¯æœåŠ¡")
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡ºï¼Œè‡ªåŠ¨æ¸…ç†èµ„æº"""
        self.cleanup()
        return False
    
    def __enter__(self):
        """åŒæ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£ï¼ˆå…¼å®¹æ€§ï¼‰"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """åŒæ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡º"""
        self.cleanup()
        return False
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç¡®ä¿èµ„æºæ¸…ç†"""
        try:
            self.cleanup()
        except Exception:
            pass

    async def translate_text_batch_async(
        self,
        segments: SegmentList,
        context: str,
        glossary: Optional[Dict[str, str]] = None,
    ) -> List[str]:
        if not segments:
            return []

        # è·å–å½“å‰äº‹ä»¶å¾ªç¯ï¼ˆå®‰å…¨æ–¹å¼ï¼‰
        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            loop = asyncio.get_event_loop()

        def _sync():
            return self.base._translate_text_batch(segments, context, glossary)

        return await loop.run_in_executor(self.executor, _sync)

    async def translate_vision_batch_async(
        self,
        segments: SegmentList,
        context: str,
        glossary: Optional[Dict[str, str]] = None,
    ) -> List[str]:
        if not segments:
            return []

        # è·å–å½“å‰äº‹ä»¶å¾ªç¯ï¼ˆå®‰å…¨æ–¹å¼ï¼‰
        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            loop = asyncio.get_event_loop()

        def _sync_one(seg: ContentSegment) -> str:
            if seg.content_type == 'image' and seg.image_path:
                return self.base._call_vision_api(seg.image_path, context)
            fallback = self.base._translate_text_batch([seg], context, glossary)
            return fallback[0] if fallback else "[Fallback Failed]"

        tasks = [loop.run_in_executor(self.executor, _sync_one, seg) for seg in segments]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        final: List[str] = []
        for r in results:
            if isinstance(r, Exception):
                final.append(f"[Failed: {str(r)}]")
            else:
                final.append(r)
        return final

    def cleanup(self):
        self.executor.shutdown(wait=True)
        logger.info("ğŸ§¹ OpenAI-compatible å¼‚æ­¥ç¿»è¯‘å™¨å·²æ¸…ç†èµ„æº")
