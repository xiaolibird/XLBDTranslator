"""
Gemini ç¿»è¯‘å®¢æˆ·ç«¯
ä½¿ç”¨ tenacity è¿›è¡Œé‡è¯•ç®¡ç†
"""
import asyncio
import base64
import json
import time
import re
import mimetypes
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor
from urllib import request, error

from google import genai
from google.genai import types
from google.genai import errors as genai_errors
from google.api_core.exceptions import GoogleAPICallError, ResourceExhausted, ServiceUnavailable, ClientError, DeadlineExceeded
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

from ..core.schema import Settings, ContentSegment, TranslationMap, SegmentList
from ..core.exceptions import (
    APIError, APIRateLimitError, APITimeoutError, APIAuthenticationError,
    JSONParseError, TranslationError
)
from .base import BaseTranslator, BaseAsyncTranslator
from .support import CachePersistenceManager, PromptManager
from ..utils.logger import get_logger

logger = get_logger(__name__)


class _GenAIModelAdapter:
    """Adapter to preserve the legacy `.generate_content(..., generation_config=...)` call shape.

    The new `google.genai` SDK uses `client.models.generate_content(model=..., contents=..., config=...)`.
    This adapter keeps existing translator code changes minimal.
    """

    def __init__(self, translator: "GeminiTranslator", *, use_system_cache: bool, purpose: str):
        self._translator = translator
        self._use_system_cache = use_system_cache
        self._purpose = purpose

    def generate_content(self, contents: Any, generation_config: Optional[Any] = None):
        translator = self._translator

        config_update: Dict[str, Any] = {}
        if generation_config is None:
            pass
        elif isinstance(generation_config, types.GenerateContentConfig):
            config_update = generation_config.model_dump(exclude_none=True)
        elif isinstance(generation_config, dict):
            config_update = dict(generation_config)
        else:
            # Best-effort conversion (e.g., pydantic types or other config objects).
            if hasattr(generation_config, "model_dump"):
                config_update = generation_config.model_dump(exclude_none=True)
            elif hasattr(generation_config, "__dict__"):
                config_update = dict(getattr(generation_config, "__dict__", {}) or {})

        config = translator._base_generation_config.model_copy(update=config_update)

        cache_name = translator.cache_refs.get("system") if self._use_system_cache else None
        if cache_name:
            # When using cached_content, we MUST NOT pass system_instruction/tools/tool_config
            # because they are already in the cache. Remove them from config.
            config = config.model_copy(update={
                "cached_content": cache_name,
                "system_instruction": None,
                "tools": None,
                "tool_config": None,
            })

        try:
            response = translator._client.models.generate_content(
                model=translator.settings.api.gemini_model,
                contents=contents,
                config=config,
            )
            if cache_name:
                logger.debug(f"ğŸ”„ {self._purpose} ä½¿ç”¨ Gemini Cache: {cache_name[:30]}...")
            return response
        except Exception as e:
            # If cached content is stale/invalid, fall back once without cache.
            if cache_name:
                logger.warning(f"âš ï¸  {self._purpose} ç¼“å­˜ä½¿ç”¨å¤±è´¥ï¼Œé™çº§ä¸ºæ™®é€šè°ƒç”¨: {e}")
                config_no_cache = config.model_copy(update={
                    "cached_content": None,
                    # Restore system_instruction from base config when not using cache
                    "system_instruction": translator._base_generation_config.system_instruction,
                })
                return translator._client.models.generate_content(
                    model=translator.settings.api.gemini_model,
                    contents=contents,
                    config=config_no_cache,
                )
            raise


class GeminiTranslator(BaseTranslator):
    """Gemini ç¿»è¯‘å®¢æˆ·ç«¯ã€‚
    
    ç»§æ‰¿è‡ª BaseTranslatorï¼Œå®ç° Google Gemini çš„å…·ä½“ç¿»è¯‘é€»è¾‘ã€‚
    """

    def __init__(
        self, 
        settings: Settings, 
        cache_manager: Optional[CachePersistenceManager] = None
    ):
        """
        Args:
            settings: å…¨å±€è®¾ç½®å¯¹è±¡ï¼ˆåŒ…å«document_pathç”¨äºè®¡ç®—hashï¼‰
            cache_manager: ç¼“å­˜æŒä¹…åŒ–ç®¡ç†å™¨ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚æœæä¾›åˆ™ä½¿ç”¨ï¼Œå¦åˆ™è‡ªåŠ¨åˆ›å»º
        """
        # è°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°
        super().__init__(settings)
        
        self.generation_config = {}
        self.cache_refs: Dict[str, str] = {}
        self._async_translator = None  # æ‡’åŠ è½½å¼‚æ­¥ç¿»è¯‘å™¨
        self._client: Optional[genai.Client] = None
        self._base_generation_config: Optional[types.GenerateContentConfig] = None
        
        # åˆå§‹åŒ– Prompt ç®¡ç†å™¨
        self.prompt_manager = PromptManager(settings)
        
        # åˆå§‹åŒ–ç¼“å­˜æŒä¹…åŒ–ç®¡ç†å™¨ï¼ˆä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ï¼Œå¦åˆ™æ ¹æ®doc_hashåˆ›å»ºï¼‰
        self.cache_persistence = cache_manager
        if self.cache_persistence is None and settings.processing.enable_gemini_caching and self.doc_hash:
            self.cache_persistence = CachePersistenceManager(settings)

        # é…ç½® API
        self._configure_api()

        # åˆå§‹åŒ–æ¨¡å‹ï¼ˆæ–° SDKï¼šClient + GenerateContentConfigï¼›é€šè¿‡é€‚é…å™¨ä¿ç•™æ—§è°ƒç”¨å½¢æ€ï¼‰
        self.model = self._create_model()
        
        # åˆ›å»ºSystem Instructionç¼“å­˜ï¼ˆå§”æ‰˜ç»™ç¼“å­˜ç®¡ç†å™¨ï¼‰
        if settings.processing.enable_gemini_caching and self.cache_persistence:
            cache_name = self.cache_persistence.get_or_create_system_cache(
                mode_entity=settings.processing.translation_mode_entity,
                system_instruction=self.prompt_manager.get_system_instruction(use_vision=settings.processing.use_vision_mode),
                model_name=settings.api.gemini_model
            )
            if cache_name:
                self.cache_refs['system'] = cache_name
                logger.info(f"âœ… System Instruction ç¼“å­˜å·²å°±ç»ª: {cache_name[:50]}...")
    
    @property
    def async_translator(self):
        """æ‡’åŠ è½½å¼‚æ­¥ç¿»è¯‘å™¨"""
        if self._async_translator is None:
            self._async_translator = AsyncGeminiTranslator(self)
        return self._async_translator
    

    def _configure_api(self):
        """é…ç½® Gemini API"""
        try:
            # Gemini Developer API
            self._client = genai.Client(api_key=self.settings.api.gemini_api_key)
        except Exception as e:
            raise APIAuthenticationError(
                "Failed to configure Gemini API. Check your API key.",
                context={"error": str(e)}
            )

    def _create_model(self):
        """åˆ›å»º Gemini æ¨¡å‹å®ä¾‹ï¼ˆæ–° SDKï¼šä»…å‡†å¤‡ base configï¼Œå¹¶è¿”å›é€‚é…å™¨ï¼‰"""

        if self._client is None:
            raise APIAuthenticationError("Gemini client is not configured")

        safety_settings = [
            types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE"),
            types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
            types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
            types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
        ]

        self.generation_config = {
            "temperature": 0.2,  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¨³å®šçš„è¾“å‡º
            "top_p": 0.95,
            "response_mime_type": "application/json",
            "max_output_tokens": 8192,
        }

        # æ ¹æ®processingæ¨¡å¼é€‰æ‹©å¯¹åº”çš„system instructionï¼ˆåŒ…å«promptå›ºå®šéƒ¨åˆ†ï¼‰
        use_vision = self.settings.processing.use_vision_mode
        system_instruction = self.prompt_manager.get_system_instruction(use_vision=use_vision)

        self._base_generation_config = types.GenerateContentConfig(
            system_instruction=system_instruction,
            safety_settings=safety_settings,
            **self.generation_config,
        )

        return _GenAIModelAdapter(self, use_system_cache=False, purpose="Gemini")

    def _get_model_with_system_cache(self, purpose: str) -> Any:
        """Return a model adapter that uses cached_content when available."""
        cache_name = self.cache_refs.get("system")
        if not cache_name:
            return self.model
        return _GenAIModelAdapter(self, use_system_cache=True, purpose=purpose)
    


    def _attempt_json_self_correction(
        self,
        original_prompt: str,
        failed_json: str,
        error_message: str,
        retries_left: int,
        is_vision: bool = False,
        image_part: Optional[Any] = None
    ) -> Optional[Any]:
        """
        å°è¯•å¼•å¯¼æ¨¡å‹è‡ªæˆ‘ä¿®æ­£ JSON è¾“å‡ºã€‚
        """
        if retries_left <= 0:
            logger.warning("âŒ JSON è‡ªæˆ‘ä¿®æ­£é‡è¯•æ¬¡æ•°å·²ç”¨å°½ã€‚")
            return None

        logger.info(f"ğŸ”„ å°è¯• JSON è‡ªæˆ‘ä¿®æ­£ (å‰©ä½™ {retries_left} æ¬¡)...")
        repair_prompt_content = self.prompt_manager.format_json_repair_prompt(
            original_prompt=original_prompt,
            broken_json=failed_json,
            error_details=error_message
        )

        try:
            # ä½¿ç”¨ä¸åŸå§‹è¯·æ±‚ç›¸åŒçš„é…ç½®
            gen_cfg = {
                "temperature": self.generation_config["temperature"],
                "top_p": self.generation_config["top_p"],
                "max_output_tokens": self.generation_config["max_output_tokens"],
                "response_mime_type": "application/json",
            }

            content_parts = [repair_prompt_content]
            if is_vision and image_part is not None:
                content_parts.append(image_part)

            response = self.model.generate_content(
                content_parts,
                generation_config=gen_cfg
            )
            raw_text = response.text.strip()

            # å°è¯•è§£æä¿®æ­£åçš„ JSON
            parsed_data = json.loads(raw_text)
            logger.info("âœ… JSON è‡ªæˆ‘ä¿®æ­£æˆåŠŸã€‚")
            return parsed_data
        except (json.JSONDecodeError, GoogleAPICallError, genai_errors.APIError, Exception) as e:
            logger.warning(f"âš ï¸ JSON è‡ªæˆ‘ä¿®æ­£å¤±è´¥ (ç¬¬ {self.settings.processing.json_repair_retries - retries_left + 1} æ¬¡): {e}")
            # é€’å½’é‡è¯•
            return self._attempt_json_self_correction(
                original_prompt=original_prompt,
                failed_json=raw_text if 'raw_text' in locals() else failed_json, # ä¼ å…¥æœ€æ–°çš„å¤±è´¥JSON
                error_message=str(e),
                retries_left=retries_left - 1,
                is_vision=is_vision,
                image_part=image_part
            )

    def translate_batch(
        self,
        segments: SegmentList,
        context: str = "",
        glossary: Optional[Dict[str, str]] = None
    ) -> List[str]:
        """
        æ ¸å¿ƒç¿»è¯‘æ–¹æ³•
        æ ¹æ®å†…å®¹ç±»å‹è‡ªåŠ¨åˆ†æµåˆ°å¯¹åº”å¤„ç†é€»è¾‘
        """
        if not segments:
            return []

        # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡
        has_image = any(seg.content_type == "image" for seg in segments)

        if has_image:
            return self._translate_vision_batch(segments, context, glossary)
        else:
            return self._translate_text_batch(segments, context, glossary)

    def translate_titles(self, titles: List[str]) -> TranslationMap:
        """ç¿»è¯‘æ ‡é¢˜åˆ—è¡¨"""
        if not titles:
            return {}

        input_json_str = json.dumps(titles, ensure_ascii=False)
        original_prompt = self.prompt_manager.format_title_prompt(input_json_str)

        try:
            model = self._get_model_with_system_cache("Title API")
            response = model.generate_content(
                original_prompt,
                generation_config=self.generation_config
            )
            raw_text = response.text.strip()

            # è§£æå“åº”ï¼Œå¹¶å¤„ç†è‡ªæˆ‘ä¿®æ­£
            parsed_data = self._handle_json_response_with_correction(
                raw_text,
                original_prompt,
                is_title_translation=True
            )

            # å½’ä¸€åŒ–å¤„ç†
            if isinstance(parsed_data, dict):
                return {str(k): str(v) for k, v in parsed_data.items() if isinstance(v, str)}
            elif isinstance(parsed_data, list) and parsed_data:
                result = {}
                for item in parsed_data:
                    if isinstance(item, dict):
                        for k, v in item.items():
                            if k != 'id':  # è·³è¿‡ id å­—æ®µ
                                result[str(k)] = str(v)
                return result

            return {}

        except Exception as e:
            logger.error(f"Title translation failed even after correction attempts: {e}")
            return {}

    def extract_glossary(self, segments: SegmentList) -> Dict[str, str]:
        """ä»å·²ç¿»è¯‘çš„ç‰‡æ®µä¸­è‡ªåŠ¨æå–æœ¯è¯­è¡¨"""
        logger.info("ğŸ§  æ­£åœ¨æå–æœ¯è¯­è¡¨ä»¥å¢å¼ºåç»­ç¿»è¯‘...")
        if not segments:
            logger.warning("   - æ— å†…å®¹å¯ä¾›æå–æœ¯è¯­è¡¨ã€‚")
            return {}

        # å‡†å¤‡ç”¨äºåˆ†æçš„æ–‡æœ¬
        text_to_analyze = []
        for seg in segments:
            if seg.is_translated:
                text_to_analyze.append(f"Original: {seg.original_text}\nTranslated: {seg.translated_text}\n---")
        
        if not text_to_analyze:
            logger.warning("   - æä¾›çš„ç‰‡æ®µå‡æœªç¿»è¯‘ï¼Œæ— æ³•æå–æœ¯è¯­ã€‚")
            return {}

        content_sample = "\n".join(text_to_analyze)
        
        # æ„å»º Prompt
        original_prompt = f"""
        You are an expert linguist and terminologist.
        Analyze the following pairs of original and translated text. Identify all key, recurring, or specialized terms (like names, places, philosophical concepts, technical jargon) and create a definitive glossary.

        RULES:
        1. The output MUST be a flat JSON object.
        2. Keys are the original English terms.
        3. Values are their corresponding Chinese translations found in the text.
        4. Focus on nouns and proper nouns.
        5. Be precise. The goal is to enforce consistency.

        Example Output Format:
        {{
            "Slavoj Å½iÅ¾ek": "æ–¯æ‹‰æ²ƒçƒ­Â·é½æ³½å…‹",
            "the Real": "å®åœ¨ç•Œ",
            "Objet petit a": "å®¢ä½“å° a"
        }}

        Text to Analyze:
        <text>
        {content_sample[:8000]}
        </text>

        Return ONLY the JSON object.
        """

        try:
            if self._client is None:
                raise APIAuthenticationError("Gemini client is not configured")

            extraction_config = types.GenerateContentConfig(
                response_mime_type="application/json",
                temperature=0.2,
                max_output_tokens=8192,
            )

            response = self._client.models.generate_content(
                model=self.settings.api.gemini_model,
                contents=original_prompt,
                config=extraction_config,
            )
            raw_text = response.text.strip()
            
            # å¤„ç†è‡ªæˆ‘ä¿®æ­£
            parsed_glossary = self._handle_json_response_with_correction(
                raw_text, 
                original_prompt, 
                is_glossary_extraction=True
            )
            
            # å½’ä¸€åŒ–ä¸åŒå¯èƒ½çš„æ¨¡å‹è¾“å‡ºæ ¼å¼ä¸ºå¹³å¦çš„ {str: str} å½¢å¼
            final_glossary: Dict[str, str] = {}

            if isinstance(parsed_glossary, dict):
                for k, v in parsed_glossary.items():
                    try:
                        if k and v:
                            final_glossary[str(k).strip()] = str(v).strip()
                    except TypeError:
                        # è·³è¿‡ä¸å¯å“ˆå¸Œæˆ–éæ ‡é‡çš„é”®
                        continue

            elif isinstance(parsed_glossary, list):
                for item in parsed_glossary:
                    if isinstance(item, dict):
                        for k, v in item.items():
                            if k and v:
                                final_glossary[str(k).strip()] = str(v).strip()
                    elif isinstance(item, (list, tuple)) and len(item) == 2:
                        k, v = item
                        if k and v:
                            final_glossary[str(k).strip()] = str(v).strip()

            # æ—¥å¿—å’Œè¿”å›
            if final_glossary:
                logger.info(f"   - âœ… æˆåŠŸæå– {len(final_glossary)} ä¸ªæœ¯è¯­ã€‚")
                for k, v in list(final_glossary.items())[:5]:
                    logger.info(f"     - '{k}' -> '{v}'")
                if len(final_glossary) > 5:
                    logger.info("     - ... (æ›´å¤šæœ¯è¯­)")
                return final_glossary

            logger.warning(f"   - âš ï¸ æœ¯è¯­æå–æœªèƒ½äº§ç”Ÿæœ‰æ•ˆå­—å…¸ã€‚åŸå§‹å“åº”ç±»å‹: {type(parsed_glossary)}. åŸå§‹å“åº”ç‰‡æ®µ: {raw_text[:200]}")
            return {}
        except Exception as e:
            logger.error(f"   - âŒ æå–æœ¯è¯­è¡¨æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return {}

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=30),
        retry=retry_if_exception_type((APIError, GoogleAPICallError)),
        reraise=True
    )
    def _translate_text_batch(
        self,
        segments: SegmentList,
        context: str,
        glossary: Optional[Dict[str, str]] = None
    ) -> List[str]:
        """æ–‡æœ¬æ‰¹é‡ç¿»è¯‘ï¼ˆå¸¦é‡è¯•ï¼‰"""
        # æ„å»ºè¾“å…¥æ•°æ®
        input_data = [
            {"id": seg.segment_id, "text": seg.original_text}
            for seg in segments
        ]
        input_json = json.dumps(input_data, ensure_ascii=False)

        # æˆªå–ä¸Šä¸‹æ–‡
        safe_context = context[-self.settings.processing.max_context_length:] if context else "No Context"

        # æ ¼å¼åŒ–æœ¯è¯­è¡¨
        if glossary:
            glossary_text = "\n".join([f"- **{k}**: Must be translated as **{v}**" for k, v in glossary.items()])
        else:
            glossary_text = "N/A"

        # æ ¼å¼åŒ–æç¤ºï¼ˆå›ºå®šéƒ¨åˆ†å·²åœ¨system instructionï¼Œä»…å¡«å……åŠ¨æ€å˜é‡ï¼‰
        original_prompt = self.prompt_manager.format_text_prompt(
            context=safe_context,
            input_json=input_json,
            glossary=glossary_text
        )

        model = self._get_model_with_system_cache("Text API")
        response = model.generate_content(
            original_prompt,
            generation_config=self.generation_config
        )
            
        raw_text = response.text.strip()
        
        # è§£æå“åº”ï¼Œå¹¶å¤„ç†è‡ªæˆ‘ä¿®æ­£
        output_list = self._handle_json_response_with_correction(
            raw_text, 
            original_prompt, 
            is_text_translation=True
        )

        # æ˜ å°„ç»“æœ
        input_ids = [s.segment_id for s in segments]
        output_map = {
            int(item['id']): str(item.get('translation', ''))
            for item in output_list
            if 'id' in item and str(item['id']).isdigit()
        }

        # ç”Ÿæˆæœ€ç»ˆç»“æœ
        results = []
        for uid in input_ids:
            results.append(output_map.get(uid, "[Failed: Missing translation]"))

        return results

    def _translate_vision_batch(
        self,
        segments: SegmentList,
        context: str,
        glossary: Optional[Dict[str, str]] = None
    ) -> List[str]:
        """è§†è§‰æ‰¹é‡ç¿»è¯‘ï¼ˆä¸²è¡Œå¤„ç†ï¼‰"""
        results = []
        current_context = context[-self.settings.processing.max_context_length:] if context else ""

        for seg in segments:
            try:
                if seg.content_type == "image" and seg.image_path:
                    translation = self._call_vision_api(
                        seg.image_path,
                        current_context
                    )
                    time.sleep(self.settings.processing.vision_rate_limit_delay)
                else:
                    # é™çº§å¤„ç†æ–‡æœ¬
                    fallback_result = self._translate_text_batch([seg], current_context, glossary)
                    translation = fallback_result[0] if fallback_result else "[Fallback Failed]"

                results.append(translation)

                # æ›´æ–°ä¸Šä¸‹æ–‡
                current_context += f"\n{translation}"
                if len(current_context) > self.settings.processing.max_context_length:
                    current_context = current_context[-self.settings.processing.max_context_length:]

            except Exception as e:
                logger.error(f"âŒ Visionç¿»è¯‘å¤±è´¥ (segment {seg.segment_id}): {e}")
                results.append(f"[Failed: {str(e)}]")
                continue

        return results

    def _call_vision_api(self, img_path: str, context: str) -> str:
        """è°ƒç”¨è§†è§‰ APIï¼ˆæ”¯æŒ Gemini Cachingï¼‰"""
        try:
            # ä½¿ç”¨ prompt_manager æ ¼å¼åŒ–æç¤º
            original_prompt = self.prompt_manager.format_vision_prompt(context)

            mime_type, _ = mimetypes.guess_type(img_path)
            mime_type = mime_type or "image/png"
            with open(img_path, "rb") as f:
                image_bytes = f.read()

            image_part = types.Part.from_bytes(data=image_bytes, mime_type=mime_type)

            vision_config = {
                "temperature": self.generation_config["temperature"],
                "top_p": self.generation_config["top_p"],
                "max_output_tokens": self.generation_config["max_output_tokens"],
                "response_mime_type": "application/json",
            }

            model = self._get_model_with_system_cache("Vision API")
            response = model.generate_content(
                [original_prompt, image_part],
                generation_config=vision_config,
            )

            raw_text = (response.text or "").strip()

            # è§£æ JSON å¹¶æå– "translation" å­—æ®µï¼Œå¤„ç†è‡ªæˆ‘ä¿®æ­£
            parsed_json = self._handle_json_response_with_correction(
                raw_text,
                original_prompt,
                is_vision_translation=True,
                image_part=image_part,
            )

            if isinstance(parsed_json, dict) and "translation" in parsed_json:
                return parsed_json["translation"]

            logger.error(
                "âŒ Vision API did not return valid JSON with a 'translation' key even after correction. "
                f"Got: {raw_text[:200]}"
            )
            return "[Failed: Invalid JSON Response]"

        except Exception as e:
            logger.error(f"âŒ Vision APIè°ƒç”¨å¤±è´¥ for {img_path}: {e}")
            return f"[Failed: {str(e)}]"

    def _handle_json_response_with_correction(
        self,
        raw_text: str,
        original_prompt: str,
        is_title_translation: bool = False,
        is_glossary_extraction: bool = False,
        is_text_translation: bool = False,
        is_vision_translation: bool = False,
        image_part: Optional[Any] = None
    ) -> Any:
        """
        å¤„ç† JSON å“åº”ï¼Œä¼˜åŒ–è§£æé¡ºåºï¼š
        1. å…ˆå°è¯•æ ‡å‡†JSONè§£æ
        2. å°è¯•å»é™¤ä»£ç å—åè§£æ
        3. å°è¯•æ­£åˆ™è¡¨è¾¾å¼å…œåº•è§£æ
        4. åªæœ‰ä»¥ä¸Šå…¨éƒ¨å¤±è´¥åï¼Œæ‰è¯·æ±‚æ¨¡å‹è‡ªæˆ‘ä¿®æ­£
        """
        final_parsed_data = None
        current_raw_text = raw_text

        for attempt in range(self.settings.processing.json_repair_retries + 1):
            # ========== é˜¶æ®µ1ï¼šæ ‡å‡†JSONè§£æ ==========
            try:
                final_parsed_data = self._repair_json_content(current_raw_text)
                logger.debug("âœ… æ ‡å‡†JSONè§£ææˆåŠŸ")
                return final_parsed_data
            except JSONParseError as e:
                logger.debug(f"âš ï¸ æ ‡å‡†JSONè§£æå¤±è´¥: {e}")
            
            # ========== é˜¶æ®µ2ï¼šæ­£åˆ™è¡¨è¾¾å¼å…œåº•è§£æ ==========
            try:
                if is_text_translation:
                    fallback_result = self._regex_fallback(current_raw_text)
                    if fallback_result and len(fallback_result) > 0:
                        # æ£€æŸ¥æ˜¯å¦æ˜¯çœŸæ­£çš„ç¿»è¯‘å¤±è´¥æ ‡ç­¾
                        if fallback_result[0].get("translation") != "[Translation Failed - JSON Parse Error]":
                            logger.info(f"âœ… æ­£åˆ™è¡¨è¾¾å¼è§£ææˆåŠŸï¼Œæå– {len(fallback_result)} æ¡ç¿»è¯‘")
                            return fallback_result
                elif is_title_translation or is_glossary_extraction:
                    fallback_result = self._regex_fallback_for_dict_like(current_raw_text)
                    if fallback_result:
                        logger.info(f"âœ… æ­£åˆ™è¡¨è¾¾å¼è§£ææˆåŠŸï¼ˆå­—å…¸æ ¼å¼ï¼‰")
                        return fallback_result
            except Exception as e:
                logger.debug(f"âš ï¸ æ­£åˆ™è¡¨è¾¾å¼è§£æå¤±è´¥: {e}")
            
            # ========== é˜¶æ®µ3ï¼šæ¨¡å‹è‡ªæˆ‘ä¿®æ­£ï¼ˆä»…å½“å‰ä¸¤é˜¶æ®µéƒ½å¤±è´¥æ—¶ï¼‰ ==========
            if attempt < self.settings.processing.json_repair_retries:
                logger.warning(f"âš ï¸ JSON è§£æå¤±è´¥ (å°è¯• {attempt+1}/{self.settings.processing.json_repair_retries + 1}): [MEDIUM] æ ‡å‡†è§£æå’Œæ­£åˆ™è§£æå‡å¤±è´¥")
                logger.info(f"ğŸ’¡ å»ºè®®: å°è¯•è¯·æ±‚æ¨¡å‹è‡ªæˆ‘ä¿®æ­£ (å‰©ä½™ {self.settings.processing.json_repair_retries - attempt} æ¬¡)")
                logger.info("ğŸ”„ å°è¯•è¯·æ±‚æ¨¡å‹è‡ªæˆ‘ä¿®æ­£...")
                
                corrected_data = self._attempt_json_self_correction(
                    original_prompt=original_prompt,
                    failed_json=current_raw_text,
                    error_message="JSONè§£æå’Œæ­£åˆ™è§£æå‡å¤±è´¥",
                    retries_left=self.settings.processing.json_repair_retries - attempt,
                    is_vision=is_vision_translation,
                    image_part=image_part
                )
                
                if corrected_data:
                    logger.info("âœ… æ¨¡å‹è‡ªæˆ‘ä¿®æ­£æˆåŠŸ")
                    final_parsed_data = corrected_data
                    return final_parsed_data
                else:
                    logger.warning("âŒ æ¨¡å‹è‡ªæˆ‘ä¿®æ­£å¤±è´¥")
                    # é‡ç½®ä¸ºåŸå§‹æ–‡æœ¬ï¼Œç»§ç»­ä¸‹ä¸€è½®å°è¯•
                    current_raw_text = raw_text
            else:
                logger.warning("âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ‰€æœ‰è§£ææ–¹æ³•å‡å¤±è´¥ã€‚")
                break

        # ========== æœ€ç»ˆå…œåº•ï¼šè¿”å›é”™è¯¯æ ‡è®° ==========
        logger.error(f"âŒ æ‰€æœ‰è§£ææ–¹æ³•å¤±è´¥ï¼ˆæ ‡å‡†JSON + æ­£åˆ™ + æ¨¡å‹ä¿®æ­£ï¼‰")
        
        if is_text_translation:
            logger.warning("âš ï¸ è¿”å›ç¿»è¯‘å¤±è´¥æ ‡ç­¾ä»¥ä¿è¯è¾“å‡ºæ–‡ä»¶å®Œæ•´æ€§")
            return [{"id": 1, "translation": "[Translation Failed - All Parsing Methods Exhausted]"}]
        elif is_title_translation or is_glossary_extraction:
            return {}
        elif is_vision_translation:
            return {}
        
        return final_parsed_data

    def _parse_json_response(self, text: str) -> List[Dict[str, Any]]:
        """è§£ææ–‡æœ¬ç¿»è¯‘çš„ JSON å“åº”ï¼Œæ”¯æŒå¤šç§æ ¼å¼"""
        try:
            # è°ƒç”¨æ–°çš„å¤„ç†å‡½æ•°ï¼Œå¹¶æ˜ç¡®è¿™æ˜¯æ–‡æœ¬ç¿»è¯‘åœºæ™¯
            # æ³¨æ„: è¿™é‡Œ original_prompt ä¼ é€’ç©ºå­—ç¬¦ä¸²ï¼Œå› ä¸º _parse_json_response ä¹‹å‰æ²¡æœ‰ç›´æ¥çš„ prompt ä¿¡æ¯ã€‚
            # åªæœ‰å½“åŸå§‹ API è°ƒç”¨å¤±è´¥æ—¶ï¼Œ_handle_json_response_with_correction æ‰ä¼šä½¿ç”¨åˆ° original_prompt è¿›è¡Œè‡ªæˆ‘ä¿®æ­£ã€‚
            # å¦‚æœæ˜¯ _parse_json_response ç‹¬ç«‹è°ƒç”¨ï¼ˆä¾‹å¦‚ä»ç¼“å­˜è¯»å–ï¼‰ï¼Œé‚£ä¹ˆ original_prompt ç¡®å®æ˜¯æœªçŸ¥çš„ã€‚
            result = self._handle_json_response_with_correction(text, "", is_text_translation=True)
            if isinstance(result, list):
                return result
            elif isinstance(result, dict) and 'translations' in result:
                return result['translations']
            else:
                return []
        except Exception as e:
            logger.error(f"âŒ æœ€ç»ˆJSONè§£æå¤±è´¥ï¼ŒåŒ…æ‹¬ä¿®æ­£å’Œæ­£åˆ™å›é€€: {e}")
            return []

    def _repair_json_content(self, text: str) -> Any:
        """ä¿®å¤ JSON å­—ç¬¦ä¸² (åªè¿›è¡Œä»£ç å—å»é™¤ï¼Œä¸è¿›è¡Œé«˜çº§å­—ç¬¦ä¸²ä¿®å¤)"""
        # å»é™¤ Markdown ä»£ç å—
        pattern = r'^```(?:json)?\s*(.*)\s*```$'
        match = re.search(pattern, text, re.DOTALL)
        if match:
            text = match.group(1)

        try:
            return json.loads(text)
        except json.JSONDecodeError as e:
            # ä¸å†è¿›è¡Œå†…éƒ¨é«˜çº§ä¿®å¤ï¼Œç›´æ¥æŠ›å‡ºï¼Œç”±ä¸Šå±‚å¤„ç†è‡ªæˆ‘ä¿®æ­£
            raise JSONParseError(f"Initial JSON parse failed: {e}")

    def _regex_fallback(self, text: str) -> List[Dict[str, Any]]:
        """æ­£åˆ™è¡¨è¾¾å¼å…œåº•è§£æ"""
        logger.info("ğŸ”„ Using regex fallback for JSON parsing...")

        # ç­–ç•¥1: æ ‡å‡†JSONæ ¼å¼
        pattern = r'"id":\s*(\d+),\s*"translation":\s*"(.*?)(?<!\\)"(?=\s*\}|\s*,)'
        matches = re.findall(pattern, text, re.DOTALL)

        if not matches:
            # ç­–ç•¥2: å•å¼•å·æ ¼å¼
            pattern_sq = r"'id':\s*(\d+),\s*'translation':\s*'(.*?)'(?=\s*\}|\s*,)"
            matches = re.findall(pattern_sq, text, re.DOTALL)

        if not matches:
            # ç­–ç•¥3: æ›´å®½æ¾çš„åŒ¹é…ï¼ˆå¤„ç†ä¸å®Œæ•´çš„JSONï¼‰
            pattern_loose = r'"id":\s*(\d+).*?"translation":\s*"(.*?)"'
            matches = re.findall(pattern_loose, text, re.DOTALL)

        if not matches:
            # ç­–ç•¥4: æåº¦å®½æ¾çš„åŒ¹é…
            pattern_ultra = r'id["\s:]+(\d+).*?translation["\s:]+["\']([^"\']*?)["\']'
            matches = re.findall(pattern_ultra, text, re.DOTALL | re.IGNORECASE)

        if not matches:
            logger.error(f"âŒ Regex fallback failed completely. Original text: {repr(text[:500])}")
            # è¿”å›ç¿»è¯‘å¤±è´¥çš„æ ‡ç­¾ï¼Œç¡®ä¿è‡³å°‘èƒ½ç”Ÿæˆå®Œæ•´çš„è¾“å‡ºæ–‡ä»¶
            logger.warning("âš ï¸ Returning translation failure tag to ensure output file integrity.")
            return [{"id": 1, "translation": "[Translation Failed - JSON Parse Error]"}]

        logger.debug(f"âœ… Regex found {len(matches)} matches.")
        return [{"id": int(mid), "translation": mtext.replace('\\"', '"').replace("\\'", "'")} for mid, mtext in matches]


# ========================================================================
# å¼‚æ­¥ç¿»è¯‘å®¢æˆ·ç«¯
# ========================================================================

class AsyncGeminiTranslator(BaseAsyncTranslator):
    """å¼‚æ­¥ Gemini ç¿»è¯‘å®¢æˆ·ç«¯ï¼Œæ”¯æŒå¹¶å‘æ‰¹é‡ç¿»è¯‘ã€‚
    
    ç»§æ‰¿è‡ª BaseAsyncTranslatorï¼Œå®ç° Gemini çš„å¼‚æ­¥ç¿»è¯‘é€»è¾‘ã€‚
    """

    def __init__(self, base_translator: GeminiTranslator):
        """
        Args:
            base_translator: åŸºç¡€çš„ GeminiTranslator å®ä¾‹ï¼Œç”¨äºå¤ç”¨é…ç½®å’ŒåŒæ­¥æ–¹æ³•
        """
        # è°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°
        super().__init__(base_translator)
        
        self.generation_config = base_translator.generation_config
        self.cache_refs = base_translator.cache_refs
        self.prompt_manager = base_translator.prompt_manager  # å¤ç”¨ prompt_manager
        
        # åˆ›å»ºçº¿ç¨‹æ± ç”¨äº I/O ç»‘å®šçš„åŒæ­¥æ“ä½œ
        self.executor = ThreadPoolExecutor(max_workers=10)
        
    async def translate_text_batch_async(
        self,
        segments: SegmentList,
        context: str,
        glossary: Optional[Dict[str, str]] = None
    ) -> List[str]:
        """
        å¼‚æ­¥æ‰¹é‡ç¿»è¯‘æ–‡æœ¬segment
        
        å°†segmentsåˆ†æˆæ›´å°çš„chunkå¹¶å‘ç¿»è¯‘ï¼Œå¤§å¹…æå‡é€Ÿåº¦
        """
        if not segments:
            return []
        
        logger.info(f"ğŸš€ ä½¿ç”¨å¼‚æ­¥æ¨¡å¼ç¿»è¯‘ {len(segments)} ä¸ªæ–‡æœ¬æ®µ...")
        
        # å°† segments åˆ†æˆå°æ‰¹æ¬¡ (é¿å…å•ä¸ªè¯·æ±‚è¿‡å¤§)
        chunk_size = min(5, self.settings.processing.batch_size)
        chunks = [segments[i:i + chunk_size] for i in range(0, len(segments), chunk_size)]
        
        # å¹¶å‘ç¿»è¯‘æ‰€æœ‰ chunk
        tasks = [
            self._translate_single_chunk_async(chunk, context, glossary)
            for chunk in chunks
        ]
        
        # ç­‰å¾…æ‰€æœ‰ç¿»è¯‘å®Œæˆ
        chunk_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # åˆå¹¶ç»“æœ
        results = []
        for chunk_result in chunk_results:
            if isinstance(chunk_result, Exception):
                logger.error(f"âŒ Chunk ç¿»è¯‘å¤±è´¥: {chunk_result}")
                # ä¸ºå¤±è´¥çš„chunkå¡«å……å¤±è´¥æ ‡è®°
                results.extend([f"[Failed: {str(chunk_result)}]"] * chunk_size)
            else:
                results.extend(chunk_result)
        
        logger.info(f"âœ… å¼‚æ­¥ç¿»è¯‘å®Œæˆï¼ŒæˆåŠŸ {len([r for r in results if not r.startswith('[Failed')])} / {len(segments)}")
        return results[:len(segments)]  # ç¡®ä¿è¿”å›æ­£ç¡®æ•°é‡çš„ç»“æœ

    async def _translate_single_chunk_async(
        self,
        segments: SegmentList,
        context: str,
        glossary: Optional[Dict[str, str]] = None
    ) -> List[str]:
        """å¼‚æ­¥ç¿»è¯‘å•ä¸ª chunk"""
        
        # å‡†å¤‡è¾“å…¥æ•°æ®
        input_data = [
            {
                "id": seg.segment_id,
                "original": seg.original_text,
                "context": (context or "No Context")[-self.settings.processing.max_context_length:]
            }
            for seg in segments
        ]
        input_json = json.dumps(input_data, ensure_ascii=False)
        
        # æ ¼å¼åŒ–æœ¯è¯­è¡¨
        glossary_text = "N/A"
        if glossary:
            glossary_text = "\n".join([f"- **{k}**: Must be translated as **{v}**" for k, v in glossary.items()])
        
        # æˆªå–ä¸Šä¸‹æ–‡
        safe_context = context[-self.settings.processing.max_context_length:] if context else "No Context"
        
        # æ ¼å¼åŒ– Prompt - ä½¿ç”¨ prompt_manager
        original_prompt = self.prompt_manager.format_text_prompt(
            context=safe_context,
            input_json=input_json,
            glossary=glossary_text
        )
        
        # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥çš„ API è°ƒç”¨ï¼ˆä½¿ç”¨ç¼“å­˜å¦‚æœå¯ç”¨ï¼‰
        loop = asyncio.get_event_loop()
        
        def _call_with_cache():
            model = self.base._get_model_with_system_cache("Async Text API")
            return model.generate_content(
                original_prompt,
                generation_config=self.generation_config
            )
        
        response = await loop.run_in_executor(self.executor, _call_with_cache)
        
        raw_text = response.text.strip()
        
        # è§£æå“åº”ï¼ˆå¤ç”¨åŒæ­¥æ–¹æ³•ï¼‰
        output_list = self.base._handle_json_response_with_correction(
            raw_text,
            original_prompt,
            is_text_translation=True
        )
        
        # æ˜ å°„ç»“æœ
        input_ids = [s.segment_id for s in segments]
        output_map = {
            int(item['id']): str(item.get('translation', ''))
            for item in output_list
            if 'id' in item and str(item['id']).isdigit()
        }
        
        # ç”Ÿæˆæœ€ç»ˆç»“æœ
        results = [output_map.get(uid, "[Translation Failed]") for uid in input_ids]
        return results

    async def translate_vision_batch_async(
        self,
        segments: SegmentList,
        context: str,
        glossary: Optional[Dict[str, str]] = None
    ) -> List[str]:
        """
        å¼‚æ­¥æ‰¹é‡ç¿»è¯‘åŒ…å«å›¾åƒçš„segment
        
        ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°ï¼Œé¿å…è§¦å‘ Gemini é€Ÿç‡é™åˆ¶
        """
        if not segments:
            return []
        
        logger.info(f"ğŸ–¼ï¸ ä½¿ç”¨å¼‚æ­¥æ¨¡å¼ç¿»è¯‘ {len(segments)} ä¸ªè§†è§‰æ®µ...")
        
        # åˆ›å»ºä¿¡å·é‡ï¼Œé™åˆ¶å¹¶å‘è§†è§‰ API è°ƒç”¨æ•°
        semaphore = asyncio.Semaphore(3)  # æœ€å¤šåŒæ—¶3ä¸ªè§†è§‰è¯·æ±‚
        
        # åˆ›å»ºç¿»è¯‘ä»»åŠ¡
        tasks = []
        for seg in segments:
            if seg.content_type == "image" and seg.image_path:
                task = self._call_vision_api_async(
                    seg.image_path,
                    context,
                    semaphore
                )
            else:
                # æ–‡æœ¬é™çº§å¤„ç†
                task = self._translate_text_fallback_async(seg, context, glossary)
            
            tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰ç¿»è¯‘å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†å¼‚å¸¸ç»“æœ
        final_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"âŒ è§†è§‰ç¿»è¯‘å¤±è´¥ (segment {segments[i].segment_id}): {result}")
                final_results.append(f"[Failed: {str(result)}]")
            else:
                final_results.append(result)
        
        logger.info(f"âœ… å¼‚æ­¥è§†è§‰ç¿»è¯‘å®Œæˆ")
        return final_results

    async def _call_vision_api_async(
        self,
        img_path: str,
        context: str,
        semaphore: asyncio.Semaphore
    ) -> str:
        """å¼‚æ­¥è°ƒç”¨è§†è§‰ APIï¼Œä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘"""
        
        async with semaphore:  # é™åˆ¶å¹¶å‘æ•°
            loop = asyncio.get_event_loop()
            
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œ I/O ç»‘å®šçš„å›¾åƒå¤„ç†
            def _process_vision():
                # ç›´æ¥è°ƒç”¨åŸºç¡€ç¿»è¯‘å™¨çš„è§†è§‰APIæ–¹æ³•
                return self.base._call_vision_api(img_path, context)
            
            result = await loop.run_in_executor(self.executor, _process_vision)
            
            # æ·»åŠ å»¶è¿Ÿé¿å…é€Ÿç‡é™åˆ¶
            await asyncio.sleep(self.settings.processing.vision_rate_limit_delay)
            
            return result

    async def _translate_text_fallback_async(
        self,
        segment: ContentSegment,
        context: str,
        glossary: Optional[Dict[str, str]]
    ) -> str:
        """å¼‚æ­¥æ–‡æœ¬é™çº§å¤„ç†"""
        loop = asyncio.get_event_loop()
        
        def _sync_fallback():
            results = self.base._translate_text_batch(
                [segment],
                context,
                glossary
            )
            return results[0] if results else "[Fallback Failed]"
        
        return await loop.run_in_executor(self.executor, _sync_fallback)

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.executor.shutdown(wait=True)
        logger.info("ğŸ§¹ å¼‚æ­¥ç¿»è¯‘å™¨å·²æ¸…ç†èµ„æº")


# ========================================================================
# OpenAI-compatible (DeepSeek) translator
# ========================================================================


class OpenAICompatibleTranslator(BaseTranslator):
    """OpenAI-compatible ç¿»è¯‘å®¢æˆ·ç«¯ï¼ˆDeepSeek API å…¼å®¹ OpenAI æ ¼å¼ï¼‰ã€‚

    DeepSeek docs:
      - base_url: https://api.deepseek.com (or https://api.deepseek.com/v1)
      - endpoint: POST /chat/completions

    Env vars are provided via Settings.api:
      - API_OPENAI_API_KEY / OPENAI_API_KEY / DEEPSEEK_API_KEY
      - OPENAI_BASE_URL / DEEPSEEK_BASE_URL
      - OPENAI_MODEL / DEEPSEEK_MODEL
    """

    def __init__(self, settings: Settings):
        super().__init__(settings)
        self.prompt_manager = PromptManager(settings)
        self._async_translator: Optional[AsyncOpenAICompatibleTranslator] = None

        self.api_key: Optional[str] = settings.api.openai_api_key
        self.base_url: str = settings.api.openai_base_url
        self.model: str = settings.api.openai_model

        if not self.api_key:
            raise APIAuthenticationError(
                "OpenAI-compatible API key is missing. Set API_OPENAI_API_KEY (or OPENAI_API_KEY/DEEPSEEK_API_KEY).",
                context={"setting": "API_OPENAI_API_KEY"},
            )

    @property
    def async_translator(self) -> 'AsyncOpenAICompatibleTranslator':
        if self._async_translator is None:
            self._async_translator = AsyncOpenAICompatibleTranslator(self)
        return self._async_translator

    def translate_batch(
        self,
        segments: SegmentList,
        context: str = "",
        glossary: Optional[Dict[str, str]] = None,
    ) -> List[str]:
        if not segments:
            return []

        has_image = any(seg.content_type == "image" for seg in segments)
        if has_image:
            return self._translate_vision_batch(segments, context)
        return self._translate_text_batch(segments, context, glossary)

    def translate_titles(self, titles: List[str]) -> TranslationMap:
        if not titles:
            return {}

        input_json_str = json.dumps(titles, ensure_ascii=False)
        original_prompt = self.prompt_manager.format_title_prompt(input_json_str)

        raw_text = self._chat_completions(
            system_instruction=self.prompt_manager.get_system_instruction(use_vision=False),
            user_content=original_prompt,
        )

        parsed_data = self._handle_json_response_with_repair(
            raw_text=raw_text,
            original_prompt=original_prompt,
            is_dict_like=True,
        )

        if isinstance(parsed_data, dict):
            return {str(k): str(v) for k, v in parsed_data.items() if isinstance(v, str)}

        if isinstance(parsed_data, list) and parsed_data:
            result: Dict[str, str] = {}
            for item in parsed_data:
                if isinstance(item, dict):
                    for k, v in item.items():
                        if k != 'id':
                            result[str(k)] = str(v)
            return result

        return {}

    def extract_glossary(self, segments: SegmentList) -> Dict[str, str]:
        if not segments:
            return {}

        text_to_analyze: List[str] = []
        for seg in segments:
            if seg.is_translated:
                text_to_analyze.append(
                    f"Original: {seg.original_text}\nTranslated: {seg.translated_text}\n---"
                )

        if not text_to_analyze:
            return {}

        content_sample = "\n".join(text_to_analyze)
        original_prompt = f"""
You are an expert linguist and terminologist.
Analyze the following pairs of original and translated text. Identify all key, recurring, or specialized terms (like names, places, philosophical concepts, technical jargon) and create a definitive glossary.

RULES:
1. The output MUST be a flat JSON object.
2. Keys are the original English terms.
3. Values are their corresponding Chinese translations found in the text.
4. Focus on nouns and proper nouns.
5. Be precise. The goal is to enforce consistency.

Text to Analyze:
<text>
{content_sample[:8000]}
</text>

Return ONLY the JSON object.
""".strip()

        raw_text = self._chat_completions(
            system_instruction="You output JSON only.",
            user_content=original_prompt,
        )

        parsed = self._handle_json_response_with_repair(
            raw_text=raw_text,
            original_prompt=original_prompt,
            is_dict_like=True,
        )

        final_glossary: Dict[str, str] = {}
        if isinstance(parsed, dict):
            for k, v in parsed.items():
                if k and v:
                    final_glossary[str(k).strip()] = str(v).strip()
        return final_glossary

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=20),
        retry=retry_if_exception_type((APIError,)),
        reraise=True,
    )
    def _translate_text_batch(
        self,
        segments: SegmentList,
        context: str,
        glossary: Optional[Dict[str, str]] = None,
    ) -> List[str]:
        input_data = [{"id": seg.segment_id, "text": seg.original_text} for seg in segments]
        input_json = json.dumps(input_data, ensure_ascii=False)

        safe_context = context[-self.settings.processing.max_context_length:] if context else "No Context"

        glossary_text = "N/A"
        if glossary:
            glossary_text = "\n".join(
                [f"- **{k}**: Must be translated as **{v}**" for k, v in glossary.items()]
            )

        original_prompt = self.prompt_manager.format_text_prompt(
            context=safe_context,
            input_json=input_json,
            glossary=glossary_text,
        )

        raw_text = self._chat_completions(
            system_instruction=self.prompt_manager.get_system_instruction(use_vision=False),
            user_content=original_prompt,
        )

        output_list = self._handle_json_response_with_repair(
            raw_text=raw_text,
            original_prompt=original_prompt,
            is_text_translation=True,
        )

        input_ids = [s.segment_id for s in segments]
        output_map = {
            int(item['id']): str(item.get('translation', ''))
            for item in output_list
            if isinstance(item, dict) and 'id' in item and str(item['id']).isdigit()
        }
        return [output_map.get(uid, "[Failed: Missing translation]") for uid in input_ids]

    def _translate_vision_batch(self, segments: SegmentList, context: str) -> List[str]:
        results: List[str] = []
        current_context = context[-self.settings.processing.max_context_length:] if context else ""

        for seg in segments:
            if seg.content_type == "image" and seg.image_path:
                results.append(self._call_vision_api(seg.image_path, current_context))
            else:
                fallback = self._translate_text_batch([seg], current_context, glossary=None)
                results.append(fallback[0] if fallback else "[Fallback Failed]")

            current_context += f"\n{results[-1]}"
            if len(current_context) > self.settings.processing.max_context_length:
                current_context = current_context[-self.settings.processing.max_context_length:]

        return results

    def _call_vision_api(self, img_path: str, context: str) -> str:
        original_prompt = self.prompt_manager.format_vision_prompt(context)

        try:
            with open(img_path, 'rb') as f:
                b64 = base64.b64encode(f.read()).decode('ascii')
            image_url = f"data:image/png;base64,{b64}"

            raw_text = self._chat_completions(
                system_instruction=self.prompt_manager.get_system_instruction(use_vision=True),
                user_content=[
                    {"type": "text", "text": original_prompt},
                    {"type": "image_url", "image_url": {"url": image_url}},
                ],
            )

            parsed = self._handle_json_response_with_repair(
                raw_text=raw_text,
                original_prompt=original_prompt,
                is_dict_like=True,
            )
            if isinstance(parsed, dict) and 'translation' in parsed:
                return str(parsed['translation'])
            return "[Failed: Invalid JSON Response]"
        except Exception as e:
            logger.error(f"âŒ OpenAI-compatible Vision API è°ƒç”¨å¤±è´¥ for {img_path}: {e}")
            return f"[Failed: {str(e)}]"

    def _build_chat_completions_url(self) -> str:
        base = (self.base_url or '').rstrip('/')
        # DeepSeek supports both https://api.deepseek.com and https://api.deepseek.com/v1
        if base.endswith('/v1'):
            return base + '/chat/completions'
        return base + '/chat/completions'

    def _chat_completions(self, system_instruction: str, user_content: Any) -> str:
        payload = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": system_instruction},
                {"role": "user", "content": user_content},
            ],
            "temperature": 0.2,
            "stream": False,
        }

        url = self._build_chat_completions_url()
        data = json.dumps(payload).encode('utf-8')
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {self.api_key}',
        }

        req = request.Request(url, data=data, headers=headers, method='POST')
        try:
            with request.urlopen(req, timeout=self.settings.processing.request_timeout) as resp:
                resp_text = resp.read().decode('utf-8')
        except error.HTTPError as e:
            body = e.read().decode('utf-8', errors='replace') if hasattr(e, 'read') else ''
            raise APIError(f"OpenAI-compatible HTTPError: {e.code} {e.reason} {body[:200]}")
        except error.URLError as e:
            raise APITimeoutError(f"OpenAI-compatible request failed: {e}")
        except TimeoutError as e:
            raise APITimeoutError(f"OpenAI-compatible request timeout: {e}")

        try:
            parsed = json.loads(resp_text)
            content = parsed['choices'][0]['message']['content']
            if not isinstance(content, str):
                return json.dumps(content, ensure_ascii=False)
            return content.strip()
        except Exception as e:
            raise APIError(f"OpenAI-compatible response parse failed: {e}")

    def _strip_code_fences(self, text: str) -> str:
        pattern = r'^```(?:json)?\s*(.*)\s*```$'
        match = re.search(pattern, text, re.DOTALL)
        return match.group(1) if match else text

    def _handle_json_response_with_repair(
        self,
        raw_text: str,
        original_prompt: str,
        *,
        is_text_translation: bool = False,
        is_dict_like: bool = False,
    ) -> Any:
        current = raw_text

        for _ in range(self.settings.processing.json_repair_retries + 1):
            try:
                current = self._strip_code_fences(current)
                return json.loads(current)
            except json.JSONDecodeError as e:
                last_err = str(e)

            if is_text_translation:
                try:
                    fallback = self._regex_fallback_for_list(current)
                    if fallback:
                        return fallback
                except Exception:
                    pass

            repair_prompt = self.prompt_manager.format_json_repair_prompt(
                original_prompt=original_prompt,
                broken_json=current,
                error_details=last_err,
            )
            current = self._chat_completions(
                system_instruction=self.prompt_manager.get_system_instruction(use_vision=False),
                user_content=repair_prompt,
            )

        if is_text_translation:
            return [{"id": 1, "translation": "[Translation Failed - All Parsing Methods Exhausted]"}]
        if is_dict_like:
            return {}
        raise JSONParseError("Failed to parse JSON after repair attempts")

    def _regex_fallback_for_list(self, text: str) -> List[Dict[str, Any]]:
        pattern = r'"id":\s*(\d+),\s*"translation":\s*"(.*?)(?<!\\)"(?=\s*\}|\s*,)'
        matches = re.findall(pattern, text, re.DOTALL)
        return [
            {"id": int(mid), "translation": mtext.replace('\\"', '"').replace("\\'", "'")}
            for mid, mtext in matches
        ]


class AsyncOpenAICompatibleTranslator(BaseAsyncTranslator):
    """å¼‚æ­¥ OpenAI-compatible ç¿»è¯‘å™¨ï¼ˆçº¿ç¨‹æ± åŒ…è£…ï¼ŒåŒæ­¥HTTPè¯·æ±‚å¹¶å‘æ‰§è¡Œï¼‰ã€‚"""

    def __init__(self, base_translator: OpenAICompatibleTranslator):
        super().__init__(base_translator)
        self.executor = ThreadPoolExecutor(max_workers=10)

    async def translate_text_batch_async(
        self,
        segments: SegmentList,
        context: str,
        glossary: Optional[Dict[str, str]] = None,
    ) -> List[str]:
        if not segments:
            return []

        loop = asyncio.get_event_loop()

        def _sync():
            return self.base._translate_text_batch(segments, context, glossary)

        return await loop.run_in_executor(self.executor, _sync)

    async def translate_vision_batch_async(
        self,
        segments: SegmentList,
        context: str,
        glossary: Optional[Dict[str, str]] = None,
    ) -> List[str]:
        if not segments:
            return []

        loop = asyncio.get_event_loop()

        def _sync_one(seg: ContentSegment) -> str:
            if seg.content_type == 'image' and seg.image_path:
                return self.base._call_vision_api(seg.image_path, context)
            fallback = self.base._translate_text_batch([seg], context, glossary)
            return fallback[0] if fallback else "[Fallback Failed]"

        tasks = [loop.run_in_executor(self.executor, _sync_one, seg) for seg in segments]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        final: List[str] = []
        for r in results:
            if isinstance(r, Exception):
                final.append(f"[Failed: {str(r)}]")
            else:
                final.append(r)
        return final

    def cleanup(self):
        self.executor.shutdown(wait=True)
        logger.info("ğŸ§¹ OpenAI-compatible å¼‚æ­¥ç¿»è¯‘å™¨å·²æ¸…ç†èµ„æº")
