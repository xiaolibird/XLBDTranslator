"""
ç¿»è¯‘æ ¸å¿ƒç®¡ç†æ¨¡å—
æ•´åˆï¼šæ–­ç‚¹ç»­ä¼ ã€ç¼“å­˜æŒä¹…åŒ–ç®¡ç†ã€Prompt ç®¡ç†
"""
import json
import time
import hashlib
import threading
from pathlib import Path
from typing import Dict, Any, Optional, List, Set, TYPE_CHECKING
from datetime import datetime, timedelta

from ..core.schema import ContentSegment, SegmentList
from ..utils.logger import get_logger

if TYPE_CHECKING:
    from ..core.schema import Settings, TranslationMode

logger = get_logger(__name__)


# ========================================================================
# 1. æ–­ç‚¹ç»­ä¼ ç®¡ç†
# ========================================================================

class CheckpointManager:
    """ç¿»è¯‘è¿›åº¦æ£€æŸ¥ç‚¹ç®¡ç†å™¨"""
    
    def __init__(self, settings: 'Settings'):
        """
        Args:
            settings: å…¨å±€è®¾ç½®å¯¹è±¡ï¼ˆä»document_pathè‡ªåŠ¨è®¡ç®—doc_hashï¼‰
        """
        self.settings = settings
        
        # ä»settingsè‡ªåŠ¨è®¡ç®—doc_hash
        from ..utils.file import get_file_hash
        doc_hash = get_file_hash(settings.files.document_path) if settings.files.document_path else "unknown"
        
        # ä» settings ä¸­è·å–é¡¹ç›®ç›®å½•
        base_dir = Path(settings.files.output_base_dir) if isinstance(settings.files.output_base_dir, str) else settings.files.output_base_dir
        self.project_dir = base_dir / doc_hash
        self.checkpoint_file = self.project_dir / "checkpoint.json"
        self.checkpoint_data: Dict = {}
        
        # è®°å½•checkpointæ–‡ä»¶è·¯å¾„ï¼Œä¾¿äºæ’æŸ¥
        logger.info(f"ğŸ“ Checkpointæ–‡ä»¶è·¯å¾„: {self.checkpoint_file.absolute()}")
        
        self._load_checkpoint()
    
    def _load_checkpoint(self):
        """åŠ è½½ç°æœ‰çš„æ£€æŸ¥ç‚¹æ–‡ä»¶"""
        if self.checkpoint_file.exists():
            try:
                with open(self.checkpoint_file, 'r', encoding='utf-8') as f:
                    self.checkpoint_data = json.load(f)
                completed_count = len(self.checkpoint_data.get('completed_segments', []))
                logger.info(f"ğŸ“‚ åŠ è½½æ£€æŸ¥ç‚¹: å·²å®Œæˆ {completed_count} ä¸ªæ®µè½")
            except Exception as e:
                logger.warning(f"âš ï¸ åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
                self.checkpoint_data = {}
        else:
            logger.info("ğŸ†• æœªå‘ç°æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œå°†ä»å¤´å¼€å§‹ç¿»è¯‘")
            self.checkpoint_data = {
                'start_time': datetime.now().isoformat(),
                'completed_segments': [],
                'failed_segments': [],
                'total_segments': 0,
                'last_update': None
            }
    
    def save_checkpoint(self):
        """ä¿å­˜å½“å‰æ£€æŸ¥ç‚¹åˆ°æ–‡ä»¶"""
        try:
            self.project_dir.mkdir(parents=True, exist_ok=True)
            self.checkpoint_data['last_update'] = datetime.now().isoformat()
            
            with open(self.checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(self.checkpoint_data, f, ensure_ascii=False, indent=2)
            
            completed = len(self.checkpoint_data.get('completed_segments', []))
            total = self.checkpoint_data.get('total_segments', 0)
            logger.debug(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {completed}/{total}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
    
    def mark_segment_completed(self, segment_id: int):
        """æ ‡è®°ä¸€ä¸ªæ®µè½ä¸ºå·²å®Œæˆ"""
        if 'completed_segments' not in self.checkpoint_data:
            self.checkpoint_data['completed_segments'] = []
        if segment_id not in self.checkpoint_data['completed_segments']:
            self.checkpoint_data['completed_segments'].append(segment_id)
    
    def remove_from_completed(self, segment_id: int):
        """ä»å·²å®Œæˆåˆ—è¡¨ä¸­ç§»é™¤ä¸€ä¸ªæ®µè½ï¼ˆç”¨äºé‡æ–°ç¿»è¯‘ï¼‰"""
        if 'completed_segments' in self.checkpoint_data:
            if segment_id in self.checkpoint_data['completed_segments']:
                self.checkpoint_data['completed_segments'].remove(segment_id)

    def mark_segment_failed(self, segment_id: int, error_msg: str = ""):
        """æ ‡è®°ä¸€ä¸ªæ®µè½ä¸ºå¤±è´¥"""
        if 'failed_segments' not in self.checkpoint_data:
            self.checkpoint_data['failed_segments'] = []
        self.checkpoint_data['failed_segments'].append({
            'segment_id': segment_id,
            'error': error_msg,
            'timestamp': datetime.now().isoformat()
        })
    
    def is_segment_completed(self, segment_id: int) -> bool:
        """æ£€æŸ¥æ®µè½æ˜¯å¦å·²å®Œæˆ"""
        return segment_id in self.checkpoint_data.get('completed_segments', [])
    
    def get_completed_segment_ids(self) -> Set[int]:
        """è·å–æ‰€æœ‰å·²å®Œæˆçš„æ®µè½ID"""
        return set(self.checkpoint_data.get('completed_segments', []))
    
    def get_pending_segments(self, all_segments: SegmentList) -> SegmentList:
        """è·å–æ‰€æœ‰æœªå®Œæˆçš„æ®µè½
        
        ç­›é€‰æ¡ä»¶ï¼ˆæ»¡è¶³ä»»ä¸€å³ä¸ºå¾…ç¿»è¯‘ï¼‰ï¼š
        1. segment_id ä¸åœ¨ completed_segments ä¸­
        2. translated_text ä¸ºç©ºæˆ–åŒ…å«å¤±è´¥æ ‡è®°
        """
        completed_ids = self.get_completed_segment_ids()
        
        # ä½¿ç”¨é›†åˆå»é‡ï¼Œé¿å…é‡å¤æ·»åŠ 
        pending_ids = set()
        pending = []
        
        for seg in all_segments:
            # å·²ç»æ·»åŠ è¿‡çš„è·³è¿‡
            if seg.segment_id in pending_ids:
                continue
            
            # æ¡ä»¶1: ä¸åœ¨å·²å®Œæˆåˆ—è¡¨ä¸­
            not_completed = seg.segment_id not in completed_ids
            
            # æ¡ä»¶2: ç¿»è¯‘ç»“æœä¸ºç©ºæˆ–åŒ…å«å¤±è´¥æ ‡è®°
            has_failed_content = (
                not seg.translated_text or
                seg.translated_text.startswith("[Failed") or
                seg.translated_text.endswith("Failed]")
            )
            
            # æ»¡è¶³ä»»ä¸€æ¡ä»¶å³ä¸ºå¾…ç¿»è¯‘
            if not_completed or has_failed_content:
                pending.append(seg)
                pending_ids.add(seg.segment_id)
                # å¦‚æœæ˜¯å¤±è´¥çš„segmentä½†åœ¨completedåˆ—è¡¨ä¸­ï¼Œéœ€è¦ç§»é™¤
                if has_failed_content and seg.segment_id in completed_ids:
                    self.remove_from_completed(seg.segment_id)
        
        if pending:
            logger.info(f"ğŸ”„ æ£€æµ‹åˆ° {len(pending)} ä¸ªå¾…ç¿»è¯‘æ®µè½ (å…± {len(all_segments)} ä¸ª)")
        else:
            logger.info(f"âœ… æ‰€æœ‰ {len(all_segments)} ä¸ªæ®µè½å‡å·²å®Œæˆ")
        return pending
    
    def update_total_segments(self, total: int):
        """æ›´æ–°æ€»æ®µè½æ•°"""
        self.checkpoint_data['total_segments'] = total
    
    def get_progress_stats(self) -> Dict:
        """è·å–è¿›åº¦ç»Ÿè®¡ä¿¡æ¯"""
        completed = len(self.checkpoint_data.get('completed_segments', []))
        failed = len(self.checkpoint_data.get('failed_segments', []))
        total = self.checkpoint_data.get('total_segments', 0)
        progress_pct = (completed / total * 100) if total > 0 else 0
        
        return {
            'completed': completed,
            'failed': failed,
            'total': total,
            'pending': total - completed,
            'progress_percentage': progress_pct,
            'start_time': self.checkpoint_data.get('start_time'),
            'last_update': self.checkpoint_data.get('last_update')
        }
    
    def reset_checkpoint(self):
        """é‡ç½®æ£€æŸ¥ç‚¹ï¼ˆé‡æ–°å¼€å§‹ç¿»è¯‘ï¼‰"""
        logger.warning("ğŸ—‘ï¸  é‡ç½®æ£€æŸ¥ç‚¹ï¼Œå°†ä»å¤´å¼€å§‹ç¿»è¯‘")
        self.checkpoint_data = {
            'start_time': datetime.now().isoformat(),
            'completed_segments': [],
            'failed_segments': [],
            'total_segments': 0,
            'last_update': None
        }
        if self.checkpoint_file.exists():
            self.checkpoint_file.unlink()


# ========================================================================
# 2. ç¼“å­˜æŒä¹…åŒ–ç®¡ç†
# ========================================================================

class CachePersistenceManager:
    """ç¼“å­˜æŒä¹…åŒ–ç®¡ç†å™¨ - ç®¡ç†Geminiç¼“å­˜ä¸æœ¬åœ°æ–‡ä»¶çš„æ˜ å°„å…³ç³»"""
    
    def __init__(self, settings: 'Settings'):
        """
        Args:
            settings: å…¨å±€è®¾ç½®å¯¹è±¡ï¼ˆä»document_pathè‡ªåŠ¨è®¡ç®—doc_hashï¼‰
        """
        self.settings = settings
        
        # ä»settingsè‡ªåŠ¨è®¡ç®—doc_hash
        from ..utils.file import get_file_hash
        doc_hash = get_file_hash(settings.files.document_path) if settings.files.document_path else "unknown"
        
        # å­˜å‚¨ä¸ºå®ä¾‹å±æ€§ï¼Œä¾›åç»­æ–¹æ³•ä½¿ç”¨
        self.doc_hash = doc_hash
        
        # ä» settings ä¸­è·å–é¡¹ç›®ç›®å½•
        base_dir = Path(settings.files.output_base_dir) if isinstance(settings.files.output_base_dir, str) else settings.files.output_base_dir
        # `output_base_dir` åœ¨ä¸åŒè°ƒç”¨æ–¹ä¸­å¯èƒ½å·²ç»æŒ‡å‘ {doc_hash} ç›®å½•ã€‚
        # ä¸ºäº†é¿å…äº§ç”Ÿ {doc_hash}/{doc_hash} çš„é‡å¤åµŒå¥—ï¼Œè¿™é‡Œåšä¸€æ¬¡æ™ºèƒ½å½’ä¸€åŒ–ã€‚
        self.project_dir = base_dir if base_dir.name == doc_hash else (base_dir / doc_hash)
        # éµå¾ª test_standards.mdï¼šå°†ç¼“å­˜å…ƒæ•°æ®æŒä¹…åŒ–åˆ° `.cache/cache_metadata.json`
        self.cache_metadata_file = self.project_dir / ".cache" / "cache_metadata.json"
        self.cache_metadata: Dict[str, Dict[str, Any]] = {
            "system_instruction": {},
            "glossary": {},
            "context": {},
            "uploaded_files": {}
        }
        
        # ========== çº¿ç¨‹å®‰å…¨æœºåˆ¶ ==========
        # ç”¨äºä¿æŠ¤ç¼“å­˜åˆ›å»ºæ“ä½œçš„é”ï¼ˆé˜²æ­¢å¼‚æ­¥æ¨¡å¼ä¸‹çš„ç«æ€æ¡ä»¶ï¼‰
        self._cache_creation_lock = threading.Lock()
        # è®°å½•æ­£åœ¨åˆ›å»ºçš„ç¼“å­˜ï¼ˆkey=content_hash, value=Trueï¼‰
        self._pending_cache_creation: Dict[str, bool] = {}
        # ç”¨äºç­‰å¾…ç¼“å­˜åˆ›å»ºå®Œæˆçš„æ¡ä»¶å˜é‡
        self._cache_created_condition = threading.Condition(self._cache_creation_lock)
        
        self._load_metadata()
    
    def _load_metadata(self):
        """ä»ç£ç›˜åŠ è½½ç¼“å­˜å…ƒæ•°æ®"""
        if self.cache_metadata_file.exists():
            try:
                with open(self.cache_metadata_file, 'r', encoding='utf-8') as f:
                    loaded_data = json.load(f)
                    self.cache_metadata.update(loaded_data)
                self._cleanup_expired_caches()
                logger.info(f"âœ… å·²åŠ è½½ç¼“å­˜å…ƒæ•°æ®: {self.cache_metadata_file}")
            except Exception as e:
                logger.warning(f"âš ï¸ åŠ è½½ç¼“å­˜å…ƒæ•°æ®å¤±è´¥: {e}")
    
    def _save_metadata(self):
        """ä¿å­˜ç¼“å­˜å…ƒæ•°æ®åˆ°ç£ç›˜"""
        try:
            self.cache_metadata_file.parent.mkdir(parents=True, exist_ok=True)
            with open(self.cache_metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache_metadata, f, indent=2, ensure_ascii=False)
            logger.debug(f"ğŸ’¾ ç¼“å­˜å…ƒæ•°æ®å·²ä¿å­˜: {self.cache_metadata_file}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç¼“å­˜å…ƒæ•°æ®å¤±è´¥: {e}")
    
    def _cleanup_expired_caches(self):
        """æ¸…ç†è¿‡æœŸçš„ç¼“å­˜è®°å½•"""
        current_time = time.time()
        cleaned_count = 0
        
        for cache_type in self.cache_metadata:
            if not isinstance(self.cache_metadata[cache_type], dict):
                continue
            expired_keys = [
                k for k, v in self.cache_metadata[cache_type].items()
                if current_time > v.get('expiry_time', 0)
            ]
            for key in expired_keys:
                del self.cache_metadata[cache_type][key]
                cleaned_count += 1
        
        if cleaned_count > 0:
            logger.info(f"ğŸ§¹ å·²æ¸…ç† {cleaned_count} ä¸ªè¿‡æœŸç¼“å­˜è®°å½•")
            self._save_metadata()
    
    def register_system_cache(
        self,
        cache_name: str,
        content_hash: str,
        ttl_hours: float = 1.0
    ) -> bool:
        """æ³¨å†ŒSystem Instructionç¼“å­˜"""
        try:
            # ä½¿ç”¨æ—¥æœŸ+doc_hash+å†…å®¹hashç”Ÿæˆç¼“å­˜é”®
            date_str = datetime.now().strftime("%Y%m%d")
            doc_hash_short = self.doc_hash[:8] if self.doc_hash else "nodoc"
            cache_key = f"sys_{date_str}_{doc_hash_short}_{content_hash[:8]}"
            
            self.cache_metadata["system_instruction"][cache_key] = {
                "cache_name": cache_name,
                "content_hash": content_hash,
                "created_at": time.time(),
                "expiry_time": time.time() + (ttl_hours * 3600),
                "ttl_hours": ttl_hours,
                "type": "system_instruction"
            }
            self._save_metadata()
            logger.info(f"ğŸ“Œ å·²æ³¨å†ŒSystem Instructionç¼“å­˜: {cache_key}")
            return True
        except Exception as e:
            logger.error(f"âŒ æ³¨å†ŒSystemç¼“å­˜å¤±è´¥: {e}")
            return False
    
    def register_glossary_cache(
        self,
        cache_name: str,
        glossary_hash: str,
        term_count: int,
        ttl_hours: float = 2.0
    ) -> bool:
        """æ³¨å†Œæœ¯è¯­è¡¨ç¼“å­˜"""
        try:
            # ä½¿ç”¨æ—¥æœŸ+doc_hash+æœ¯è¯­è¡¨hashç”Ÿæˆç¼“å­˜é”®
            date_str = datetime.now().strftime("%Y%m%d")
            doc_hash_short = self.doc_hash[:8] if self.doc_hash else "nodoc"
            cache_key = f"glo_{date_str}_{doc_hash_short}_{glossary_hash[:8]}"
            
            self.cache_metadata["glossary"][cache_key] = {
                "cache_name": cache_name,
                "glossary_hash": glossary_hash,
                "term_count": term_count,
                "created_at": time.time(),
                "expiry_time": time.time() + (ttl_hours * 3600),
                "ttl_hours": ttl_hours,
                "type": "glossary"
            }
            self._save_metadata()
            logger.info(f"ğŸ“Œ å·²æ³¨å†Œæœ¯è¯­è¡¨ç¼“å­˜: {cache_key} ({term_count}é¡¹)")
            return True
        except Exception as e:
            logger.error(f"âŒ æ³¨å†Œæœ¯è¯­è¡¨ç¼“å­˜å¤±è´¥: {e}")
            return False
    
    def register_context_cache(
        self,
        cache_name: str,
        context_hash: str,
        segment_range: str,
        ttl_hours: float = 1.0
    ) -> bool:
        """æ³¨å†Œä¸Šä¸‹æ–‡ç¼“å­˜"""
        try:
            cache_key = f"context_{segment_range}_{context_hash[:8]}"
            self.cache_metadata["context"][cache_key] = {
                "cache_name": cache_name,
                "context_hash": context_hash,
                "segment_range": segment_range,
                "created_at": time.time(),
                "expiry_time": time.time() + (ttl_hours * 3600),
                "ttl_hours": ttl_hours,
                "type": "context"
            }
            self._save_metadata()
            logger.info(f"ğŸ“Œ å·²æ³¨å†Œä¸Šä¸‹æ–‡ç¼“å­˜: {cache_key}")
            return True
        except Exception as e:
            logger.error(f"âŒ æ³¨å†Œä¸Šä¸‹æ–‡ç¼“å­˜å¤±è´¥: {e}")
            return False
    
    def register_uploaded_file(
        self,
        file_path: str,
        file_uri: str,
        file_hash: str,
        mime_type: str = "image/jpeg"
    ) -> bool:
        """æ³¨å†Œå·²ä¸Šä¼ æ–‡ä»¶ï¼ˆGemini Developer APIä¸“ç”¨ï¼‰"""
        try:
            cache_key = f"file_{file_hash[:12]}"
            self.cache_metadata["uploaded_files"][cache_key] = {
                "file_path": file_path,
                "file_uri": file_uri,
                "file_hash": file_hash,
                "mime_type": mime_type,
                "uploaded_at": time.time(),
                "type": "uploaded_file"
            }
            self._save_metadata()
            logger.debug(f"ğŸ“Œ å·²æ³¨å†Œä¸Šä¼ æ–‡ä»¶: {Path(file_path).name}")
            return True
        except Exception as e:
            logger.error(f"âŒ æ³¨å†Œä¸Šä¼ æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def get_system_cache(self, content_hash: str) -> Optional[str]:
        """è·å–System Instructionç¼“å­˜åç§°ï¼ˆé€šè¿‡å†…å®¹hashæŸ¥æ‰¾ï¼‰"""
        # éå†æ‰€æœ‰system instructionç¼“å­˜ï¼Œæ ¹æ®content_hashæŸ¥æ‰¾
        current_time = time.time()
        for cache_key, cache_info in self.cache_metadata["system_instruction"].items():
            if (cache_info.get('content_hash') == content_hash and 
                current_time < cache_info.get('expiry_time', 0)):
                logger.debug(f"â™»ï¸  å¤ç”¨Systemç¼“å­˜: {cache_key}")
                return cache_info.get('cache_name')
        return None
    
    def get_glossary_cache(self, glossary_hash: str) -> Optional[str]:
        """è·å–æœ¯è¯­è¡¨ç¼“å­˜åç§°"""
        cache_key = f"glossary_{glossary_hash[:8]}"
        cache_info = self.cache_metadata["glossary"].get(cache_key)
        if cache_info and time.time() < cache_info.get('expiry_time', 0):
            logger.debug(f"â™»ï¸  å¤ç”¨æœ¯è¯­è¡¨ç¼“å­˜: {cache_key}")
            return cache_info.get('cache_name')
        return None
    
    def get_context_cache(self, context_hash: str) -> Optional[str]:
        """è·å–ä¸Šä¸‹æ–‡ç¼“å­˜åç§°"""
        for cache_key, cache_info in self.cache_metadata["context"].items():
            if (cache_info.get('context_hash') == context_hash and
                time.time() < cache_info.get('expiry_time', 0)):
                logger.debug(f"â™»ï¸  å¤ç”¨ä¸Šä¸‹æ–‡ç¼“å­˜: {cache_key}")
                return cache_info.get('cache_name')
        return None
    
    def get_uploaded_file_uri(self, file_hash: str) -> Optional[str]:
        """è·å–å·²ä¸Šä¼ æ–‡ä»¶çš„URI"""
        cache_key = f"file_{file_hash[:12]}"
        cache_info = self.cache_metadata["uploaded_files"].get(cache_key)
        if cache_info:
            logger.debug(f"â™»ï¸  å¤ç”¨ä¸Šä¼ æ–‡ä»¶: {cache_key}")
            return cache_info.get('file_uri')
        return None
    
    def list_all_caches(self) -> Dict[str, List[Dict[str, Any]]]:
        """åˆ—å‡ºæ‰€æœ‰ç¼“å­˜"""
        result = {}
        current_time = time.time()
        
        for cache_type, caches in self.cache_metadata.items():
            if not isinstance(caches, dict):
                continue
            active_caches = []
            for cache_key, cache_info in caches.items():
                expiry_time = cache_info.get('expiry_time', 0)
                is_expired = current_time > expiry_time
                cache_info_copy = cache_info.copy()
                cache_info_copy['key'] = cache_key
                cache_info_copy['is_expired'] = is_expired
                if not is_expired or cache_type == "uploaded_files":
                    active_caches.append(cache_info_copy)
            result[cache_type] = active_caches
        return result
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "total_caches": 0,
            "active_caches": 0,
            "expired_caches": 0,
            "by_type": {}
        }
        current_time = time.time()
        
        for cache_type, caches in self.cache_metadata.items():
            if not isinstance(caches, dict):
                continue
            total = len(caches)
            active = sum(1 for c in caches.values() 
                        if time.time() < c.get('expiry_time', float('inf')))
            expired = total - active
            stats["by_type"][cache_type] = {
                "total": total,
                "active": active,
                "expired": expired
            }
            stats["total_caches"] += total
            stats["active_caches"] += active
            stats["expired_caches"] += expired
        return stats
    
    def clear_all_caches(self):
        """æ¸…é™¤æ‰€æœ‰ç¼“å­˜è®°å½•"""
        self.cache_metadata = {
            "system_instruction": {},
            "glossary": {},
            "context": {},
            "uploaded_files": {}
        }
        self._save_metadata()
        logger.info("ğŸ§¹ å·²æ¸…é™¤æ‰€æœ‰ç¼“å­˜è®°å½•")
    
    def get_or_create_system_cache(
        self,
        system_instruction: str,
        model_name: str,
        display_name: Optional[str] = None
    ) -> Optional[str]:
        """
        ç»Ÿä¸€çš„System Instructionç¼“å­˜è·å–æˆ–åˆ›å»ºæ–¹æ³•ï¼ˆçº¿ç¨‹å®‰å…¨ç‰ˆæœ¬ï¼‰
        
        ä½¿ç”¨åŒé‡æ£€æŸ¥é”å®šæ¨¡å¼é˜²æ­¢å¼‚æ­¥æ¨¡å¼ä¸‹çš„ç«æ€æ¡ä»¶ï¼š
        - ç¬¬ä¸€æ¬¡æ£€æŸ¥ï¼šæ— é”å¿«é€Ÿè·¯å¾„ï¼Œå¦‚æœç¼“å­˜å·²å­˜åœ¨ç›´æ¥è¿”å›
        - åŠ é”ä¿æŠ¤ï¼šç¡®ä¿åŒä¸€æ—¶é—´åªæœ‰ä¸€ä¸ªçº¿ç¨‹åˆ›å»ºç¼“å­˜
        - ç¬¬äºŒæ¬¡æ£€æŸ¥ï¼šé˜²æ­¢åœ¨ç­‰å¾…é”æœŸé—´å…¶ä»–çº¿ç¨‹å·²åˆ›å»ºç¼“å­˜
        - ç­‰å¾…æœºåˆ¶ï¼šå¦‚æœç¼“å­˜æ­£åœ¨åˆ›å»ºä¸­ï¼Œç­‰å¾…å®Œæˆè€Œä¸æ˜¯é‡å¤åˆ›å»º
        
        Args:
            system_instruction: ç³»ç»ŸæŒ‡ä»¤å†…å®¹
            model_name: æ¨¡å‹åç§°
            display_name: ç¼“å­˜æ˜¾ç¤ºåç§°ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            ç¼“å­˜åç§°ï¼ˆcache_nameï¼‰ï¼Œå¦‚æœåˆ›å»ºå¤±è´¥åˆ™è¿”å›None
        """
        # è®¡ç®—å†…å®¹å“ˆå¸Œ
        content_hash = self.compute_content_hash(system_instruction)
        
        # ========== ç¬¬ä¸€æ¬¡æ£€æŸ¥ï¼ˆæ— é”å¿«é€Ÿè·¯å¾„ï¼‰==========
        existing_cache = self.get_system_cache(content_hash)
        if existing_cache:
            logger.info(f"â™»ï¸  å¤ç”¨å·²æœ‰System Instructionç¼“å­˜: {existing_cache[:50]}...")
            return existing_cache
        
        # ========== åŠ é”ä¿æŠ¤åˆ›å»ºè¿‡ç¨‹ ==========
        with self._cache_creation_lock:
            # ========== ç¬¬äºŒæ¬¡æ£€æŸ¥ï¼ˆé˜²æ­¢é‡å¤åˆ›å»ºï¼‰==========
            existing_cache = self.get_system_cache(content_hash)
            if existing_cache:
                logger.debug(f"ğŸ”’ é”å†…æ£€æµ‹åˆ°ç¼“å­˜å·²åˆ›å»º: {existing_cache[:30]}...")
                return existing_cache
            
            # ========== æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–çº¿ç¨‹æ­£åœ¨åˆ›å»º ==========
            if content_hash in self._pending_cache_creation:
                logger.info(f"â³ æ£€æµ‹åˆ°å…¶ä»– worker æ­£åœ¨åˆ›å»ºç¼“å­˜ï¼Œç­‰å¾…å®Œæˆ...")
                # ç­‰å¾…ç¼“å­˜åˆ›å»ºå®Œæˆï¼ˆæœ€å¤šç­‰å¾… 30 ç§’ï¼‰
                wait_start = time.time()
                while content_hash in self._pending_cache_creation:
                    timeout_remaining = 30.0 - (time.time() - wait_start)
                    if timeout_remaining <= 0:
                        logger.warning("âš ï¸  ç­‰å¾…ç¼“å­˜åˆ›å»ºè¶…æ—¶ï¼ˆ30ç§’ï¼‰ï¼Œç»§ç»­å°è¯•åˆ›å»º")
                        break
                    # é‡Šæ”¾é”å¹¶ç­‰å¾…é€šçŸ¥
                    self._cache_created_condition.wait(timeout=min(1.0, timeout_remaining))
                
                # ç­‰å¾…å®Œæˆåå†æ¬¡æ£€æŸ¥ç¼“å­˜
                existing_cache = self.get_system_cache(content_hash)
                if existing_cache:
                    logger.info(f"âœ… ç­‰å¾…å®Œæˆï¼Œç¼“å­˜å·²å°±ç»ª: {existing_cache[:30]}...")
                    return existing_cache
            
            # ========== æ ‡è®°æ­£åœ¨åˆ›å»º ==========
            self._pending_cache_creation[content_hash] = True
            logger.debug(f"ğŸ”¨ å¼€å§‹åˆ›å»ºç¼“å­˜ (hash: {content_hash[:8]}...)")
        
        # ========== åˆ›å»ºç¼“å­˜ï¼ˆé‡Šæ”¾é”ï¼Œå…è®¸å…¶ä»–çº¿ç¨‹ç­‰å¾…ï¼‰==========
        cache_name = None
        try:
            from google import genai
            from google.genai import types

            client = genai.Client(api_key=self.settings.api.gemini_api_key)
            ttl_seconds = int(self.settings.processing.cache_ttl_hours * 3600)
            
            # ä½¿ç”¨æ—¥æœŸå’Œhashç”Ÿæˆæ˜¾ç¤ºåç§°
            if display_name is None:
                date_str = datetime.now().strftime("%Y%m%d")
                display_name = f"sys_{date_str}_{content_hash[:8]}"

            cache = client.caches.create(
                model=model_name,
                config=types.CreateCachedContentConfig(
                    display_name=display_name,
                    system_instruction=system_instruction,
                    ttl=f"{ttl_seconds}s",
                ),
            )

            cache_name = cache.name
            logger.info(f"âœ… System Instructionç¼“å­˜å·²åˆ›å»º: {cache_name[:50]}...")
            logger.debug(f"   ç¼“å­˜TTL: {self.settings.processing.cache_ttl_hours}å°æ—¶")
            logger.debug(f"   ç¼“å­˜å†…å®¹é•¿åº¦: {len(system_instruction):,} å­—ç¬¦")
            
            # ========== ç«‹å³ä¿å­˜å…ƒæ•°æ®ï¼ˆåŠ é”ä¿æŠ¤ï¼‰==========
            with self._cache_creation_lock:
                self.register_system_cache(
                    cache_name=cache_name,
                    content_hash=content_hash,
                    ttl_hours=self.settings.processing.cache_ttl_hours
                )
                # å¼ºåˆ¶ç«‹å³å†™å…¥ç£ç›˜
                self._save_metadata()
            
            return cache_name
            
        except ImportError:
            logger.warning("âš ï¸  google.genai æ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡ç¼“å­˜åˆ›å»º")
            return None
        except Exception as e:
            logger.warning(f"âš ï¸  åˆ›å»º System Instruction ç¼“å­˜å¤±è´¥: {e}")
            logger.debug(f"   ç»§ç»­æ— ç¼“å­˜æ¨¡å¼...")
            return None
        finally:
            # ========== æ¸…é™¤æ­£åœ¨åˆ›å»ºæ ‡è®°å¹¶é€šçŸ¥ç­‰å¾…çº¿ç¨‹ ==========
            with self._cache_creation_lock:
                self._pending_cache_creation.pop(content_hash, None)
                # é€šçŸ¥æ‰€æœ‰ç­‰å¾…çš„çº¿ç¨‹
                self._cache_created_condition.notify_all()
                logger.debug(f"ğŸ”“ ç¼“å­˜åˆ›å»ºæµç¨‹ç»“æŸï¼Œå·²é€šçŸ¥ç­‰å¾…çº¿ç¨‹")
    
    def get_or_create_glossary_cache(
        self,
        glossary: Dict[str, str],
        model_name: str
    ) -> Optional[str]:
        """
        ç»Ÿä¸€çš„æœ¯è¯­è¡¨ç¼“å­˜è·å–æˆ–åˆ›å»ºæ–¹æ³•
        
        Args:
            glossary: æœ¯è¯­è¡¨å­—å…¸
            model_name: æ¨¡å‹åç§°
            
        Returns:
            ç¼“å­˜åç§°ï¼ˆcache_nameï¼‰ï¼Œå¦‚æœåˆ›å»ºå¤±è´¥åˆ™è¿”å›None
        """
        if not glossary:
            return None
        
        # è®¡ç®—æœ¯è¯­è¡¨å“ˆå¸Œ
        glossary_text = json.dumps(glossary, ensure_ascii=False, sort_keys=True)
        glossary_hash = self.compute_content_hash(glossary_text)
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰å¯å¤ç”¨çš„ç¼“å­˜
        existing_cache = self.get_glossary_cache(glossary_hash)
        if existing_cache:
            logger.info(f"â™»ï¸  å¤ç”¨å·²æœ‰æœ¯è¯­è¡¨ç¼“å­˜: {existing_cache[:50]}...")
            return existing_cache
        
        # åˆ›å»ºæ–°ç¼“å­˜
        try:
            from google import genai
            from google.genai import types

            client = genai.Client(api_key=self.settings.api.gemini_api_key)
            ttl_seconds = int(self.settings.processing.cache_ttl_hours * 2 * 3600)
            
            # æ ¼å¼åŒ–æœ¯è¯­è¡¨å†…å®¹
            glossary_content = "\n".join([
                f"- **{k}**: Must be translated as **{v}**" 
                for k, v in glossary.items()
            ])

            cache = client.caches.create(
                model=model_name,
                config=types.CreateCachedContentConfig(
                    display_name=f"glossary_{glossary_hash[:8]}",
                    contents=[
                        types.Content(
                            role="user",
                            parts=[types.Part.from_text(text=glossary_content)],
                        )
                    ],
                    ttl=f"{ttl_seconds}s",
                ),
            )

            cache_name = cache.name
            logger.info(f"âœ… æœ¯è¯­è¡¨ç¼“å­˜å·²åˆ›å»º: {cache_name[:50]}... ({len(glossary)}é¡¹)")
            
            # æ³¨å†Œåˆ°æŒä¹…åŒ–ç®¡ç†å™¨
            self.register_glossary_cache(
                cache_name=cache_name,
                glossary_hash=glossary_hash,
                term_count=len(glossary),
                ttl_hours=self.settings.processing.cache_ttl_hours * 2
            )
            
            return cache_name
            
        except ImportError:
            logger.warning("âš ï¸  google.genai æ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡æœ¯è¯­è¡¨ç¼“å­˜åˆ›å»º")
            return None
        except Exception as e:
            logger.warning(f"âš ï¸  åˆ›å»ºæœ¯è¯­è¡¨ç¼“å­˜å¤±è´¥: {e}")
            return None
    
    @staticmethod
    def compute_content_hash(content: str) -> str:
        """è®¡ç®—å†…å®¹å“ˆå¸Œå€¼"""
        return hashlib.md5(content.encode('utf-8')).hexdigest()


# ========================================================================
# 3. Prompt ç®¡ç†å™¨
# ========================================================================

class PromptManager:
    """Prompt æ¨¡æ¿ç®¡ç†å™¨ï¼Œåœ¨åˆå§‹åŒ–æ—¶åŠ è½½æ‰€æœ‰æ¨¡æ¿å’Œé…ç½®"""
    
    def __init__(self, settings: 'Settings'):
        """
        åˆå§‹åŒ– Prompt ç®¡ç†å™¨
        
        Args:
            settings: å…¨å±€è®¾ç½®å¯¹è±¡ï¼ŒåŒ…å« translation_mode_entity
        """
        self.settings = settings
        self.mode_entity = settings.processing.translation_mode_entity
        
        # æ ¹æ®translator provideré€‰æ‹©promptç‰ˆæœ¬
        provider = getattr(settings.api, 'translator_provider', 'gemini').lower()
        is_cloud_provider = provider in {'deepseek', 'openai', 'openai-compatible', 'openai_compatible', 'gemini'}
        
        if is_cloud_provider:
            # äº‘ç«¯APIä½¿ç”¨å®Œæ•´ç‰ˆæœ¬çš„promptï¼ˆæ›´å¥½çš„ç¿»è¯‘è´¨é‡ï¼‰
            self.system_instruction_base = self._load_prompt_template("system_instruction.md")
            self.text_translation_prompt = self._load_prompt_template("text_translation_prompt.md")
            logger.info("ğŸŒ äº‘ç«¯APIæ¨¡å¼ï¼šä½¿ç”¨å®Œæ•´ç‰ˆpromptï¼ˆé«˜è´¨é‡ç¿»è¯‘ï¼‰")
        else:
            # æœ¬åœ°æ¨¡å‹ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬ï¼ˆèŠ‚çœtokenï¼‰
            self.system_instruction_base = self._load_prompt_template("system_instruction_simple.md")
            self.text_translation_prompt = self._load_prompt_template("text_translation_prompt_simple.md")
            logger.info("ğŸ  æœ¬åœ°æ¨¡å¼ï¼šä½¿ç”¨ç®€åŒ–ç‰ˆpromptï¼ˆèŠ‚çœèµ„æºï¼‰")
        
        # è§†è§‰å’ŒJSONä¿®å¤promptä¿æŒä¸å˜
        self.vision_translation_prompt = self._load_prompt_template("vision_translation_prompt.md")
        self.json_repair_prompt = self._load_prompt_template("json_repair_prompt.md")
    
    def _load_prompt_template(self, template_name: str) -> str:
        """ä»æ–‡ä»¶åŠ è½½ Prompt æ¨¡æ¿"""
        path = Path(__file__).parent.parent.parent / "config" / "prompts" / template_name
        try:
            with open(path, 'r', encoding='utf-8') as f:
                return f.read()
        except FileNotFoundError:
            logger.warning(f"âš ï¸ Prompt template not found: {path}, using fallback")
            return "Translate the following text: {input_json}"
    
    def get_system_instruction(
        self, 
        use_vision: bool = False,
        include_mode: bool = False,
        include_glossary: bool = False,
        glossary_text: str = ""
    ) -> str:
        """
        è·å–èåˆäº†promptæ¨¡æ¿çš„system instruction
        
        ç¼“å­˜ç­–ç•¥ï¼š
        - é¢„ç¿»è¯‘é˜¶æ®µï¼šåªåŒ…å« base + prompt_templateï¼ˆæ—  modeã€æ—  glossaryï¼‰
        - æ­£å¼ç¿»è¯‘é˜¶æ®µï¼šåŒ…å« base + prompt_template + mode + glossary
        
        Args:
            use_vision: æ˜¯å¦ä½¿ç”¨è§†è§‰æ¨¡å¼
            include_mode: æ˜¯å¦åŒ…å«ç¿»è¯‘æ¨¡å¼é…ç½®ï¼ˆæ­£å¼ç¿»è¯‘æ—¶ä¸º Trueï¼‰
            include_glossary: æ˜¯å¦åŒ…å«æœ¯è¯­è¡¨ï¼ˆæ­£å¼ç¿»è¯‘æ—¶ä¸º Trueï¼‰
            glossary_text: æ ¼å¼åŒ–çš„æœ¯è¯­è¡¨æ–‡æœ¬
        
        Returns:
            å®Œæ•´çš„ system instruction
        """
        parts = [self.system_instruction_base]
        
        # æ·»åŠ  prompt æ¨¡æ¿
        if use_vision:
            parts.append(f"\n\n---\n\n# TRANSLATION PROMPT TEMPLATE\n\n{self.vision_translation_prompt}\n")
        else:
            parts.append(f"\n\n---\n\n# TRANSLATION PROMPT TEMPLATE\n\n{self.text_translation_prompt}\n")
        
        # æ·»åŠ ç¿»è¯‘æ¨¡å¼ï¼ˆæ­£å¼ç¿»è¯‘æ—¶ï¼‰
        if include_mode and self.mode_entity:
            mode_section = f"""
---

# ACTIVE TRANSLATION MODE

**Mode Name**: {self.mode_entity.name}

**Your Role**:
{self.mode_entity.role_desc}

**Your Style & Approach**:
{self.mode_entity.style}

**CRITICAL**: You MUST follow this mode's philosophy for ALL translations.
"""
            parts.append(mode_section)
        
        # æ·»åŠ æœ¯è¯­è¡¨ï¼ˆæ­£å¼ç¿»è¯‘æ—¶ï¼‰
        if include_glossary and glossary_text:
            glossary_section = f"""
---

# MANDATORY GLOSSARY

The following terms MUST be translated exactly as specified. These are non-negotiable:

<glossary>
{glossary_text}
</glossary>

**CRITICAL**: Always check the glossary before translating any term. If a term appears in the glossary, you MUST use the specified translation.
"""
            parts.append(glossary_section)
        
        return "".join(parts)
    
    def get_mode_prefix(self) -> str:
        """
        è·å–Modeé…ç½®ä½œä¸ºUser messageçš„å‰ç¼€ï¼ˆå·²å¼ƒç”¨ï¼Œmode ç°åœ¨åŒ…å«åœ¨ system instruction ä¸­ï¼‰
        
        Returns:
            æ ¼å¼åŒ–çš„æ¨¡å¼å‰ç¼€å­—ç¬¦ä¸²
        """
        if not self.mode_entity:
            return ""
        
        # ä½¿ç”¨å±æ€§è®¿é—®
        role_desc = self.mode_entity.role_desc
        style = self.mode_entity.style
        mode_name = self.mode_entity.name
        
        return f"""{'='*80}
âš ï¸ ACTIVE TRANSLATION MODE: {mode_name}
{'='*80}

Your Role:
{role_desc}

Your Style & Approach:
{style}

**CRITICAL**: Follow THIS mode's philosophy for the translation below.
{'='*80}

"""
    
    def format_text_prompt(
        self,
        context: str,
        input_json: str,
        glossary: str = ""
    ) -> str:
        """
        æ ¼å¼åŒ–æ–‡æœ¬ç¿»è¯‘çš„å®Œæ•´æç¤ºï¼ˆç”¨æˆ·æ¶ˆæ¯éƒ¨åˆ†ï¼‰
        
        æ–°è®¾è®¡ï¼š
        - glossary å’Œ mode å·²ç»åœ¨ system instruction ç¼“å­˜ä¸­
        - è¿™é‡Œåªæä¾›åŠ¨æ€å†…å®¹ï¼šcontext å’Œ input_json
        - é¢„ç¿»è¯‘é˜¶æ®µï¼šglossary ä¸ºç©º
        - æ­£å¼ç¿»è¯‘é˜¶æ®µï¼šglossary å·²åœ¨ system instruction ä¸­ï¼Œè¿™é‡Œå¯ä»¥ä¸ä¼ 
        
        Args:
            context: ä¸Šä¸‹æ–‡æ–‡æœ¬ï¼ˆå‰ä¸€ä¸ª batch çš„ç¿»è¯‘ç»“æœæˆ–åŸæ–‡ï¼‰
            input_json: è¾“å…¥çš„ JSON æ•°æ®
            glossary: æœ¯è¯­è¡¨æ–‡æœ¬ï¼ˆå¯é€‰ï¼Œç”¨äºéç¼“å­˜æ¨¡å¼ï¼‰
        
        Returns:
            æ ¼å¼åŒ–çš„å®Œæ•´æç¤º
        """
        parts = []
        
        # æ·»åŠ ä¸Šä¸‹æ–‡ï¼ˆåŠ¨æ€å†…å®¹ï¼Œæ¯æ¬¡è¯·æ±‚éƒ½ä¸åŒï¼‰
        if context and context.strip():
            parts.append(f"# Context from Previous Segments\n<previous_context>\n{context}\n</previous_context>\n")
        else:
            parts.append("# Context from Previous Segments\n<previous_context>\n(Beginning of document - no previous context)\n</previous_context>\n")
        
        # å¦‚æœæœ¯è¯­è¡¨åœ¨æ¶ˆæ¯ä¸­æä¾›ï¼ˆéç¼“å­˜æ¨¡å¼æˆ–é¢„ç¿»è¯‘é˜¶æ®µï¼‰
        if glossary and glossary.strip():
            parts.append(f"# Glossary Reference\n<glossary>\n{glossary}\n</glossary>\n")
        
        # æ·»åŠ è¾“å…¥æ•°æ®
        parts.append(f"# Input Data\n{input_json}")
        
        return "\n".join(parts)
    
    def format_vision_prompt(self, context: str) -> str:
        """
        æ ¼å¼åŒ–è§†è§‰ç¿»è¯‘çš„å®Œæ•´æç¤º
        
        Args:
            context: ä¸Šä¸‹æ–‡æ–‡æœ¬
        
        Returns:
            æ ¼å¼åŒ–çš„å®Œæ•´æç¤º
        """
        parts = []
        
        # æ·»åŠ æ¨¡å¼å‰ç¼€
        mode_prefix = self.get_mode_prefix()
        if mode_prefix:
            parts.append(mode_prefix)
        
        # æ·»åŠ ä¸Šä¸‹æ–‡
        if context and context.strip():
            parts.append(f"# Context from Previous Page\n<previous_context>\n{context}\n</previous_context>")
        else:
            parts.append("# Context from Previous Page\nNo previous context.")
        
        return "\n".join(parts)
    
    def format_title_prompt(self, text_list: str) -> str:
        """
        æ ¼å¼åŒ–æ ‡é¢˜ç¿»è¯‘æç¤º
        
        Args:
            text_list: JSON æ ¼å¼çš„æ ‡é¢˜åˆ—è¡¨
        
        Returns:
            æ ¼å¼åŒ–çš„æ ‡é¢˜ç¿»è¯‘æç¤º
        """
        style = self.mode_entity.style if self.mode_entity else "Professional and accurate"
        
        return f"""You are a professional translator. Translate the following list of document headers/titles into Chinese.

Your style: {style}

Input JSON: {text_list}

**You MUST OBEY THE FOLLOWING RULE!!!!!!**
Output JSON format: A flat JSON Dictionary where keys are the source text and values are the translation.
Example: {{"Chapter 1": "ç¬¬ä¸€ç« ", "Index": "ç´¢å¼•"}}

Return ONLY the JSON object."""
    
    def format_json_repair_prompt(
        self,
        original_prompt: str,
        broken_json: str,
        error_details: str
    ) -> str:
        """
        æ ¼å¼åŒ– JSON ä¿®å¤æç¤º
        
        Args:
            original_prompt: åŸå§‹æç¤º
            broken_json: æŸåçš„ JSON å­—ç¬¦ä¸²
            error_details: é”™è¯¯è¯¦æƒ…
        
        Returns:
            æ ¼å¼åŒ–çš„ JSON ä¿®å¤æç¤º
        """
        return self.json_repair_prompt.format(
            original_prompt=original_prompt,
            broken_json=broken_json,
            error_details=error_details
        )

