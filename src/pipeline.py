import os
import json
import re
from abc import ABC, abstractmethod
import csv
import logging
from typing import List, Dict, Any, Optional, Iterator, Tuple, Literal
from dataclasses import dataclass, asdict

# ç¬¬ä¸‰æ–¹åº“
import fitz  # PyMuPDF
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup

# æœ¬åœ°æ¨¡å—
from .file_io import extract_text_from_epub_item
from .pdf_analyzer import detect_pdf_type
from .config import Settings
from .errors import DocumentParseError, FileSystemError

# ==============================================================================
# 0. å…¨å±€èµ„æºåˆå§‹åŒ–
# ==============================================================================

logger = logging.getLogger(__name__)


# =====================================================
# 1. æ–‡æœ¬æ®µè½æ•°æ®ç»“æ„å®šä¹‰
#    æ”¾åœ¨æœ€ä¸Šé¢ï¼Œå› ä¸ºåé¢çš„å‡½æ•°éƒ½è¦ç”¨åˆ°å®ƒ
# =====================================================
@dataclass
class ContentSegment:
    segment_id: int
    original_text: str
    translated_text: str = ""
    
    # --- ç»“æ„å…ƒæ•°æ® (å…³é”®ä¿®æ”¹) ---
    is_new_chapter: bool = False
    chapter_title: str = ""
    page_index: int = 0  # å¯¹äº EPUB å¯ä»¥æ˜¯ 0 æˆ–ç‰¹å®šé€»è¾‘
    toc_level: int = 1 # TOC å±‚çº§ (1=H2, 2=H3, ...)ï¼Œé»˜è®¤ 1
    
    # --- å†…å®¹ç±»å‹ ---
    content_type: Literal["text", "image"] = "text"
    image_path: Optional[str] = None

# =====================================================
# 2. æ¸²æŸ“å™¨é€»è¾‘ 
# =====================================================
class MarkdownRenderer:
    def __init__(self, settings: Settings):
        self.settings = settings
        
        # --- æ ·å¼æ¨¡æ¿ ---
        # é¡µé¢åˆ†éš”ç¬¦ (åªåœ¨éç« èŠ‚å¼€å¤´æ˜¾ç¤º)
        self.page_fmt = "\n\n###### --- åŸæ–‡ç¬¬ {page} é¡µ --- \n\n"
        
        # å›¾ç‰‡æ ¼å¼
        self.image_fmt = "\n\n![Segment {id}]({path})"
        self.image_caption_fmt = "\n> ğŸ’¡ **å›¾æ³¨/å†…å®¹è¯‘æ–‡**\n> {caption}"
        self.image_footer_fmt = "\n\nğŸ”– **Segment {id}** (Image)\n"

    def render_segment(self, seg: ContentSegment) -> str:
        """
        æ¸²æŸ“å•ä¸ª Segment ä¸º Markdown å­—ç¬¦ä¸²ã€‚
        
        Args:
            seg: å†…å®¹ç‰‡æ®µå¯¹è±¡
        """
        parts = []

        # 1. ç¡®å®šæ˜¯å¦åŒè¯­å¯¹ç…§ (å‚æ•°ä¼˜å…ˆçº§ > è®¾ç½®ä¼˜å…ˆçº§)
        # ä½¿ç”¨ getattr æä¾›é»˜è®¤å€¼ Falseï¼Œé˜²æ­¢ settings æ²¡é…æŠ¥é”™
        
        retain_original = getattr(self.settings, 'retain_original', False)

        # =========================================================
        # A. ç»“æ„å±‚ï¼šåŠ¨æ€ç« èŠ‚æ ‡é¢˜ (Dynamic Header Rendering)
        # =========================================================
        # ä¼˜å…ˆçº§æœ€é«˜ï¼šå¦‚æœæ˜¯æ–°ç« èŠ‚ï¼Œæ˜¾ç¤º H1-H6 æ ‡é¢˜
        if seg.is_new_chapter and seg.chapter_title:
            # 1. è·å–å±‚çº§ï¼Œé»˜è®¤ä¸º 1
            level = getattr(seg, 'toc_level', 1)
            
            # 2. è®¡ç®— Markdown çš„ # æ•°é‡ (é™åˆ¶åœ¨ 1-6 ä¹‹é—´)
            # Level 1 -> ## (2) å› ä¸ºé€šå¸¸ H1 æ˜¯ä¹¦å
            hash_count = min(level + 1, 6) 
            hashes = "#" * hash_count
            
            parts.append(f"\n\n{hashes} ğŸ“– {seg.chapter_title}\n\n")
        
        # ç»“æ„å±‚ï¼šé¡µç æ ‡è®° (Page Marker)
        # ä»…åœ¨: 1.ç¡®å®ç¿»é¡µäº† 2.ä¸æ˜¯æ–°ç« èŠ‚ 3.å…¨å±€é…ç½®å…è®¸æ˜¾ç¤º æ—¶æ‰æ¸²æŸ“
        elif seg.page_index is not None and not seg.is_new_chapter:
            show_marker = getattr(self.settings, 'render_page_markers', True)
            
            # é’ˆå¯¹ Vision æ¨¡å¼çš„ç‰¹æ®Šä¼˜åŒ–ï¼šå¦‚æœæ˜¯å›¾ç‰‡ï¼Œä¸”æ²¡å¼ºåˆ¶è¦æ±‚ï¼Œå¯ä»¥ä¸æ˜¾ç¤ºé¡µç 
            # (å¯é€‰é€»è¾‘ï¼šå¦‚æœä½ å¸Œæœ› Vision æ¨¡å¼ä¹Ÿä¸æ˜¾ç¤ºé¡µç ï¼Œå¯ä»¥åœ¨è¿™é‡ŒåŠ åˆ¤æ–­)
            # if seg.content_type == "image": show_marker = False

            if show_marker:
                parts.append(self.page_fmt.format(page=seg.page_index + 1))

        # =========================================================
        # B. å†…å®¹å±‚ï¼šå›¾ç‰‡å¤„ç†
        # =========================================================
        if seg.content_type == "image" and seg.image_path:
            parts.append(self.image_fmt.format(id=seg.segment_id, path=seg.image_path))
            
            if seg.translated_text:
                clean_trans = seg.translated_text.replace('\\n', '\n').strip()
                parts.append(self.image_caption_fmt.format(caption=clean_trans))
            
            parts.append(self.image_footer_fmt.format(id=seg.segment_id))
            parts.append("---")
            return "".join(parts)

        # =========================================================
        # C. å†…å®¹å±‚ï¼šçº¯æ–‡æœ¬å¤„ç†
        # =========================================================
        trans_text = (seg.translated_text or "").replace('\\n', '\n').strip()
        original_text = (seg.original_text or "").replace('\r', '').strip()
        
        # æ®µè½ ID æ ‡è®°
        parts.append(f"\n\nğŸ”– **Segment {seg.segment_id}**\n")
        
        # è¾…åŠ©æ¸…æ´—å‡½æ•°
        clean_split = lambda t: [l.rstrip('\\').strip() for l in t.split('\n')]

        if retain_original:
            # --- åŒè¯­å¯¹ç…§æ¨¡å¼ ---
            orig_paras = [p for p in original_text.split('\n\n') if p.strip()]
            trans_paras = [p for p in trans_text.split('\n\n') if p.strip()]
            
            # é€æ®µå¯¹ç…§æ¸²æŸ“
            for i in range(max(len(orig_paras), len(trans_paras))):
                block = []
                p_orig = clean_split(orig_paras[i]) if i < len(orig_paras) else []
                p_trans = clean_split(trans_paras[i]) if i < len(trans_paras) else []

                if p_orig:
                    # åŸæ–‡ç¨å¾®ç¼©è¿›ä¸€ç‚¹ï¼Œæˆ–è€…åŠ ç²—ï¼Œçœ‹ä¸ªäººå–œå¥½
                    block.append(f"åŸæ–‡ï¼š{p_orig[0]}")
                    block.extend([f"      {line}" for line in p_orig[1:]])
                
                if p_trans:
                    for j, line in enumerate(p_trans):
                        # å¤„ç† LLM å¯èƒ½è¾“å‡ºçš„ Markdown æ ‡é¢˜ï¼Œä¿æŒæ ¼å¼
                        if line.startswith('#'): 
                            block.append(f"\n{line}\n")
                        elif j == 0: 
                            block.append(f"> è¯‘æ–‡ï¼š{line}")
                        else: 
                            block.append(f">       {line}")
                
                if block: parts.append("\n".join(block) + "\n")
        else:
            # --- çº¯è¯‘æ–‡æ¨¡å¼ ---
            lines = clean_split(trans_text)
            formatted = []
            for line in lines:
                if line.startswith('#'): 
                    formatted.append(f"\n{line}\n")
                else: 
                    # çº¯è¯‘æ–‡æ¨¡å¼ç»Ÿä¸€åŠ å¼•ç”¨å—ï¼Œæˆ–è€…ç›´æ¥è¾“å‡ºçœ‹ä½ å–œå¥½
                    formatted.append(f"> {line}" if line else ">")
            parts.append("\n".join(formatted))

        parts.append("\n\n---")
        return "".join(parts)
# ==============================================================================
# 3. åŸºç±»ï¼šå®šä¹‰æµæ°´çº¿éª¨æ¶
# ==============================================================================

class BaseDocPipeline(ABC):
    """
    æ–‡æ¡£å¤„ç†æµæ°´çº¿çš„æŠ½è±¡åŸºç±»ã€‚
    è´Ÿè´£å°†æ–‡æ¡£æµè½¬æ¢ä¸º List[ContentSegment] å¯¹è±¡æµã€‚
    """
    def __init__(self, file_path: str, cache_path: str, settings: Any):
        self.file_path = file_path
        self.cache_path = cache_path
        self.settings = settings
        
        # ç»“æœå®¹å™¨
        self.all_segments: List[ContentSegment] = []
        self.global_id_counter: int = 0
        
        # æ–‡æœ¬ç¼“å†²åŒº
        self.rolling_buffer: List[str] = [] # æ”¹ç”¨ List[str] æ€§èƒ½æ›´å¥½
        self.current_buffer_length: int = 0
        
        # --- ä¸Šä¸‹æ–‡çŠ¶æ€ (Context State) ---
        # è¿™äº›çŠ¶æ€éšç€éå†è¿‡ç¨‹åŠ¨æ€æ›´æ–°ï¼Œå†³å®šäº†ä¸‹ä¸€ä¸ª Segment çš„å…ƒæ•°æ®
        self.current_chapter_title: str = "å‰è¨€/æœªå‘½åç« èŠ‚"
        self.current_page_index: int = 0
        self.pending_new_chapter: bool = False # æ ‡è®°ä¸‹ä¸€ä¸ªç”Ÿæˆçš„ Segment æ˜¯å¦éœ€è¦åªæœ‰ç« èŠ‚å¤´
        self.current_toc_level: int = 1 
        # ç« èŠ‚æ˜ å°„è¡¨ {UnitKey: ChapterTitle}
        self.chapter_map: Dict[Any, str] = {} 

    def run(self) -> List[ContentSegment]:
        """ä¸»æµç¨‹ï¼šè¿­ä»£å•å…ƒ -> ç»´æŠ¤çŠ¶æ€ -> ç”Ÿæˆå¯¹è±¡"""
        logger.info(f"Starting pipeline '{self.__class__.__name__}' for {os.path.basename(self.file_path)}")
        
        self._load_metadata()
        
        # éå†å†…å®¹å•å…ƒ (UnitKey é€šå¸¸æ˜¯ é¡µç  æˆ– æ–‡ä»¶å)
        for unit_key, content, content_type in self._iter_content_units():
            
            # --- A. è§†è§‰/å›¾ç‰‡æ¨¡å¼å¤„ç† ---
            if content_type == "image":
                # 1. å…ˆæ¸…ç©ºä¹‹å‰çš„æ–‡æœ¬ç¼“å†²åŒº
                self._flush_buffer()
                
                # 2. ç›´æ¥ç”Ÿæˆå›¾ç‰‡ Segment
                seg = ContentSegment(
                    segment_id=self.global_id_counter,
                    original_text="", 
                    content_type="image",
                    image_path=content, 
                    page_index=unit_key if isinstance(unit_key, int) else 0,
                    # ç»§æ‰¿å½“å‰ç« èŠ‚ä¸Šä¸‹æ–‡
                    chapter_title=self.current_chapter_title, 
                    toc_level=self.current_toc_level, # ã€æ–°å¢ã€‘ä¼ å…¥å±‚çº§
                    is_new_chapter=False 
                )
                self.all_segments.append(seg)
                self.global_id_counter += 1
                continue

            # --- B. çº¯æ–‡æœ¬æ¨¡å¼å¤„ç† ---
            if not content or not content.strip():
                continue
            
            # 1. æ£€æŸ¥ç« èŠ‚å˜æ›´
            # chap_info å¯èƒ½æ˜¯å­—ç¬¦ä¸²(æ—§é€»è¾‘) æˆ– å­—å…¸(æ–°é€»è¾‘)
            chap_info = self.chapter_map.get(unit_key)
            
            if chap_info:
                # é¢„å…ˆè§£æå‡ºæ–°æ ‡é¢˜
                new_title = ""
                new_level = 1
                
                if isinstance(chap_info, dict):
                    new_title = chap_info.get("title", "Untitled")
                    new_level = chap_info.get("level", 1)
                else:
                    new_title = str(chap_info)
                    new_level = 1

                # ã€å…³é”®ä¿®æ­£ã€‘: åªæœ‰å½“æ ‡é¢˜å‘ç”Ÿ *å˜åŒ–* æ—¶ï¼Œæ‰è§¦å‘æ–°ç« èŠ‚é€»è¾‘
                # å¦åˆ™è¯´æ˜æˆ‘ä»¬è¿˜åœ¨åŒä¸€ä¸ªç« èŠ‚æ–‡ä»¶çš„ä¸åŒæ®µè½é‡Œ
                if new_title != self.current_chapter_title:
                    
                    # ç¡®å®æ˜¯æ–°ç« èŠ‚äº† -> ç»“ç®—æ—§è´¦ï¼Œå¼€å¯æ–°ç¯‡ç« 
                    self._flush_buffer()
                    
                    self.current_chapter_title = new_title
                    self.current_toc_level = new_level
                    
                    logger.debug(f"New chapter detected: {new_title}")
                    self.pending_new_chapter = True
            
            # 2. æ›´æ–°å½“å‰é¡µç  (é’ˆå¯¹ PDF)
            if isinstance(unit_key, int):
                self.current_page_index = unit_key

            # 3. ç´¯ç§¯æ–‡æœ¬
            self.rolling_buffer.append(content)
            self.current_buffer_length += len(content)
            
            # 4. æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†å—
            if self.current_buffer_length >= self.settings.max_chunk_size:
                self._flush_buffer()
        
        # å¤„ç†å‰©ä½™å†…å®¹
        self._flush_buffer()
        
        self._save_cache()
        logger.info(f"Pipeline finished. Generated {len(self.all_segments)} segments.")
        return self.all_segments

    def _flush_buffer(self):
        """å°†å½“å‰ç¼“å†²åŒºæ‰“åŒ…æˆä¸€ä¸ª Segment"""
        if not self.rolling_buffer:
            return
            
        full_text = "\n\n".join(self.rolling_buffer)
        
        # åˆ›å»ºå¯¹è±¡æ—¶ä¼ å…¥å½“å‰çŠ¶æ€
        seg = ContentSegment(
            segment_id=self.global_id_counter,
            original_text=full_text,
            content_type="text",
            # è¿™é‡Œä¼ å…¥çš„ä¸€å®šè¦æ˜¯å­—ç¬¦ä¸²ï¼Œä¸èƒ½æ˜¯å­—å…¸
            chapter_title=self.current_chapter_title, 
            # ã€æ–°å¢ã€‘ä¼ å…¥å±‚çº§ä¿¡æ¯
            toc_level=self.current_toc_level,
            is_new_chapter=self.pending_new_chapter,
            page_index=self.current_page_index
        )
        
        self.all_segments.append(seg)
        self.global_id_counter += 1
        
        # é‡ç½®çŠ¶æ€
        self.rolling_buffer = []
        self.current_buffer_length = 0
        self.pending_new_chapter = False

    def _save_cache(self):
        """ä¿å­˜ä¸º JSON (éœ€è¦å°† dataclass è½¬ dict)"""
        data = [asdict(seg) for seg in self.all_segments]
        with open(self.cache_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    @abstractmethod
    def _load_metadata(self):
        pass

    @abstractmethod
    def _iter_content_units(self) -> Iterator[Tuple[Any, str, str]]:
        """
        Yields: (unit_key, content, content_type)
        content_type: 'text' | 'image'
        """
        pass

# ==============================================================================
# 3. EPUB å®ç°ç±»
# ==============================================================================

class EPUBPipeline(BaseDocPipeline):
    def _load_metadata(self):
        """
        è§£æ EPUB å…ƒæ•°æ®å’Œç›®å½•ç»“æ„ã€‚
        """
        logger.info("Parsing EPUB metadata.")
        # 1. è¯»å– EPUB
        self.book = epub.read_epub(self.file_path)
        
        # 2. å°è¯•ä» NCX/NAV è·å–ç›®å½• (Flatten)
        # è¿™é‡Œå‡è®¾ä½ å·²ç»å®ç°äº† _flatten_epub_to_standard
        standardized_items = self._flatten_epub_to_standard(self.book.toc)

        # 3. å…œåº•é€»è¾‘ï¼šå¦‚æœç›®å½•ä¸ºç©ºï¼Œä½¿ç”¨ Spine (é˜…è¯»é¡ºåº)
        if not standardized_items:
            logger.warning("âš ï¸ EPUB TOC is empty. Falling back to Spine (linear reading order).")
            
            # ä½¿ç”¨ Spine éå†ï¼Œå®ƒä»£è¡¨äº†ä¹¦çš„çœŸå®é˜…è¯»é¡ºåº
            # item_id æ˜¯ manifest é‡Œçš„ ID, linear è¡¨ç¤ºæ˜¯å¦çº¿æ€§é˜…è¯»(yes/no)
            for item_id, linear in self.book.spine:
                item = self.book.get_item_with_id(item_id)
                
                if item:
                    # è¿‡æ»¤æ‰é HTML æ–‡æ¡£ (æ¯”å¦‚å›¾ç‰‡è™½ç„¶åœ¨ spine é‡Œä½†ä¸æ˜¯æ–‡æ¡£)
                    # æ³¨æ„ï¼šéœ€è¦ import ebooklib
                    if item.get_type() != ebooklib.ITEM_DOCUMENT:
                        continue
                        
                    # è¿‡æ»¤æ‰æ˜æ˜¾çš„å¯¼èˆªæ–‡ä»¶ (Nav)
                    # å¾ˆå¤š nav æ–‡ä»¶æ²¡ä»€ä¹ˆå¯ç¿»è¯‘çš„ï¼Œå®¹æ˜“äº§ç”Ÿç©º seg
                    if 'nav' in (item.get_name() or "").lower():
                        continue

                    # ç”Ÿæˆä¸€ä¸ªä¸´æ—¶æ ‡é¢˜ (å› ä¸º Spine é‡Œæ²¡æœ‰æ ‡é¢˜ä¿¡æ¯)
                    # item.get_name() é€šå¸¸æ˜¯ 'Text/part001.xhtml'ï¼Œä½œä¸ºæ ‡é¢˜å¾ˆéš¾çœ‹
                    # æˆ‘ä»¬å¯ä»¥ç”¨æ–‡ä»¶åï¼Œæˆ–è€…ç›´æ¥å« "Section X"
                    file_name = item.get_name()
                    
                    standardized_items.append({
                        'level': 1,
                        'title': f"Section: {file_name}", # æˆ–è€…ç”¨ item.title å¦‚æœæœ‰çš„è¯
                        'key': file_name
                    })

        # 4. ç»Ÿä¸€å¤„ç†
        use_bc = getattr(self.settings, 'use_breadcrumb', True)
        self.chapter_map = process_unified_toc(standardized_items, use_breadcrumb=use_bc)
        logger.info(f"âœ… Metadata loaded. Chapter Map size: {len(self.chapter_map)}")

    def _iter_content_units(self):
        """
        [æ ¸å¿ƒä¿®æ”¹] æŒ‰ç…§ EPUB Spine éå†ï¼Œå¹¶è§£æ HTML å—çº§å…ƒç´ ã€‚
        ä¸å†è¿”å›ä¸€æ•´å—æ–‡æœ¬ï¼Œè€Œæ˜¯ yield å•ä¸ªæ®µè½/æ ‡é¢˜ã€‚
        """
        # å®šä¹‰æˆ‘ä»¬éœ€è¦æå–çš„å—çº§æ ‡ç­¾
        # æ’é™¤ divï¼Œé˜²æ­¢é‡å¤æå–ï¼ˆå› ä¸º div é€šå¸¸åŒ…å« pï¼‰
        BLOCK_TAGS = ['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'blockquote', 'pre']

        for item_id, linear in self.book.spine:
            item = self.book.get_item_with_id(item_id)
            
            if not item: continue
            if item.get_type() != ebooklib.ITEM_DOCUMENT: continue
            
            # 1. è§£æ HTML
            try:
                raw_content = item.get_content()
                soup = BeautifulSoup(raw_content, 'html.parser')
                
                # è·å–æ–‡ä»¶åä½œä¸º Key (ç”¨äºæŸ¥ç« èŠ‚æ ‡é¢˜)
                unit_key = item.get_name()

                # 2. æ‰¾åˆ° Body (å¦‚æœæ²¡ Body å°±æ‰¾å…¨æ–‡æ¡£)
                root = soup.find('body') or soup

                # 3. éå†æ‰€æœ‰å—çº§å…ƒç´ 
                # è¿™é‡Œä½¿ç”¨ find_all å¯èƒ½ä¼šé‡åˆ°åµŒå¥—é—®é¢˜ï¼ˆæ¯”å¦‚ li é‡Œé¢æœ‰ pï¼‰ï¼Œ
                # ä½†å¯¹äºå¤§å¤šæ•° EPUBï¼Œç›´æ¥æå–è¿™äº›æ ‡ç­¾æ˜¯æœ€ç¨³å¦¥çš„ç­–ç•¥ã€‚
                for tag in root.find_all(BLOCK_TAGS):
                    
                    # 4. æå–çº¯æ–‡æœ¬
                    # separator=' ' å¤„ç† <p>Hello<br>World</p> -> "Hello World"
                    text = tag.get_text(separator=' ', strip=True)
                    
                    # 5. è¿‡æ»¤æ‰ç©ºæ ‡ç­¾æˆ–æçŸ­çš„å™ªéŸ³
                    if not text:
                        continue
                        
                    # 6. Yield å•ä¸ªæ®µè½
                    # è¿™æ · BaseDocPipeline é‡Œçš„ rolling_buffer å°±ä¼šä¸€æ®µä¸€æ®µåœ°å¢åŠ 
                    yield unit_key, text, "text"

            except Exception as e:
                logger.error(f"Failed to parse HTML structure for {item_id}: {e}")
                continue

    def _flatten_epub_to_standard(self, toc, level=1):
        items = []
        for node in toc:
            # å…¼å®¹ ebooklib çš„ä¸¤ç§èŠ‚ç‚¹æ ¼å¼
            entry = node[0] if isinstance(node, (list, tuple)) else node
            children = node[1] if isinstance(node, (list, tuple)) and len(node) > 1 else []
            
            # ã€å¾®è°ƒã€‘åŒæ—¶æ£€æŸ¥ hasattr å’Œ href æ˜¯å¦çœŸçš„æœ‰å€¼
            if hasattr(entry, 'href') and entry.href:
                items.append({
                    'level': level,
                    'title': entry.title or "Untitled",
                    'key': entry.href.split('#')[0] # key æ˜¯æ–‡ä»¶å
                })
                
            if children:
                items.extend(self._flatten_epub_to_standard(children, level + 1))
        return items

# ==============================================================================
# 5. PDF å®ç°ç±» 
# ==============================================================================

class PDFPipeline(BaseDocPipeline):
    def __init__(self, file_path: str, cache_path: str, settings: Any, extra_config: dict = None):
        super().__init__(file_path, cache_path, settings)
        self.doc: Optional[fitz.Document] = None
        self.config = extra_config or {}

    def _load_metadata(self):
        """
        åŠ è½½å…ƒæ•°æ®å¹¶é€‚é… process_unified_toc æ¶æ„ã€‚
        æ”¯æŒï¼šCSV è‡ªå®šä¹‰ -> PDF åŸç”Ÿ TOC -> çº¯é¡µç å›é€€ã€‚
        """
        self.doc = fitz.open(self.file_path)
        
        # 1. å®šä¹‰ä¸­é—´å±‚ï¼šæ ‡å‡†ä¸‰å…ƒç»„åˆ—è¡¨
        # æ¯ä¸€é¡¹ç»“æ„: {'level': int, 'title': str, 'key': int}
        standardized_items = []
        
        # =========================================================
        # åˆ†æ”¯ A: å°è¯•åŠ è½½ CSV è‡ªå®šä¹‰ç›®å½• (ä¼˜å…ˆçº§æœ€é«˜)
        # =========================================================
        if self.settings.custom_toc_path and os.path.exists(self.settings.custom_toc_path):
            logger.info(f"Loading custom TOC from CSV: {self.settings.custom_toc_path}")
            try:
                # utf-8-sig å…¼å®¹ Excel ä¿å­˜çš„ CSV
                with open(self.settings.custom_toc_path, 'r', encoding='utf-8-sig') as f:
                    reader = csv.DictReader(f)
                    for row in reader:
                        # å¥å£®æ€§è¯»å–ï¼šå¤„ç† CSV åˆ—åå¤§å°å†™æˆ–ç©ºæ ¼
                        # å‡è®¾æ ‡å‡†åˆ—å: Page, Title, Level (å¯é€‰)
                        row_lower = {k.lower().strip(): v for k, v in row.items()}
                        
                        page_str = row_lower.get('page') or row_lower.get('é¡µç ')
                        if not page_str: continue
                        
                        p_idx = int(page_str) - 1 # ç”¨æˆ·ä¹ æƒ¯ 1-based, å†…éƒ¨é€»è¾‘ 0-based
                        
                        title = row_lower.get('title') or row_lower.get('æ ‡é¢˜') or f"Page {p_idx+1}"
                        level_str = row_lower.get('level') or row_lower.get('å±‚çº§') or "1"
                        
                        if p_idx >= 0:
                            standardized_items.append({
                                'level': int(level_str),
                                'title': title.strip(),
                                'key': p_idx
                            })
            except Exception as e:
                logger.error(f"Failed to parse CSV TOC: {e}. Falling back to native TOC.")
                standardized_items = [] # è§£æå¤±è´¥ï¼Œæ¸…ç©ºä»¥è§¦å‘å›é€€

        # =========================================================
        # åˆ†æ”¯ B: å°è¯•åŠ è½½ PDF åŸç”Ÿ TOC (å¦‚æœ CSV ä¸ºç©º)
        # =========================================================
        if not standardized_items:
            # fitz.get_toc() è¿”å›: [[lvl, title, page, ...], ...]
            raw_toc = self.doc.get_toc()
            if raw_toc:
                logger.info("Loading native PDF TOC.")
                for item in raw_toc:
                    lvl = item[0]
                    title = item[1]
                    page_num = item[2]
                    
                    p_idx = page_num - 1
                    if p_idx >= 0:
                        standardized_items.append({
                            'level': lvl,
                            'title': title,
                            'key': p_idx
                        })

        # =========================================================
        # åˆ†æ”¯ C: çº¯é¡µç å›é€€æ¨¡å¼ (å¦‚æœä»¥ä¸Šéƒ½ä¸ºç©º)
        # =========================================================
        is_fallback_mode = False
        if not standardized_items:
            logger.info("No TOC found. Falling back to Page-as-Chapter mode.")
            is_fallback_mode = True
            # æ¯ä¸€é¡µéƒ½ä½œä¸ºä¸€ä¸ª Level 1 çš„ç« èŠ‚
            for i in range(len(self.doc)):
                standardized_items.append({
                    'level': 1,
                    'title': f"Page {i+1}",
                    'key': i
                })

        # =========================================================
        # 2. ç»Ÿä¸€è°ƒç”¨æ ¸å¿ƒç­–ç•¥
        # =========================================================
        
        # è·å–é¢åŒ…å±‘å¼€å…³ (é»˜è®¤å¼€å¯)
        use_bc = getattr(self.settings, 'use_breadcrumb', True)
        
        # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæ˜¯çº¯é¡µç å›é€€æ¨¡å¼ï¼Œå¼ºåˆ¶å…³é—­é¢åŒ…å±‘
        # å¦åˆ™ä¼šå˜æˆ "Page 1 > Page 2 > Page 3..." è¿™ç§è’è°¬çš„å±‚çº§
        if is_fallback_mode:
            use_bc = False
            
        # è°ƒç”¨ process_unified_toc ç”Ÿæˆæœ€ç»ˆ Map
        # ç»“æœæ ¼å¼: { 0: {"title": "...", "level": 1}, 5: {"title": "...", "level": 2} }
        self.chapter_map = process_unified_toc(standardized_items, use_breadcrumb=use_bc)
        
        # (å¯é€‰) ä¿å­˜ raw items ä¾› process_flow è¿›è¡Œé¢„ç¿»è¯‘ä½¿ç”¨
        self.raw_toc_entries = standardized_items
        
        logger.info(f"Metadata loaded. Chapter Map contains {len(self.chapter_map)} entries.")

    def _iter_content_units(self) -> Iterator[Tuple[int, str, str]]:
        """
        æ ¹æ®æ¨¡å¼ Yield å†…å®¹ã€‚
        Vision æ¨¡å¼ -> type='image', content=path
        Text æ¨¡å¼ -> type='text', content=string
        """
        use_vision = self.config.get("use_vision_mode", False)
        start_page = 0
        end_page = len(self.doc)

        for i in range(start_page, end_page):
            page = self.doc[i]
            
            if use_vision:
                # --- Vision æ¨¡å¼ ---
                img_path = self._save_page_image(page, i)
                if img_path:
                    # yield (é¡µç , å›¾ç‰‡è·¯å¾„, ç±»å‹)
                    yield i, img_path, "image"
            else:
                # --- Text æ¨¡å¼ ---
                text = self._extract_text(page, i)
                # yield (é¡µç , æ–‡æœ¬å†…å®¹, ç±»å‹)
                yield i, text, "text"

    def _save_page_image(self, page, page_idx) -> str:
        """
        ä»¥ 200 DPI æ¸²æŸ“è£åˆ‡åçš„é¡µé¢å›¾ç‰‡ã€‚
        ç›´æ¥åœ¨æ¸²æŸ“é˜¶æ®µæ ¹æ® Settings é‡Œçš„ Margin å‚æ•°ç§»é™¤é¡µçœ‰é¡µè„šã€‚
        """
        try:
            # 1. å‡†å¤‡ç›®å½•
            project_dir = os.path.dirname(os.path.abspath(self.cache_path))

            # åœ¨é¡¹ç›®ç›®å½•ä¸‹åˆ›å»º images æ–‡ä»¶å¤¹
            image_dir = os.path.join(project_dir, "images")
            os.makedirs(image_dir, exist_ok=True)
            # 2. ç”Ÿæˆæ–‡ä»¶å
            filename = f"page_{page_idx + 1:04d}.jpg"
            full_path = os.path.join(image_dir, filename)

            if os.path.exists(full_path):
                return full_path

            # 3. è®¡ç®—è£åˆ‡åŒºåŸŸ (Clip Rect) - é€»è¾‘åŒ _extract_text
            w = page.rect.width
            h = page.rect.height

            # è·å–è®¾ç½® (ç™¾åˆ†æ¯”)
            m_top = getattr(self.settings, 'MARGIN_TOP', 0.0)
            m_bottom = getattr(self.settings, 'MARGIN_BOTTOM', 0.0)
            m_left = getattr(self.settings, 'MARGIN_LEFT', 0.0)
            m_right = getattr(self.settings, 'MARGIN_RIGHT', 0.0)

            # è®¡ç®—åæ ‡
            x0 = w * m_left
            y0 = h * m_top
            x1 = w * (1.0 - m_right)
            y1 = h * (1.0 - m_bottom)

            # å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœè£åˆ‡å‚æ•°æœ‰é—®é¢˜ï¼ˆæ¯”å¦‚æŠŠå›¾åˆ‡æ²¡äº†ï¼‰ï¼Œå›é€€åˆ°å…¨é¡µ
            if x0 >= x1 or y0 >= y1:
                logger.warning(f"Page {page_idx}: Invalid margins for image. Rendering full page.")
                clip_rect = page.rect
            else:
                clip_rect = fitz.Rect(x0, y0, x1, y1)

            # 4. è®¾ç½®ç¼©æ”¾ (é”å®š 200 DPI)
            target_dpi = 200
            zoom = target_dpi / 72
            mat = fitz.Matrix(zoom, zoom)

            # 5. æ¸²æŸ“ (å…³é”®ç‚¹ï¼šä¼ å…¥ clip å‚æ•°)
            # fitz ä¼šåªæ¸²æŸ“ clip_rect æŒ‡å®šçš„åŒºåŸŸï¼Œå¹¶åº”ç”¨ matrix ç¼©æ”¾
            pix = page.get_pixmap(matrix=mat, clip=clip_rect, alpha=False)
            
            # 6. ä¿å­˜
            pix.save(full_path)
            
            return full_path

        except Exception as e:
            logger.error(f"Failed to render image for page {page_idx}: {e}")
            return ""

    def _extract_text(self, page, page_idx) -> str:
        """
        æå–é¡µé¢æ–‡æœ¬ï¼Œæ ¹æ® settings é‡Œçš„ç™¾åˆ†æ¯”å‚æ•°è¿›è¡Œè£åˆ‡ (Clip)ã€‚
        """
        try:
            # 1. è·å–é¡µé¢åŸå§‹å°ºå¯¸
            w = page.rect.width
            h = page.rect.height

            # 2. è·å–è£åˆ‡æ¯”ä¾‹ (é»˜è®¤ä¸º 0ï¼Œå³ä¸è£åˆ‡)
            # å®¹é”™å¤„ç†ï¼šä½¿ç”¨ getattr é¿å… settings ç¼ºå°‘å­—æ®µæŠ¥é”™
            m_top = getattr(self.settings, 'MARGIN_TOP', 0.0)
            m_bottom = getattr(self.settings, 'MARGIN_BOTTOM', 0.0)
            m_left = getattr(self.settings, 'MARGIN_LEFT', 0.0)
            m_right = getattr(self.settings, 'MARGIN_RIGHT', 0.0)

            # 3. è®¡ç®—è£åˆ‡çŸ©å½¢ (Clip Rect)
            # å·¦ä¸Šè§’ (x0, y0) -> å³ä¸‹è§’ (x1, y1)
            x0 = w * m_left
            y0 = h * m_top
            x1 = w * (1.0 - m_right)
            y1 = h * (1.0 - m_bottom)

            # å®‰å…¨æ€§æ£€æŸ¥ï¼šé˜²æ­¢ margin è®¾ç½®è¿‡å¤§å¯¼è‡´åŒºåŸŸé‡å æˆ–æ— æ•ˆ
            if x0 >= x1 or y0 >= y1:
                logger.warning(f"Page {page_idx}: Margins match or overlap content area. Returning raw text.")
                return page.get_text("text", sort=True).strip()

            clip_rect = fitz.Rect(x0, y0, x1, y1)

            # 4. æå–æ–‡æœ¬ (å¸¦ Clip)
            # sort=True å°è¯•æŒ‰é˜…è¯»é¡ºåºé‡æ–°æ’åˆ—æ–‡æœ¬å—
            text = page.get_text("text", clip=clip_rect, sort=True)
            
            return text.strip()

        except Exception as e:
            logger.error(f"Failed to extract text from page {page_idx}: {e}")
            return ""

# ==============================================================================
# 6. å·¥å‚å…¥å£
# ==============================================================================

def compile_structure(
    file_path: str,
    cache_path: str,
    settings: Any, # ä½ çš„ Settings å¯¹è±¡
    project_config: Optional[Dict[str, Any]] = None
) -> List[ContentSegment]:  # <--- æ³¨æ„ï¼šè¿”å›å€¼å˜å¼ºç±»å‹äº†
    """
    æ™ºèƒ½å·¥å‚å‡½æ•°ï¼šæ ¹æ®æ–‡ä»¶ç±»å‹å®ä¾‹åŒ–å¯¹åº”çš„ Pipeline å¹¶æ‰§è¡Œã€‚
    """
    ext = os.path.splitext(file_path)[1].lower()
    final_config = project_config or {}
    
    pipeline: Optional[BaseDocPipeline] = None

    if ext == '.epub':
        pipeline = EPUBPipeline(file_path, cache_path, settings)

    elif ext == '.pdf':
        # --- è‡ªåŠ¨å†³ç­–é€»è¾‘ (ä¿ç•™åŸé€»è¾‘) ---
        # å¦‚æœç”¨æˆ·æ²¡æœ‰å¼ºåˆ¶æŒ‡å®šæ¨¡å¼ï¼Œä¸”æˆ‘ä»¬éœ€è¦è‡ªåŠ¨æ£€æµ‹
        if "use_vision_mode" not in final_config:
            logger.info("Auto-detecting PDF type for vision mode decision.")
            try:
                pdf_type = detect_pdf_type(file_path)
                is_image_only = (pdf_type == "image_only")
                # is_image_only = False # ä¸´æ—¶å ä½
                
                final_config["use_vision_mode"] = is_image_only
                logger.info(f"Vision mode set to {is_image_only} based on detection.")
            except Exception as e:
                logger.warning(f"Detection failed: {e}. Defaulting to text mode.")
                final_config["use_vision_mode"] = False

        pipeline = PDFPipeline(file_path, cache_path, settings, extra_config=final_config)

    else:
        raise ValueError(f"Unsupported file format: {ext}")
        
    # æ‰§è¡Œ Pipelineï¼Œè¿”å›å¯¹è±¡åˆ—è¡¨
    return pipeline.run()

# ==============================================================================
# 7. TOC ç»Ÿä¸€å¤„ç†å‡½æ•° 
# ==============================================================================
def process_unified_toc(
    raw_toc_items: List[Dict[str, Any]], 
    use_breadcrumb: bool = True
) -> Dict[Any, Dict[str, Any]]:
    """
    [é€šç”¨æ ¸å¿ƒ] ç»Ÿä¸€å¤„ç† TOCã€‚
    Args:
        raw_toc_items: List of {'level': int, 'title': str, 'key': Any}
        use_breadcrumb: True or False æ˜¯å¦å¯ç”¨é¢åŒ…å±‘å¯¼èˆªæ ¼å¼è€Œéå±‚çº§æ ¼å¼
    Returns:
        { key: {"title": "Final Title", "level": int} }
    """
    chapter_map = {}
    title_stack = [] # è·¯å¾„æ ˆ
    
    for item in raw_toc_items:
        level = item['level']
        raw_title = item['title'].strip()
        key = item['key']
        
        # 1. ç»´æŠ¤æ ˆï¼šä¿ç•™çˆ¶çº§è·¯å¾„
        if len(title_stack) >= level:
            title_stack = title_stack[:level-1]
        title_stack.append(raw_title)
        
        # 2. ç­–ç•¥åº”ç”¨
        if use_breadcrumb:
            final_title = " > ".join(title_stack)
            final_level = 1 # é¢åŒ…å±‘å¼ºåˆ¶å±‚çº§ä¸º 1 (H2)
        else:
            final_title = raw_title
            final_level = level # ä¿ç•™åŸå§‹è¯­ä¹‰å±‚çº§
            
        # 3. å†™å…¥ Map (é˜²è¦†ç›–ï¼šä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°)
        if key not in chapter_map:
            chapter_map[key] = {
                "title": final_title,
                "level": final_level
            }
            
    return chapter_map