"""
EPUB æ¸²æŸ“å™¨
è´Ÿè´£å°†ç¿»è¯‘åçš„ ContentSegment åˆ—è¡¨å¡«å›åŸ EPUB æ–‡ä»¶ï¼Œè¾“å‡ºç¿»è¯‘ç‰ˆæœ¬
"""
import copy
import zipfile
import tempfile
import shutil
import re
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Set
from collections import defaultdict

import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup, NavigableString, Tag

from ..core.schema import ContentSegment, Settings, SegmentList
from ..utils.logger import get_logger

logger = get_logger(__name__)


class EPUBRenderer:
    """
    EPUB æ¸²æŸ“å™¨
    
    èŒè´£ï¼š
    - å°†ç¿»è¯‘åçš„å†…å®¹å¡«å›åŸ EPUB æ–‡ä»¶çš„å¯¹åº”ä½ç½®
    - ç»§æ‰¿åŸæ–‡ä»¶çš„æ ¼å¼å’Œæ ·å¼
    - æ”¯æŒä¿ç•™åŸæ–‡ï¼ˆåŒè¯­æ¨¡å¼ï¼‰
    - è¾“å‡ºä¸º {filename}_translated.epub
    """
    
    # éœ€è¦ç¿»è¯‘çš„å—çº§æ ‡ç­¾
    BLOCK_TAGS = ['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'blockquote', 'pre', 'div']
    
    def __init__(self, settings: Settings):
        self.settings = settings
        self.retain_original = self._get_retain_original_setting()
        self.logger = logger
    
    def _get_retain_original_setting(self) -> bool:
        """ä» settings è·å–æ˜¯å¦ä¿ç•™åŸæ–‡çš„é…ç½®"""
        try:
            val = self.settings.processing.retain_original
            # æ˜¾å¼å¤„ç† None çš„æƒ…å†µ
            if val is None:
                return False
            return bool(val)
        except AttributeError:
            return False
    
    def render_to_file(
        self,
        segments: SegmentList,
        original_epub_path: Path,
        output_path: Path,
        title: str = "Document",
        translated_title: str = ""
    ) -> None:
        """
        å°†ç¿»è¯‘å†…å®¹å¡«å›åŸ EPUB æ–‡ä»¶ï¼Œç”Ÿæˆç¿»è¯‘ç‰ˆæœ¬
        
        Args:
            segments: ç¿»è¯‘åçš„ç‰‡æ®µåˆ—è¡¨
            original_epub_path: åŸ EPUB æ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            title: åŸå§‹æ–‡æ¡£æ ‡é¢˜
            translated_title: ç¿»è¯‘åçš„æ–‡æ¡£æ ‡é¢˜
        """
        self.logger.info(f"ğŸ“– å¼€å§‹æ¸²æŸ“ EPUB: {original_epub_path.name}")
        self.logger.info(f"   - ä¿ç•™åŸæ–‡æ¨¡å¼: {self.retain_original}")
        
        # 1. æ„å»ºæ–‡æœ¬æ˜ å°„è¡¨: åŸæ–‡ -> è¯‘æ–‡
        translation_map = self._build_translation_map(segments)
        self.logger.info(f"   - æ„å»ºæ˜ å°„è¡¨: {len(translation_map)} ä¸ªæ–‡æœ¬ç‰‡æ®µ")
        
        # è°ƒè¯•ï¼šè¾“å‡ºå‰3ä¸ªæ˜ å°„ç¤ºä¾‹
        if translation_map:
            self.logger.debug("   - æ˜ å°„è¡¨ç¤ºä¾‹ï¼ˆå‰3ä¸ªï¼‰:")
            for i, (orig, trans) in enumerate(list(translation_map.items())[:3]):
                orig_preview = orig[:50] + "..." if len(orig) > 50 else orig
                trans_preview = trans[:50] + "..." if len(trans) > 50 else trans
                self.logger.debug(f"     [{i+1}] {orig_preview} â†’ {trans_preview}")
        
        # 2. è¯»å–åŸ EPUB
        book = epub.read_epub(str(original_epub_path))
        
        # æ£€æŸ¥å¹¶è®°å½• TOC å’Œ spine ä¿¡æ¯
        self.logger.debug(f"   - åŸ EPUB æœ‰ TOC: {bool(book.toc)}")
        self.logger.debug(f"   - åŸ EPUB æœ‰ spine: {bool(book.spine)}")
        
        # è¯Šæ–­ TOC ç»“æ„
        if book.toc:
            self._diagnose_toc(book.toc)
        
        # ç«‹å³æ£€æŸ¥æ˜¯å¦æœ‰å†…å®¹ä¸º None çš„æ–‡æ¡£
        none_content_items = []
        for item in book.get_items():
            if item.get_type() == ebooklib.ITEM_DOCUMENT:
                try:
                    content = item.get_content()
                    if content is None:
                        none_content_items.append(item.get_name())
                except:
                    pass
        
        if none_content_items:
            self.logger.warning(f"   âš ï¸ åŸ EPUB ä¸­å‘ç° {len(none_content_items)} ä¸ªå†…å®¹ä¸º None çš„æ–‡æ¡£:")
            for name in none_content_items:
                self.logger.warning(f"      - {name}")
        
        # 3. æ›´æ–°å…ƒæ•°æ®ï¼ˆæ ‡é¢˜ï¼‰
        if translated_title:
            # æ›´æ–°ä¹¦å
            book.set_title(f"{translated_title} - {title}")
        
        # 4. æ›´æ–° TOCï¼ˆç›®å½•ç»“æ„ï¼‰ä¸­çš„æ ‡é¢˜
        # å¦‚æœ TOC æ›´æ–°å¤±è´¥ï¼Œä¸å½±å“ä¸»æµç¨‹
        try:
            if book.toc:
                self._update_toc(book, translation_map)
            else:
                self.logger.warning("   âš ï¸ åŸ EPUB æ²¡æœ‰ TOC ç»“æ„")
        except Exception as e:
            self.logger.warning(f"   âš ï¸ æ›´æ–° TOC å¤±è´¥ï¼ˆè·³è¿‡ï¼‰: {e}")
            import traceback
            self.logger.debug(traceback.format_exc())
        
        # 5. ç»Ÿè®¡æ‰€æœ‰èµ„æºç±»å‹
        resource_stats = defaultdict(int)
        for item in book.get_items():
            item_type = item.get_type()
            resource_stats[item_type] += 1
        
        # è¾“å‡ºèµ„æºç»Ÿè®¡
        type_names = {
            ebooklib.ITEM_UNKNOWN: "æœªçŸ¥",
            ebooklib.ITEM_DOCUMENT: "æ–‡æ¡£",
            ebooklib.ITEM_IMAGE: "å›¾ç‰‡",
            ebooklib.ITEM_STYLE: "æ ·å¼è¡¨",
            ebooklib.ITEM_SCRIPT: "è„šæœ¬",
            ebooklib.ITEM_FONT: "å­—ä½“",
            ebooklib.ITEM_NAVIGATION: "å¯¼èˆª",
            ebooklib.ITEM_COVER: "å°é¢"
        }
        self.logger.info("   ğŸ“¦ åŸ EPUB èµ„æºç»Ÿè®¡:")
        for item_type, count in sorted(resource_stats.items()):
            type_name = type_names.get(item_type, f"ç±»å‹{item_type}")
            self.logger.info(f"      - {type_name}: {count} ä¸ª")
        
        # 6. éå†æ‰€æœ‰æ–‡æ¡£é¡¹ï¼Œæ›¿æ¢æ–‡æœ¬å†…å®¹
        modified_count = 0
        total_replacements = 0
        
        for item in book.get_items():
            if item.get_type() != ebooklib.ITEM_DOCUMENT:
                continue
            
            item_name = item.get_name()
            
            # ä¿®æ”¹ HTML å†…å®¹
            try:
                original_content = item.get_content()
                
                # é˜²æŠ¤ï¼šå¦‚æœå†…å®¹ä¸ºç©ºï¼Œè·³è¿‡
                if not original_content:
                    self.logger.debug(f"   âŠ˜ è·³è¿‡ç©ºå†…å®¹: {item_name}")
                    continue
                
                modified_content, replacements = self._replace_text_in_html(
                    original_content,
                    translation_map
                )
                
                # é˜²æŠ¤ï¼šå¦‚æœè¿”å›çš„å†…å®¹ä¸º Noneï¼Œä¿æŒåŸå†…å®¹ä¸å˜
                if modified_content is None:
                    self.logger.warning(f"   âš ï¸ {item_name} å¤„ç†åè¿”å› Noneï¼Œä¿æŒåŸå†…å®¹")
                    # ä¸è°ƒç”¨ set_contentï¼Œä¿æŒåŸå†…å®¹
                    continue
                
                # åªæœ‰åœ¨æœ‰æ›¿æ¢æ—¶æ‰æ›´æ–°å†…å®¹
                if replacements > 0:
                    item.set_content(modified_content)
                    modified_count += 1
                    total_replacements += replacements
                    self.logger.debug(f"   âœ“ å·²æ›´æ–°: {item_name} ({replacements} å¤„)")
                # å¦‚æœæ²¡æœ‰æ›¿æ¢ï¼Œä¹Ÿä¸éœ€è¦ set_contentï¼Œä¿æŒåŸå†…å®¹
                    
            except Exception as e:
                self.logger.warning(f"   âš ï¸ å¤„ç† {item_name} æ—¶å‡ºé”™: {e}")
                import traceback
                self.logger.debug(traceback.format_exc())
                # å‘ç”Ÿå¼‚å¸¸æ—¶ä¸ä¿®æ”¹ itemï¼Œä¿æŒåŸå†…å®¹
                continue
        
        # 7. ç¡®ä¿ç”Ÿæˆ NCX å’Œ Nav æ–‡ä»¶
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ NCX/Navï¼Œå¦‚æœæ²¡æœ‰åˆ™æ·»åŠ 
        has_ncx = False
        has_nav = False
        
        for item in book.get_items():
            if isinstance(item, epub.EpubNcx):
                has_ncx = True
            elif isinstance(item, epub.EpubNav):
                has_nav = True
        
        if not has_ncx:
            self.logger.info("   + æ·»åŠ  NCX æ–‡ä»¶ï¼ˆEPUB 2.0 å…¼å®¹ï¼‰")
            book.add_item(epub.EpubNcx())
        
        if not has_nav:
            self.logger.info("   + æ·»åŠ  Nav æ–‡ä»¶ï¼ˆEPUB 3.0 å…¼å®¹ï¼‰")
            book.add_item(epub.EpubNav())
        
        # 8. ä¿å­˜è¾“å‡º
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ç¡®ä¿ spine å­˜åœ¨ï¼ˆé˜…è¯»é¡ºåºï¼‰
        if not book.spine:
            self.logger.warning("   âš ï¸ åŸ EPUB æ²¡æœ‰ spineï¼Œå¯èƒ½å¯¼è‡´é˜…è¯»é¡ºåºæ··ä¹±")
        
        # æ£€æŸ¥æ‰€æœ‰ items çš„å†…å®¹æ˜¯å¦ä¸º None
        self.logger.debug("   ğŸ” æ£€æŸ¥æ‰€æœ‰ items å†…å®¹...")
        items_with_none_content = []
        for item in book.get_items():
            try:
                content = item.get_content()
                if content is None and item.get_type() == ebooklib.ITEM_DOCUMENT:
                    items_with_none_content.append(item.get_name())
                    self.logger.warning(f"   âš ï¸ å‘ç°å†…å®¹ä¸º None çš„æ–‡æ¡£: {item.get_name()}")
            except Exception as e:
                self.logger.debug(f"   æ£€æŸ¥ {item.get_name()} æ—¶å‡ºé”™: {e}")
        
        if items_with_none_content:
            self.logger.warning(f"   âš ï¸ å‘ç° {len(items_with_none_content)} ä¸ªå†…å®¹ä¸º None çš„æ–‡æ¡£")
            self.logger.warning(f"   å°è¯•ä» book ä¸­ç§»é™¤è¿™äº›æ–‡æ¡£ä»¥ç»§ç»­...")
            # ä» items åˆ—è¡¨ä¸­ç§»é™¤è¿™äº›æœ‰é—®é¢˜çš„æ–‡æ¡£
            book.items = [item for item in book.items if item.get_name() not in items_with_none_content]
        
        # å†™å…¥ EPUBï¼Œç¡®ä¿ç”Ÿæˆ NCX æ–‡ä»¶ï¼ˆEPUB 2.0 å…¼å®¹ï¼‰
        # ebooklib ä¼šè‡ªåŠ¨æ ¹æ® book.toc å’Œ book.spine ç”Ÿæˆ toc.ncx
        try:
            epub.write_epub(str(output_path), book, {})
            self.logger.info(f"âœ… EPUB æ¸²æŸ“å®Œæˆ: {output_path}")
        except Exception as e:
            self.logger.error(f"âŒ å†™å…¥ EPUB å¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            raise
        
        self.logger.info(f"   - ä¿®æ”¹äº† {modified_count} ä¸ªæ–‡æ¡£ï¼Œå…± {total_replacements} å¤„æ›¿æ¢")
        self.logger.info(f"   - ä¿ç•™äº†æ‰€æœ‰éæ–‡æ¡£èµ„æºï¼ˆå›¾ç‰‡ã€æ ·å¼ã€å­—ä½“ç­‰ï¼‰")
        self.logger.info(f"   - å·²ç”Ÿæˆ NCX ç›®å½•æ–‡ä»¶ï¼ˆEPUB 2.0/3.0 å…¼å®¹ï¼‰")
    
    def _diagnose_toc(self, toc_items, level=0):
        """è¯Šæ–­ TOC ç»“æ„ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ç¼ºå¤±çš„ uid"""
        for item in toc_items:
            if isinstance(item, tuple):
                section = item[0]
                children = item[1] if len(item) > 1 else []
                
                # æ£€æŸ¥å¿…éœ€å±æ€§
                has_uid = hasattr(section, 'uid') and section.uid is not None
                title = getattr(section, 'title', 'No title')
                
                if not has_uid:
                    self.logger.warning(f"   {'  ' * level}âš ï¸ TOC Section ç¼ºå°‘ uid: {title}")
                else:
                    self.logger.debug(f"   {'  ' * level}âœ“ {title} (uid: {section.uid})")
                
                if children:
                    self._diagnose_toc(children, level + 1)
            else:
                # å•ä¸ª item
                has_uid = hasattr(item, 'uid') and item.uid is not None
                title = getattr(item, 'title', 'No title')
                
                if not has_uid:
                    self.logger.warning(f"   {'  ' * level}âš ï¸ TOC item ç¼ºå°‘ uid: {title}")
                else:
                    self.logger.debug(f"   {'  ' * level}âœ“ {title} (uid: {item.uid})")
    
    def _generate_uid(self, title: str, index: int) -> str:
        """ä¸º TOC item ç”Ÿæˆå”¯ä¸€çš„ uid"""
        import re
        # æ¸…ç†æ ‡é¢˜ï¼Œåªä¿ç•™å­—æ¯æ•°å­—å’Œä¸‹åˆ’çº¿
        clean_title = re.sub(r'[^\w\s-]', '', title)
        clean_title = re.sub(r'[\s-]+', '_', clean_title.strip())
        clean_title = clean_title.lower()[:50]  # é™åˆ¶é•¿åº¦
        return f"toc_{clean_title}_{index}" if clean_title else f"toc_item_{index}"
    
    def _update_toc(self, book: epub.EpubBook, translation_map: Dict[str, str]) -> None:
        """
        æ›´æ–° EPUB çš„ TOCï¼ˆç›®å½•ï¼‰ç»“æ„ä¸­çš„æ ‡é¢˜
        
        å¤„ç†ä¸¤ç§æƒ…å†µï¼š
        1. EPUB 2.x: NCX æ–‡ä»¶
        2. EPUB 3.x: NAV æ–‡ä»¶
        
        æ³¨æ„ï¼šåªæ›´æ–°æ ‡é¢˜ï¼Œä¸ä¿®æ”¹ TOC ç»“æ„ï¼Œç¡®ä¿ uid ç­‰å±æ€§ä¸ä¸¢å¤±
        å¦‚æœ item ç¼ºå°‘ uidï¼Œä¼šè‡ªåŠ¨ç”Ÿæˆä¸€ä¸ª
        """
        # ç”¨äºç”Ÿæˆå”¯ä¸€ uid çš„è®¡æ•°å™¨
        uid_counter = {'count': 0}
        
        # é€’å½’æ›´æ–° TOC æ ‘
        def update_toc_recursive(toc_items):
            """é€’å½’æ›´æ–° TOC é¡¹ï¼Œç¡®ä¿ä¿ç•™æ‰€æœ‰åŸå§‹å±æ€§ï¼Œç¼ºå¤±çš„ uid ä¼šè‡ªåŠ¨è¡¥å……"""
            updated = []
            for item in toc_items:
                if isinstance(item, tuple):
                    # (Section, [å­é¡¹])
                    section = item[0]
                    children = item[1] if len(item) > 1 else []
                    
                    # å¦‚æœç¼ºå°‘ uidï¼Œè‡ªåŠ¨ç”Ÿæˆä¸€ä¸ª
                    if not hasattr(section, 'uid') or section.uid is None:
                        title = getattr(section, 'title', 'Unknown')
                        section.uid = self._generate_uid(title, uid_counter['count'])
                        uid_counter['count'] += 1
                        self.logger.info(f"   + ä¸º TOC item ç”Ÿæˆ uid: {title} -> {section.uid}")
                    
                    # ç¿»è¯‘æ ‡é¢˜
                    if hasattr(section, 'title') and section.title:
                        original_title = section.title
                        normalized = self._normalize_text(original_title)
                        translated = translation_map.get(normalized)
                        
                        if not translated:
                            translated = self._fuzzy_match(normalized, translation_map)
                        
                        if translated:
                            section.title = translated
                            self.logger.debug(f"   TOC: {original_title} â†’ {translated}")
                    
                    # é€’å½’å¤„ç†å­é¡¹
                    if children:
                        updated_children = update_toc_recursive(children)
                        updated.append((section, updated_children))
                    else:
                        updated.append(section)
                else:
                    # å•ä¸ª Section
                    # å¦‚æœç¼ºå°‘ uidï¼Œè‡ªåŠ¨ç”Ÿæˆä¸€ä¸ª
                    if not hasattr(item, 'uid') or item.uid is None:
                        title = getattr(item, 'title', 'Unknown')
                        item.uid = self._generate_uid(title, uid_counter['count'])
                        uid_counter['count'] += 1
                        self.logger.info(f"   + ä¸º TOC item ç”Ÿæˆ uid: {title} -> {item.uid}")
                    
                    if hasattr(item, 'title') and item.title:
                        original_title = item.title
                        normalized = self._normalize_text(original_title)
                        translated = translation_map.get(normalized)
                        
                        if not translated:
                            translated = self._fuzzy_match(normalized, translation_map)
                        
                        if translated:
                            item.title = translated
                            self.logger.debug(f"   TOC: {original_title} â†’ {translated}")
                    
                    updated.append(item)
            
            return updated
        
        # æ›´æ–°ä¸» TOC
        if book.toc:
            try:
                book.toc = update_toc_recursive(book.toc)
                self.logger.info("   âœ“ å·²æ›´æ–° TOC ç›®å½•ç»“æ„")
            except Exception as e:
                self.logger.error(f"   âŒ æ›´æ–° TOC å¤±è´¥: {e}")
                import traceback
                self.logger.debug(traceback.format_exc())
                # å¤±è´¥æ—¶ä¸ä¿®æ”¹ TOC
                pass
    
    def _build_translation_map(self, segments: SegmentList) -> Dict[str, str]:
        """
        æ„å»ºæ–‡æœ¬æ˜ å°„è¡¨
        
        ä» segments ä¸­æå–åŸæ–‡åˆ°è¯‘æ–‡çš„æ˜ å°„
        
        Returns:
            {normalized_original_text: translated_text}
        """
        translation_map: Dict[str, str] = {}
        
        for seg in segments:
            # è·³è¿‡å›¾ç‰‡ç±»å‹
            if seg.content_type == "image":
                continue
            
            original = seg.original_text.strip()
            translated = seg.translated_text.strip() if seg.translated_text else ""
            
            if not original or not translated:
                continue
            
            # ç­–ç•¥1: å…ˆå°è¯•å®Œæ•´åŒ¹é…ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
            # è¿™æ ·å¯ä»¥ä¿ç•™å®Œæ•´çš„è¯‘æ–‡ï¼Œé¿å…æ®µè½æ‹†åˆ†å¯¼è‡´çš„ä¸¢å¤±
            normalized_full = self._normalize_text(original)
            if normalized_full and normalized_full not in translation_map:
                translation_map[normalized_full] = translated
            
            # ç­–ç•¥2: å°†é•¿æ–‡æœ¬æ‹†åˆ†ä¸ºæ®µè½è¿›è¡ŒåŒ¹é…ï¼ˆä½œä¸ºå¤‡é€‰ï¼‰
            # å› ä¸º parser æŒ‰æ®µè½ yieldï¼Œè€Œ segment å¯èƒ½åˆå¹¶äº†å¤šä¸ªæ®µè½
            original_paragraphs = [p.strip() for p in original.split('\n\n') if p.strip()]
            translated_paragraphs = [p.strip() for p in translated.split('\n\n') if p.strip()] if translated else []
            
            # é€æ®µè½å»ºç«‹æ˜ å°„ï¼Œå¤„ç†æ®µè½æ•°ä¸ä¸€è‡´çš„æƒ…å†µ
            max_paras = max(len(original_paragraphs), len(translated_paragraphs))
            
            for i in range(max_paras):
                orig_para = original_paragraphs[i] if i < len(original_paragraphs) else ""
                trans_para = translated_paragraphs[i] if i < len(translated_paragraphs) else ""
                
                if orig_para and trans_para:
                    normalized = self._normalize_text(orig_para)
                    if normalized and normalized not in translation_map:
                        translation_map[normalized] = trans_para
                elif not orig_para and trans_para:
                    # è¯‘æ–‡æ®µè½å¤šäºåŸæ–‡æ®µè½çš„æƒ…å†µï¼ˆå¯èƒ½æ˜¯ç¿»è¯‘æ‰©å±•ï¼‰
                    # å°†å¤šä½™çš„è¯‘æ–‡é™„åŠ åˆ°æœ€åä¸€ä¸ªåŸæ–‡æ®µè½çš„è¯‘æ–‡ä¸­
                    if original_paragraphs:
                        last_orig_normalized = self._normalize_text(original_paragraphs[-1])
                        if last_orig_normalized in translation_map:
                            # è¿½åŠ åˆ°å·²æœ‰è¯‘æ–‡
                            translation_map[last_orig_normalized] += "\n\n" + trans_para
        
        return translation_map
    
    def _replace_text_in_html(
        self,
        html_content: bytes,
        translation_map: Dict[str, str]
    ) -> Tuple[bytes, int]:
        """
        åœ¨ HTML å†…å®¹ä¸­æ›¿æ¢æ–‡æœ¬
        
        Args:
            html_content: åŸå§‹ HTML å­—èŠ‚å†…å®¹
            translation_map: åŸæ–‡åˆ°è¯‘æ–‡çš„æ˜ å°„
        
        Returns:
            (ä¿®æ”¹åçš„ HTML å­—èŠ‚å†…å®¹, æ›¿æ¢æ¬¡æ•°)
        """
        # é˜²æŠ¤ï¼šç©ºå†…å®¹ç›´æ¥è¿”å›
        if not html_content:
            return html_content or b'', 0
        
        # è§£æ HTML
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # æŸ¥æ‰¾ body
        body = soup.find('body') or soup
        
        # è®°å½•æ›¿æ¢æ¬¡æ•°
        replacement_count = 0
        
        # éå†æ‰€æœ‰å—çº§æ ‡ç­¾ï¼ˆä»å†…åˆ°å¤–ï¼Œé¿å…é‡å¤å¤„ç†ï¼‰
        processed_tags: Set[int] = set()
        
        for tag in body.find_all(self.BLOCK_TAGS):
            tag_id = id(tag)
            
            # æ£€æŸ¥æ˜¯å¦å·²è¢«çˆ¶æ ‡ç­¾å¤„ç†è¿‡
            if tag_id in processed_tags:
                continue
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å·²å¤„ç†çš„å­æ ‡ç­¾ï¼ˆè·³è¿‡å®¹å™¨æ ‡ç­¾ï¼‰
            has_processed_child = False
            for child in tag.find_all(self.BLOCK_TAGS):
                if id(child) in processed_tags:
                    has_processed_child = True
                    break
            
            if has_processed_child:
                continue
            
            # å¤„ç†è¯¥æ ‡ç­¾ï¼Œä¼ å…¥ soup ç”¨äºåˆ›å»ºæ–°æ ‡ç­¾
            result = self._process_tag(tag, translation_map, soup)
            if result:
                replacement_count += 1
                processed_tags.add(tag_id)
        
        # è¿”å›ä¿®æ”¹åçš„ HTMLï¼ˆæ·»åŠ é˜²æŠ¤ï¼‰
        try:
            html_str = str(soup)
            if html_str is None:
                self.logger.warning("BeautifulSoup è½¬æ¢ä¸ºå­—ç¬¦ä¸²è¿”å› None")
                return html_content, 0  # è¿”å›åŸå†…å®¹
            return html_str.encode('utf-8'), replacement_count
        except Exception as e:
            self.logger.warning(f"HTML åºåˆ—åŒ–å¤±è´¥: {e}")
            return html_content, 0  # è¿”å›åŸå†…å®¹
    
    def _normalize_text(self, text: str) -> str:
        """è§„æ•´åŒ–æ–‡æœ¬ï¼Œç”¨äºåŒ¹é…æ¯”è¾ƒ"""
        if not text:
            return ""
        # å»é™¤å¤šä½™ç©ºç™½ï¼Œä¿ç•™å•ä¸ªç©ºæ ¼
        normalized = re.sub(r'\s+', ' ', text.strip())
        return normalized
    
    def _markdown_to_html(self, text: str) -> str:
        """
        å°†æ–‡æœ¬ä¸­çš„ Markdown è¯­æ³•è½¬æ¢ä¸º HTML æ ‡ç­¾
        
        æ”¯æŒçš„è¯­æ³•ï¼š
        - **bold** â†’ <strong>bold</strong>
        - *italic* â†’ <em>italic</em>
        - ~~strikethrough~~ â†’ <del>strikethrough</del>
        - `code` â†’ <code>code</code>
        - [link](url) â†’ <a href="url">link</a>
        """
        if not text:
            return text
        
        # 1. å¤„ç†é“¾æ¥ [text](url)
        # æ³¨æ„ï¼šéœ€è¦å…ˆå¤„ç†é“¾æ¥ï¼Œé¿å…ä¸å…¶ä»–è¯­æ³•å†²çª
        text = re.sub(
            r'\[([^\]]+)\]\(([^\)]+)\)',
            r'<a href="\2">\1</a>',
            text
        )
        
        # 2. å¤„ç†è¡Œå†…ä»£ç  `code`
        text = re.sub(
            r'`([^`]+)`',
            r'<code>\1</code>',
            text
        )
        
        # 3. å¤„ç†åˆ é™¤çº¿ ~~text~~
        text = re.sub(
            r'~~([^~]+)~~',
            r'<del>\1</del>',
            text
        )
        
        # 4. å¤„ç†ç²—ä½“ **text** æˆ– __text__
        # æ³¨æ„ï¼šéœ€è¦ä½¿ç”¨éè´ªå©ªåŒ¹é…ï¼Œé¿å…è·¨æ®µåŒ¹é…
        text = re.sub(
            r'\*\*(.+?)\*\*',
            r'<strong>\1</strong>',
            text
        )
        text = re.sub(
            r'__(.+?)__',
            r'<strong>\1</strong>',
            text
        )
        
        # 5. å¤„ç†æ–œä½“ *text* æˆ– _text_
        # æ³¨æ„ï¼šéœ€è¦é¿å…åŒ¹é…å·²ç»è½¬æ¢çš„ <strong> æ ‡ç­¾ä¸­çš„å†…å®¹
        # ä½¿ç”¨è´Ÿå‘å‰ç»å’Œè´Ÿå‘åé¡¾æ¥é¿å…åŒ¹é…æ˜Ÿå·å‘¨å›´çš„å­—æ¯
        text = re.sub(
            r'(?<![*\w])\*([^\*]+?)\*(?![*\w])',
            r'<em>\1</em>',
            text
        )
        text = re.sub(
            r'(?<!_)\b_([^_]+?)_\b(?!_)',
            r'<em>\1</em>',
            text
        )
        
        return text
    
    def _process_tag(self, tag: Tag, translation_map: Dict[str, str], soup: BeautifulSoup) -> bool:
        """
        å¤„ç†å•ä¸ªæ ‡ç­¾ï¼Œæ›¿æ¢å…¶æ–‡æœ¬å†…å®¹
        
        Args:
            tag: BeautifulSoup æ ‡ç­¾å¯¹è±¡
            translation_map: åŸæ–‡åˆ°è¯‘æ–‡çš„æ˜ å°„
            soup: BeautifulSoup å¯¹è±¡ï¼Œç”¨äºåˆ›å»ºæ–°æ ‡ç­¾
            
        Returns:
            æ˜¯å¦æˆåŠŸæ›¿æ¢
        """
        # è·å–æ ‡ç­¾çš„çº¯æ–‡æœ¬å†…å®¹
        original_text = tag.get_text(separator=' ', strip=True)
        normalized_text = self._normalize_text(original_text)
        
        if not normalized_text:
            return False
        
        # æŸ¥æ‰¾ç¿»è¯‘
        translated_text = translation_map.get(normalized_text)
        
        if not translated_text:
            # å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆå¤„ç†å°å·®å¼‚ï¼‰
            translated_text = self._fuzzy_match(normalized_text, translation_map)
        
        if not translated_text:
            return False
        
        # è°ƒè¯•ï¼šæ£€æŸ¥è¯‘æ–‡é•¿åº¦
        if len(original_text) > 50 or len(translated_text) > 50:
            self.logger.debug(f"   [åŒ¹é…æˆåŠŸ] åŸæ–‡: {original_text[:50]}... ({len(original_text)} å­—ç¬¦)")
            self.logger.debug(f"   [åŒ¹é…æˆåŠŸ] è¯‘æ–‡: {translated_text[:50]}... ({len(translated_text)} å­—ç¬¦)")
        
        # æ›¿æ¢å†…å®¹
        if self.retain_original:
            # ä¿ç•™åŸæ–‡æ¨¡å¼ï¼šå…ˆè¯‘æ–‡ï¼Œå†åŸæ–‡
            self._replace_tag_content_bilingual(tag, translated_text, original_text, soup)
        else:
            # çº¯è¯‘æ–‡æ¨¡å¼
            self._replace_tag_content(tag, translated_text)
        
        return True
    
    def _fuzzy_match(self, text: str, translation_map: Dict[str, str]) -> Optional[str]:
        """
        æ¨¡ç³ŠåŒ¹é…ç¿»è¯‘
        
        å¤„ç†ç”±äºç©ºç™½å­—ç¬¦å·®å¼‚å¯¼è‡´çš„åŒ¹é…å¤±è´¥
        """
        if len(text) < 10:
            return None
            
        # å°è¯•æ‰¾åˆ°æœ€ç›¸ä¼¼çš„åŒ¹é…
        best_match = None
        best_score = 0.0
        
        for orig, trans in translation_map.items():
            if len(orig) < 10:
                continue
                
            score = self._similarity(text, orig)
            if score > best_score and score > 0.85:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                best_score = score
                best_match = trans
        
        return best_match
    
    def _similarity(self, s1: str, s2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦"""
        if not s1 or not s2:
            return 0.0
        
        # å¯¹äºçŸ­æ–‡æœ¬ï¼Œä½¿ç”¨ç®€å•çš„å­ä¸²åŒ¹é…
        if len(s1) < 50 or len(s2) < 50:
            shorter = min(s1, s2, key=len)
            longer = max(s1, s2, key=len)
            if shorter in longer:
                return len(shorter) / len(longer)
        
        # ä½¿ç”¨è¯é›†åˆçš„ Jaccard ç›¸ä¼¼åº¦
        # å¯¹ä¸­æ–‡ä½¿ç”¨å­—ç¬¦çº§åˆ«
        if self._contains_cjk(s1) or self._contains_cjk(s2):
            set1 = set(s1.replace(' ', ''))
            set2 = set(s2.replace(' ', ''))
        else:
            set1 = set(s1.lower().split())
            set2 = set(s2.lower().split())
        
        if not set1 or not set2:
            return 0.0
        
        intersection = len(set1 & set2)
        union = len(set1 | set2)
        
        return intersection / union if union > 0 else 0.0
    
    def _contains_cjk(self, text: str) -> bool:
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å« CJK å­—ç¬¦"""
        for char in text:
            if '\u4e00' <= char <= '\u9fff':  # CJK ç»Ÿä¸€æ±‰å­—
                return True
            if '\u3040' <= char <= '\u309f':  # å¹³å‡å
                return True
            if '\u30a0' <= char <= '\u30ff':  # ç‰‡å‡å
                return True
        return False
    
    def _replace_tag_content(self, tag: Tag, new_text: str) -> None:
        """
        æ›¿æ¢æ ‡ç­¾å†…å®¹ä¸ºçº¯è¯‘æ–‡
        
        ä¿ç•™æ ‡ç­¾æœ¬èº«çš„å±æ€§ï¼Œä½†ç”¨æ–°æ–‡æœ¬æ›¿æ¢å†…å®¹
        """
        # è½¬æ¢ Markdown è¯­æ³•ä¸º HTML
        html_text = self._markdown_to_html(new_text)
        
        # æ¸…ç©ºå†…å®¹
        tag.clear()
        
        # å¦‚æœåŒ…å« HTML æ ‡ç­¾ï¼Œéœ€è¦è§£æåæ’å…¥
        if '<' in html_text and '>' in html_text:
            # ä½¿ç”¨ BeautifulSoup è§£æ HTML ç‰‡æ®µ
            # æ³¨æ„ï¼šéœ€è¦åŒ…è£…åœ¨ä¸€ä¸ªå®¹å™¨æ ‡ç­¾ä¸­ä»¥æ­£ç¡®è§£æ
            temp_soup = BeautifulSoup(f'<div>{html_text}</div>', 'html.parser')
            # æå–å®¹å™¨å†…çš„æ‰€æœ‰å­å…ƒç´ 
            for element in temp_soup.div.children:
                # å¤åˆ¶å…ƒç´ ä»¥é¿å…ç§»åŠ¨é—®é¢˜
                if isinstance(element, NavigableString):
                    tag.append(NavigableString(str(element)))
                else:
                    tag.append(element)
        else:
            # çº¯æ–‡æœ¬ï¼Œç›´æ¥æ·»åŠ 
            tag.append(NavigableString(html_text))
    
    def _replace_tag_content_bilingual(
        self,
        tag: Tag,
        translated_text: str,
        original_text: str,
        soup: BeautifulSoup
    ) -> None:
        """
        æ›¿æ¢æ ‡ç­¾å†…å®¹ä¸ºåŒè¯­ï¼ˆå…ˆè¯‘æ–‡ååŸæ–‡ï¼‰
        
        æ ¼å¼ï¼š
        <p>åŸæ–‡</p> â†’ <p><span class="translated">è¯‘æ–‡</span><br/><span class="original">åŸæ–‡</span></p>
        """
        # æ¸…ç©ºåŸå†…å®¹
        tag.clear()
        
        # è½¬æ¢è¯‘æ–‡çš„ Markdown è¯­æ³•ä¸º HTML
        translated_html = self._markdown_to_html(translated_text)
        
        # è°ƒè¯•ï¼šæ£€æŸ¥è¯‘æ–‡é•¿åº¦
        if len(translated_text) > 100:
            self.logger.debug(f"   [åŒè¯­æ¨¡å¼] è¯‘æ–‡é•¿åº¦: {len(translated_text)} å­—ç¬¦")
            self.logger.debug(f"   [åŒè¯­æ¨¡å¼] è¯‘æ–‡é¢„è§ˆ: {translated_text[:100]}...")
        
        # åˆ›å»ºè¯‘æ–‡ span
        trans_span = soup.new_tag('span')
        trans_span['class'] = 'translated'
        
        # å¦‚æœè¯‘æ–‡åŒ…å« HTML æ ‡ç­¾ï¼Œéœ€è¦è§£æåæ’å…¥
        if '<' in translated_html and '>' in translated_html:
            temp_soup = BeautifulSoup(f'<div>{translated_html}</div>', 'html.parser')
            for element in temp_soup.div.children:
                if isinstance(element, NavigableString):
                    trans_span.append(NavigableString(str(element)))
                else:
                    trans_span.append(element)
        else:
            trans_span.string = translated_html
        
        # åˆ›å»ºæ¢è¡Œ
        br = soup.new_tag('br')
        
        # è½¬æ¢åŸæ–‡çš„ Markdown è¯­æ³•ä¸º HTMLï¼ˆåŸæ–‡å¯èƒ½ä¹Ÿæœ‰æ ¼å¼ï¼‰
        original_html = self._markdown_to_html(original_text)
        
        # åˆ›å»ºåŸæ–‡ span
        orig_span = soup.new_tag('span')
        orig_span['class'] = 'original'
        orig_span['style'] = 'color: #999; font-size: 0.9em;'
        
        # å¦‚æœåŸæ–‡åŒ…å« HTML æ ‡ç­¾ï¼Œéœ€è¦è§£æåæ’å…¥
        if '<' in original_html and '>' in original_html:
            temp_soup = BeautifulSoup(f'<div>{original_html}</div>', 'html.parser')
            for element in temp_soup.div.children:
                if isinstance(element, NavigableString):
                    orig_span.append(NavigableString(str(element)))
                else:
                    orig_span.append(element)
        else:
            orig_span.string = original_html
        
        # æŒ‰é¡ºåºæ·»åŠ ï¼šå…ˆè¯‘æ–‡ï¼Œå†åŸæ–‡
        tag.append(trans_span)
        tag.append(br)
        tag.append(orig_span)
        
        # è°ƒè¯•ï¼šéªŒè¯æ·»åŠ åçš„å†…å®¹
        result_text = tag.get_text()
        if len(result_text) < len(translated_text) * 0.5:
            self.logger.warning(f"   âš ï¸ [åŒè¯­æ¨¡å¼] è¯‘æ–‡å¯èƒ½ä¸¢å¤±ï¼åŸè¯‘æ–‡ {len(translated_text)} å­—ç¬¦ï¼Œç»“æœåªæœ‰ {len(result_text)} å­—ç¬¦")


def render_epub(
    segments: SegmentList,
    original_epub_path: Path,
    output_path: Path,
    settings: Settings,
    title: str = "Document",
    translated_title: str = ""
) -> None:
    """
    ä¾¿æ·å‡½æ•°ï¼šæ¸²æŸ“ EPUB æ–‡ä»¶
    
    Args:
        segments: ç¿»è¯‘åçš„ç‰‡æ®µåˆ—è¡¨
        original_epub_path: åŸ EPUB æ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        settings: å…¨å±€è®¾ç½®
        title: åŸå§‹æ–‡æ¡£æ ‡é¢˜
        translated_title: ç¿»è¯‘åçš„æ–‡æ¡£æ ‡é¢˜
    """
    renderer = EPUBRenderer(settings)
    renderer.render_to_file(
        segments=segments,
        original_epub_path=original_epub_path,
        output_path=output_path,
        title=title,
        translated_title=translated_title
    )


# ============================================================================
# HTML to EPUB è½¬æ¢å™¨ï¼ˆç”¨äº PDF ç­‰é EPUB æºæ–‡ä»¶ç”Ÿæˆ EPUBï¼‰
# ============================================================================

class HTMLToEPUBConverter:
    """
    HTML â†’ EPUB è½¬æ¢å™¨
    
    èŒè´£ï¼š
    - æ¥æ”¶æ¸²æŸ“å¥½çš„ HTML å†…å®¹
    - æŒ‰ç« èŠ‚åˆ†å‰²å¹¶åˆ›å»º EPUB ç»“æ„
    - ä¿ç•™æ ·å¼å’Œæ ¼å¼
    - ç”Ÿæˆç¬¦åˆ EPUB 3 æ ‡å‡†çš„ç”µå­ä¹¦
    
    é€‚ç”¨åœºæ™¯ï¼šPDF/TXT ç­‰é EPUB æºæ–‡ä»¶ç¿»è¯‘åè¾“å‡º EPUB
    """
    
    def __init__(self, settings: Settings):
        self.settings = settings
        self.logger = logger
    
    def convert_to_epub(
        self,
        html_content: str,
        output_path: Path,
        title: str = "Document",
        translated_title: str = "",
        author: str = "Unknown",
        language: str = "zh"
    ) -> None:
        """
        å°† HTML å†…å®¹è½¬æ¢ä¸º EPUB æ–‡ä»¶
        
        Args:
            html_content: å®Œæ•´çš„ HTML å†…å®¹ï¼ˆåŒ…å« <html>, <body> ç­‰æ ‡ç­¾ï¼‰
            output_path: è¾“å‡º EPUB æ–‡ä»¶è·¯å¾„
            title: åŸå§‹æ ‡é¢˜
            translated_title: ç¿»è¯‘åçš„æ ‡é¢˜
            author: ä½œè€…å
            language: è¯­è¨€ä»£ç ï¼ˆzh, en, ja ç­‰ï¼‰
        """
        self.logger.info(f"ğŸ“š å¼€å§‹ç”Ÿæˆ EPUB: {output_path.name}")
        
        # 1. åˆ›å»º EPUB ä¹¦ç±å¯¹è±¡
        book = epub.EpubBook()
        
        # 2. è®¾ç½®å…ƒæ•°æ®
        display_title = f"{translated_title} - {title}" if translated_title else title
        book.set_identifier(f'id_{output_path.stem}')
        book.set_title(display_title)
        book.set_language(language)
        book.add_author(author)
        
        # 3. æå– <body> å†…å®¹å’Œ CSS æ ·å¼
        body_content = self._extract_body_content(html_content)
        css_content = self._extract_css_content(html_content)
        
        # 4. æŒ‰ç« èŠ‚åˆ†å‰²å†…å®¹
        chapters = self._split_into_chapters(body_content)
        self.logger.info(f"   - æ£€æµ‹åˆ° {len(chapters)} ä¸ªç« èŠ‚")
        
        # 5. åˆ›å»º CSS æ ·å¼æ–‡ä»¶
        css = epub.EpubItem(
            uid="style",
            file_name="style.css",
            media_type="text/css",
            content=css_content.encode('utf-8') if css_content else self._get_default_css().encode('utf-8')
        )
        book.add_item(css)
        
        # 6. åˆ›å»ºç« èŠ‚å¯¹è±¡
        epub_chapters = []
        spine_items = ['nav']  # EPUB spine é¡ºåº
        toc_items = []  # ç›®å½•é¡¹
        
        for i, (chapter_title, chapter_html) in enumerate(chapters, 1):
            chapter_id = f'chapter_{i}'
            chapter_file = f'chapter_{i}.xhtml'
            
            # åˆ›å»ºç« èŠ‚ HTMLï¼ˆXHTML æ ¼å¼ï¼‰
            chapter_content = f'''<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
    <title>{chapter_title}</title>
    <link rel="stylesheet" type="text/css" href="style.css"/>
</head>
<body>
    {chapter_html}
</body>
</html>'''
            
            # åˆ›å»º EPUB ç« èŠ‚å¯¹è±¡
            chapter = epub.EpubHtml(
                title=chapter_title,
                file_name=chapter_file,
                lang=language,
                uid=chapter_id
            )
            chapter.content = chapter_content
            
            book.add_item(chapter)
            epub_chapters.append(chapter)
            spine_items.append(chapter)
            toc_items.append(chapter)
        
        # 7. è®¾ç½® TOCï¼ˆç›®å½•ï¼‰
        book.toc = tuple(toc_items)
        
        # 8. æ·»åŠ é»˜è®¤çš„ NCX å’Œ Nav æ–‡ä»¶
        book.add_item(epub.EpubNcx())
        book.add_item(epub.EpubNav())
        
        # 9. è®¾ç½® spineï¼ˆé˜…è¯»é¡ºåºï¼‰
        book.spine = spine_items
        
        # 10. ä¿å­˜ EPUB
        output_path.parent.mkdir(parents=True, exist_ok=True)
        epub.write_epub(str(output_path), book, {})
        
        self.logger.info(f"âœ… EPUB ç”Ÿæˆå®Œæˆ: {output_path}")
    
    def _extract_body_content(self, html_content: str) -> str:
        """æå– <body> æ ‡ç­¾å†…çš„å†…å®¹"""
        match = re.search(r'<body[^>]*>(.*?)</body>', html_content, re.DOTALL | re.IGNORECASE)
        if match:
            return match.group(1)
        return html_content  # å¦‚æœæ²¡æœ‰ body æ ‡ç­¾ï¼Œè¿”å›å…¨éƒ¨å†…å®¹
    
    def _extract_css_content(self, html_content: str) -> str:
        """æå– <style> æ ‡ç­¾å†…çš„ CSS å†…å®¹"""
        css_parts = []
        for match in re.finditer(r'<style[^>]*>(.*?)</style>', html_content, re.DOTALL | re.IGNORECASE):
            css_parts.append(match.group(1))
        return '\n\n'.join(css_parts)
    
    def _split_into_chapters(self, html_content: str) -> list:
        """
        æŒ‰æ ‡é¢˜åˆ†å‰² HTML å†…å®¹ä¸ºç« èŠ‚
        
        Returns:
            [(chapter_title, chapter_html), ...]
        """
        # æŸ¥æ‰¾æ‰€æœ‰ h2-h5 æ ‡é¢˜ï¼ˆç« èŠ‚åˆ†éš”ç¬¦ï¼‰
        chapter_pattern = r'(<h[2-5][^>]*>.*?</h[2-5]>)'
        
        # åˆ†å‰²å†…å®¹
        parts = re.split(chapter_pattern, html_content, flags=re.DOTALL | re.IGNORECASE)
        
        chapters = []
        current_title = "å‰è¨€"  # é»˜è®¤ç¬¬ä¸€ç« æ ‡é¢˜
        current_content = []
        
        for i, part in enumerate(parts):
            # æ£€æŸ¥æ˜¯å¦ä¸ºæ ‡é¢˜
            if re.match(r'<h[2-5]', part, re.IGNORECASE):
                # ä¿å­˜ä¸Šä¸€ç« èŠ‚
                if current_content:
                    chapters.append((current_title, ''.join(current_content)))
                    current_content = []
                
                # æå–æ–°æ ‡é¢˜æ–‡æœ¬ï¼ˆå»é™¤ emoji å’Œæ ‡ç­¾ï¼‰
                title_text = re.sub(r'<.*?>', '', part)
                title_text = re.sub(r'[ğŸ“šğŸ“–ğŸ“„ğŸ“ğŸ“ŒğŸ§­ğŸ”–]', '', title_text).strip()
                current_title = title_text or f"ç« èŠ‚ {len(chapters) + 1}"
                
                # æ ‡é¢˜æœ¬èº«ä¹ŸåŠ å…¥å†…å®¹
                current_content.append(part)
            else:
                # æ™®é€šå†…å®¹
                if part.strip():
                    current_content.append(part)
        
        # æ·»åŠ æœ€åä¸€ç« 
        if current_content:
            chapters.append((current_title, ''.join(current_content)))
        
        # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°ä»»ä½•ç« èŠ‚ï¼Œå°†å…¨éƒ¨å†…å®¹ä½œä¸ºä¸€ç« 
        if not chapters:
            chapters.append(("å…¨æ–‡", html_content))
        
        return chapters
    
    def _get_default_css(self) -> str:
        """è·å–é»˜è®¤çš„ EPUB CSS æ ·å¼"""
        return """
/* EPUB é»˜è®¤æ ·å¼ */
body {
    font-family: "Source Han Serif", "Noto Serif CJK", serif;
    line-height: 1.8;
    margin: 1em;
    text-align: justify;
}

h1, h2, h3, h4, h5, h6 {
    font-weight: bold;
    margin-top: 1em;
    margin-bottom: 0.5em;
    page-break-after: avoid;
}

h2 { font-size: 1.8em; }
h3 { font-size: 1.5em; }
h4 { font-size: 1.3em; }
h5 { font-size: 1.1em; }

p {
    margin: 0.8em 0;
    text-indent: 2em;
}

.content-block {
    margin: 1.5em 0;
    padding: 1em;
    border-left: 3px solid #3498db;
    background-color: #f8f9fa;
}

.page-marker {
    display: inline-block;
    font-size: 0.85em;
    color: #7f8c8d;
    margin-right: 0.5em;
}

/* åŒè¯­æ¨¡å¼æ ·å¼ */
.translated {
    display: block;
    margin-bottom: 0.5em;
}

.original {
    display: block;
    color: #666;
    font-size: 0.9em;
    font-style: italic;
    border-left: 2px solid #ddd;
    padding-left: 1em;
    margin-top: 0.5em;
}

/* å¼•ç”¨å—æ ·å¼ */
blockquote {
    border-left: 3px solid #ccc;
    padding-left: 1em;
    margin: 1em 0;
    color: #666;
    font-style: italic;
}

/* å›¾ç‰‡ */
img {
    max-width: 100%;
    height: auto;
    display: block;
    margin: 1em auto;
}

/* ä»£ç å— */
pre, code {
    font-family: 'Courier New', monospace;
    background-color: #f4f4f4;
    padding: 0.2em 0.4em;
    border-radius: 3px;
}

pre {
    padding: 1em;
    overflow-x: auto;
}
"""


def render_html_to_epub(
    html_content: str,
    output_path: Path,
    settings: Settings,
    title: str = "Document",
    translated_title: str = "",
    author: str = "Unknown"
) -> None:
    """
    ä¾¿æ·å‡½æ•°ï¼šå°† HTML å†…å®¹è½¬æ¢ä¸º EPUB
    
    Args:
        html_content: å®Œæ•´çš„ HTML å†…å®¹
        output_path: è¾“å‡ºè·¯å¾„
        settings: å…¨å±€è®¾ç½®
        title: åŸå§‹æ ‡é¢˜
        translated_title: ç¿»è¯‘åçš„æ ‡é¢˜
        author: ä½œè€…å
    """
    converter = HTMLToEPUBConverter(settings)
    converter.convert_to_epub(
        html_content=html_content,
        output_path=output_path,
        title=title,
        translated_title=translated_title,
        author=author
    )
