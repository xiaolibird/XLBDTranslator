"""
ç¿»è¯‘å·¥ä½œæµæ¨¡å—
"""
import asyncio
import json, os, traceback, signal, threading
from pathlib import Path
from typing import Dict, Optional, List

from ..core.schema import Settings, SegmentList, ContentSegment
from ..core.exceptions import TranslationError
from ..utils.logger import logger
from ..utils.file import create_output_directory, get_file_hash
from ..translator import GeminiTranslator, OpenAICompatibleTranslator, CheckpointManager
from ..parser.loader import load_document_structure as parse_document
from ..parser.helpers import is_likely_chinese
from ..renderer.markdown import MarkdownRenderer

# å°è¯•å¯¼å…¥ Rich è¿›åº¦æ˜¾ç¤º
try:
    from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn, TimeElapsedColumn
    from rich.console import Console
    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False


# å…¨å±€å¼•ç”¨ï¼Œç”¨äºä¿¡å·å¤„ç†å™¨è®¿é—®å½“å‰å·¥ä½œæµå®ä¾‹
_current_workflow: Optional['TranslationWorkflow'] = None


def _emergency_save_handler(signum, frame):
    """ç´§æ€¥ä¿å­˜ä¿¡å·å¤„ç†å™¨ï¼ˆæ•è· SIGTERM/SIGINTï¼‰"""
    global _current_workflow
    signal_name = signal.Signals(signum).name
    logger.warning(f"âš ï¸ æ”¶åˆ° {signal_name} ä¿¡å·ï¼Œå°è¯•ç´§æ€¥ä¿å­˜...")
    
    if _current_workflow is not None:
        try:
            _current_workflow._emergency_save()
            logger.info("âœ… ç´§æ€¥ä¿å­˜å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ ç´§æ€¥ä¿å­˜å¤±è´¥: {e}")
    
    # é‡æ–°æŠ›å‡ºä¿¡å·ï¼Œå…è®¸è¿›ç¨‹æ­£å¸¸ç»ˆæ­¢
    signal.signal(signum, signal.SIG_DFL)
    os.kill(os.getpid(), signum)


class TranslationWorkflow:
    """
    ç¿»è¯‘å·¥ä½œæµç±» - å°è£…å®Œæ•´çš„æ–‡æ¡£ç¿»è¯‘ä¸šåŠ¡é€»è¾‘
    
    èŒè´£ï¼š
    - æ–‡æ¡£åŠ è½½å’Œè§£æ
    - æ ‡é¢˜é¢„ç¿»è¯‘
    - æœ¯è¯­è¡¨ç”Ÿæˆå’Œç®¡ç†
    - æ‰¹é‡ç¿»è¯‘æ‰§è¡Œï¼ˆåŒæ­¥/å¼‚æ­¥ï¼‰
    - è¿›åº¦ç®¡ç†å’Œæ–­ç‚¹ç»­ä¼ 
    - æœ€ç»ˆæ–‡æ¡£æ¸²æŸ“
    """
    
    def __init__(self, settings: Settings):
        """
        åˆå§‹åŒ–ç¿»è¯‘å·¥ä½œæµ
        
        Args:
            settings: å…¨å±€è®¾ç½®å¯¹è±¡ï¼ŒåŒ…å«æ‰€æœ‰é…ç½®ä¿¡æ¯
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° (stage, progress, message)
        """
        global _current_workflow
        
        self.settings = settings
        self.file_path = settings.files.document_path
        self.file_hash = get_file_hash(self.file_path)
        self.project_name = self.file_hash
        
        # å‡†å¤‡å·¥ä½œç›®å½•
        self.project_dir = create_output_directory(
            settings.files.output_base_dir,
            self.project_name
        )
        self.structure_path = self.project_dir / "structure_map.json"
        
        # æ ¸å¿ƒç»„ä»¶ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self.all_segments: Optional[SegmentList] = None
        self._segment_index: Dict[int, int] = {}  # segment_id -> list index å¿«é€Ÿç´¢å¼•
        self.translator: Optional[GeminiTranslator] = None
        self.cache_manager = None
        self.checkpoint: Optional[CheckpointManager] = None
        self.glossary: Optional[Dict[str, str]] = None
        
        # æ³¨å†Œä¿¡å·å¤„ç†å™¨ï¼ˆç”¨äºç´§æ€¥ä¿å­˜ï¼‰
        _current_workflow = self
        try:
            signal.signal(signal.SIGTERM, _emergency_save_handler)
            signal.signal(signal.SIGINT, _emergency_save_handler)
            logger.debug("âœ… å·²æ³¨å†Œç´§æ€¥ä¿å­˜ä¿¡å·å¤„ç†å™¨ (SIGTERM/SIGINT)")
        except Exception as e:
            logger.warning(f"âš ï¸ æ— æ³•æ³¨å†Œä¿¡å·å¤„ç†å™¨: {e}")
    
    def _emergency_save(self) -> None:
        """ç´§æ€¥ä¿å­˜å½“å‰çŠ¶æ€ï¼ˆç”¨äºä¿¡å·å¤„ç†ï¼‰"""
        logger.warning("ğŸ†˜ æ‰§è¡Œç´§æ€¥ä¿å­˜...")
        try:
            if self.all_segments:
                self._save_structure_map(self.all_segments)
                logger.info(f"   - structure_map.json å·²ä¿å­˜ ({len(self.all_segments)} segments)")
            if self.checkpoint:
                self.checkpoint.save_checkpoint()
                logger.info("   - checkpoint å·²ä¿å­˜")
        except Exception as e:
            logger.error(f"âŒ ç´§æ€¥ä¿å­˜å¤±è´¥: {e}")

    # Deprecated: progress reporting removed for a cleaner workflow interface.
    # Progress callbacks were removed to simplify the TranslationWorkflow class.

    def _optimize_batch_size_for_provider(self):
        """æ ¹æ®translator providerä¼˜åŒ–batch_sizeï¼Œå¹³è¡¡é€Ÿåº¦å’Œcontexté™åˆ¶
        
        Returns:
            tuple: (æ˜¯å¦ä¼˜åŒ–, åŸå§‹batch_size, ä¼˜åŒ–åbatch_size, åŸå› )
        """
        provider = getattr(self.settings.api, 'translator_provider', 'gemini').lower()
        original_batch_size = self.settings.processing.batch_size
        max_chunk_size = self.settings.processing.max_chunk_size
        
        # ä¼°ç®—æ¯æ‰¹æ¬¡çš„å­—ç¬¦é‡ï¼ˆä¸å«promptï¼‰
        estimated_chars_per_batch = original_batch_size * max_chunk_size
        
        # æ ¹æ®providerè°ƒæ•´batch_size
        if provider in {'deepseek', 'openai', 'openai-compatible'}:
            # äº‘ç«¯APIä½¿ç”¨å®Œæ•´ç‰ˆpromptï¼Œå­—ç¬¦æ•°è¾ƒå¤šï¼Œéœ€è¦å‡å°‘batch_size
            # DeepSeek/OpenAI context limit çº¦32K-128Kï¼Œä½†è€ƒè™‘promptå¼€é”€ï¼Œä¿å®ˆè®¾ç½®ä¸º60Kä¸Šé™
            max_safe_chars = 60000
            prompt_overhead = 2000  # ä¼°ç®—å®Œæ•´ç‰ˆpromptçš„å­—ç¬¦æ•°
            
            # è®¡ç®—å®‰å…¨çš„batch_size
            safe_batch_size = max(1, (max_safe_chars - prompt_overhead) // max_chunk_size)
            optimized_batch_size = min(original_batch_size, safe_batch_size, 3)  # æœ€å¤š3ä½œä¸ºç¡¬ä¸Šé™
            
            logger.info(f"ğŸ”§ å‚æ•°ä¼˜åŒ–åˆ†æ ({provider.upper()}):")
            logger.info(f"   ğŸ“Š å½“å‰é…ç½®: batch_size={original_batch_size}, max_chunk_size={max_chunk_size}")
            logger.info(f"   ğŸ“ ä¼°ç®—å­—ç¬¦é‡: {estimated_chars_per_batch:,} å­—ç¬¦/æ‰¹æ¬¡ (ä¸å«prompt)")
            logger.info(f"   âš ï¸  å®‰å…¨ä¸Šé™: {max_safe_chars:,} å­—ç¬¦/æ‰¹æ¬¡ (å«{prompt_overhead:,}å­—ç¬¦promptå¼€é”€)")
            logger.info(f"   ğŸ¯ ä¼˜åŒ–ç»“æœ: batch_size {original_batch_size} â†’ {optimized_batch_size}")
            
            if optimized_batch_size < original_batch_size:
                new_estimated_chars = optimized_batch_size * max_chunk_size + prompt_overhead
                logger.info(f"   âœ… æ–°é…ç½®å­—ç¬¦é‡: {new_estimated_chars:,} å­—ç¬¦/æ‰¹æ¬¡ (å®‰å…¨èŒƒå›´å†…)")
                logger.info(f"   ğŸ’¡ å¤‡é€‰æ–¹æ¡ˆ: å¯è€ƒè™‘å‡å°‘ max_chunk_size è‡³ {max_chunk_size//2} ä»¥å¢åŠ batch_size")
            else:
                logger.info(f"   âœ… å½“å‰é…ç½®å·²å®‰å…¨: {estimated_chars_per_batch + prompt_overhead:,} å­—ç¬¦/æ‰¹æ¬¡")
            
            reason = f"äº‘ç«¯APIä½¿ç”¨å®Œæ•´ç‰ˆpromptï¼Œå‡å°‘batch_sizeé¿å…è¶…å‡ºcontexté™åˆ¶"
            
        elif provider == 'gemini':
            # Geminiä½¿ç”¨å®Œæ•´ç‰ˆpromptï¼Œä½†context windowè¾ƒå¤§ (1M+ tokens)
            # ä¿å®ˆè®¾ç½®ä¸Šé™ä¸º20ä¸‡å­—ç¬¦
            max_safe_chars = 200000
            prompt_overhead = 2000
            
            safe_batch_size = max(1, (max_safe_chars - prompt_overhead) // max_chunk_size)
            optimized_batch_size = min(original_batch_size, safe_batch_size, 4)  # æœ€å¤š4
            
            logger.info(f"ğŸ”§ å‚æ•°ä¼˜åŒ–åˆ†æ (GEMINI):")
            logger.info(f"   ğŸ“Š å½“å‰é…ç½®: batch_size={original_batch_size}, max_chunk_size={max_chunk_size}")
            logger.info(f"   ğŸ“ ä¼°ç®—å­—ç¬¦é‡: {estimated_chars_per_batch:,} å­—ç¬¦/æ‰¹æ¬¡ (ä¸å«prompt)")
            logger.info(f"   âš ï¸  å®‰å…¨ä¸Šé™: {max_safe_chars:,} å­—ç¬¦/æ‰¹æ¬¡ (å«{prompt_overhead:,}å­—ç¬¦promptå¼€é”€)")
            logger.info(f"   ğŸ¯ ä¼˜åŒ–ç»“æœ: batch_size {original_batch_size} â†’ {optimized_batch_size}")
            
            if optimized_batch_size < original_batch_size:
                new_estimated_chars = optimized_batch_size * max_chunk_size + prompt_overhead
                logger.info(f"   âœ… æ–°é…ç½®å­—ç¬¦é‡: {new_estimated_chars:,} å­—ç¬¦/æ‰¹æ¬¡ (å®‰å…¨èŒƒå›´å†…)")
                logger.info(f"   ğŸ’¡ å¤‡é€‰æ–¹æ¡ˆ: å¯è€ƒè™‘å‡å°‘ max_chunk_size è‡³ {max_chunk_size//2} ä»¥å¢åŠ batch_size")
            else:
                logger.info(f"   âœ… å½“å‰é…ç½®å·²å®‰å…¨: {estimated_chars_per_batch + prompt_overhead:,} å­—ç¬¦/æ‰¹æ¬¡")
            
            reason = f"Geminiä½¿ç”¨å®Œæ•´ç‰ˆpromptï¼Œé€‚åº¦å‡å°‘batch_sizeä¿è¯ç¨³å®šæ€§"
            
        else:
            # æœ¬åœ°æ¨¡å‹ä½¿ç”¨ç®€åŒ–ç‰ˆpromptï¼Œå¯ä»¥ä¿æŒè¾ƒå¤§batch_size
            # æœ¬åœ°æ¨¡å‹é€šå¸¸æœ‰æ›´å¤§çš„context window
            max_safe_chars = 100000  # æœ¬åœ°æ¨¡å‹é€šå¸¸æ”¯æŒæ›´å¤§çš„ä¸Šä¸‹æ–‡
            prompt_overhead = 500     # ç®€åŒ–ç‰ˆpromptå¼€é”€è¾ƒå°
            
            safe_batch_size = max(1, (max_safe_chars - prompt_overhead) // max_chunk_size)
            optimized_batch_size = min(original_batch_size, safe_batch_size)
            
            logger.info(f"ğŸ”§ å‚æ•°ä¼˜åŒ–åˆ†æ ({provider.upper()}):")
            logger.info(f"   ğŸ“Š å½“å‰é…ç½®: batch_size={original_batch_size}, max_chunk_size={max_chunk_size}")
            logger.info(f"   ğŸ“ ä¼°ç®—å­—ç¬¦é‡: {estimated_chars_per_batch:,} å­—ç¬¦/æ‰¹æ¬¡ (ä¸å«prompt)")
            logger.info(f"   âš ï¸  å®‰å…¨ä¸Šé™: {max_safe_chars:,} å­—ç¬¦/æ‰¹æ¬¡ (å«{prompt_overhead:,}å­—ç¬¦promptå¼€é”€)")
            
            if optimized_batch_size < original_batch_size:
                logger.info(f"   ğŸ¯ ä¼˜åŒ–ç»“æœ: batch_size {original_batch_size} â†’ {optimized_batch_size}")
                new_estimated_chars = optimized_batch_size * max_chunk_size + prompt_overhead
                logger.info(f"   âœ… æ–°é…ç½®å­—ç¬¦é‡: {new_estimated_chars:,} å­—ç¬¦/æ‰¹æ¬¡ (å®‰å…¨èŒƒå›´å†…)")
                logger.info(f"   ğŸ’¡ å¤‡é€‰æ–¹æ¡ˆ: å¯è€ƒè™‘å‡å°‘ max_chunk_size è‡³ {max_chunk_size//2} ä»¥å¢åŠ batch_size")
            else:
                logger.info(f"   âœ… å‚æ•°ä¿æŒ: batch_size = {original_batch_size} (å½“å‰é…ç½®å·²å®‰å…¨)")
                logger.info(f"   ğŸ“ å½“å‰å­—ç¬¦é‡: {estimated_chars_per_batch + prompt_overhead:,} å­—ç¬¦/æ‰¹æ¬¡")
            
            reason = f"æœ¬åœ°æ¨¡å‹ä½¿ç”¨ç®€åŒ–ç‰ˆpromptï¼Œä¿æŒåŸæœ‰batch_size"
        
        # åº”ç”¨ä¼˜åŒ–åçš„batch_size
        optimized = optimized_batch_size != original_batch_size
        if optimized:
            self.settings.processing.batch_size = optimized_batch_size
        
        return optimized, original_batch_size, optimized_batch_size, reason

    def _build_translation_mode_config(self) -> Dict[str, str]:
        """æ„å»ºç”¨äºè°ƒç”¨ç¿»è¯‘å™¨çš„ translation_mode_config å­—å…¸"""
        mode_entity = getattr(self.settings.processing, 'translation_mode_entity', None)
        if mode_entity:
            return {
                'name': getattr(mode_entity, 'name', 'Auto'),
                'style': getattr(mode_entity, 'style', 'Fluent and precise'),
                'role_desc': getattr(mode_entity, 'role_desc', 'Expert translator')
            }
        return {
            'name': str(getattr(self.settings.processing, 'translation_mode', 'Default')),
            'style': 'Fluent and precise',
            'role_desc': 'Expert translator'
        }
        
    def execute(self) -> None:
        """æ‰§è¡Œå®Œæ•´çš„ç¿»è¯‘å·¥ä½œæµ"""
        logger.info(f"ğŸš€ å¼€å§‹å¤„ç†æ–‡æ¡£: {self.file_path.name}")
        mode_name = getattr(self.settings.processing.translation_mode_entity, 'name', 'Default') if self.settings.processing.translation_mode_entity else 'Default'
        logger.info(f"   - ç¿»è¯‘æ¨¡å¼: {mode_name}")
        logger.info(f"   - é¡¹ç›®æ ‡è¯† (Hash): {self.project_name}")
        
        try:
            # 0. å‚æ•°ä¼˜åŒ–ï¼šæ ¹æ®translator providerè°ƒæ•´batch_size
            self._optimize_batch_size_for_provider()

            # 1. åŠ è½½æ–‡æ¡£ç»“æ„
            self._load_document()
            segment_count = len(self.all_segments) if self.all_segments else 0

            # 2. åˆå§‹åŒ–ç¿»è¯‘å™¨ï¼ˆæ­¤æ—¶ä¸åˆ›å»ºç¼“å­˜ï¼Œç­‰æœ¯è¯­è¡¨ç”Ÿæˆåå†åˆ›å»ºï¼‰
            self._initialize_translator()

            # 3. ç”Ÿæˆæœ¯è¯­è¡¨ï¼ˆé€šè¿‡é¢„ç¿»è¯‘ï¼Œå¼ºåˆ¶åŒæ­¥æ¨¡å¼ï¼‰
            self._generate_glossary()
            glossary_size = len(self.glossary) if self.glossary else 0
            
            # 4. æœ¯è¯­è¡¨ç¼–è¾‘äº¤äº’ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.settings.processing.enable_glossary_edit and self.glossary:
                self._prompt_glossary_edit()

            # 5. åˆå§‹åŒ–æ–­ç‚¹ç»­ä¼ 
            self._initialize_checkpoint()

            # 6. æ‰§è¡Œç¿»è¯‘å¾ªç¯
            self._run_translation_loop()

            # 7. ç¿»è¯‘å®Œæˆåå¤„ç†æ ‡é¢˜ï¼ˆåˆ©ç”¨æœ¯è¯­è¡¨ä¿æŒä¸€è‡´æ€§ï¼‰
            self._post_translate_titles()

            # 8. æ¸…ç†èµ„æº
            self._cleanup_resources()

            # 9. æ¸²æŸ“æœ€ç»ˆæ–‡æ¡£
            self._render_output()

        except Exception as e:
            logger.error(f"âŒ ç¿»è¯‘å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
            raise
    
    def _load_document(self) -> None:
        """
        Load é˜¶æ®µï¼šåŠ è½½æ–‡æ¡£ç»“æ„åˆ°å†…å­˜

        ä¼˜å…ˆçº§ï¼š
        1. ä» structure_map.json åŠ è½½ï¼ˆåŒ…å«ç¿»è¯‘çŠ¶æ€ï¼‰
        2. è§£æåŸå§‹æ–‡æ¡£ç”Ÿæˆæ–°ç»“æ„
        """
        logger.info("ğŸ“– åŠ è½½æ–‡æ¡£ç»“æ„...")
        
        # 1. å°è¯•ä» structure_map.json åŠ è½½
        if self.structure_path.exists() and self.settings.processing.enable_cache:
            try:
                with open(self.structure_path, 'r', encoding='utf-8') as f:
                    raw_data = json.load(f)
                    segments = [ContentSegment(**item) for item in raw_data]
                    logger.info(f"ğŸ“¦ ä»ç»“æ„æ–‡ä»¶åŠ è½½ {len(segments)} ä¸ªç‰‡æ®µ")
                    self.all_segments = segments
                    self._build_segment_index()  # æ„å»ºå¿«é€Ÿç´¢å¼•
                    logger.info(f"âœ… å·²åŠ è½½ {len(self.all_segments)} ä¸ªå†…å®¹ç‰‡æ®µ")
                    return
            except Exception as e:
                logger.warning(f"âš ï¸ structure_map.json æŸåï¼Œå°†é‡æ–°è§£æ: {e}")
        
        # 2. é‡æ–°è§£ææ–‡æ¡£
        logger.info("âš™ï¸ è§£ææ–‡æ¡£ç»“æ„...")
        segments = parse_document(self.file_path, self.structure_path, self.settings)
        
        if segments:
            logger.info(f"âœ… è§£æå®Œæˆï¼Œç”Ÿæˆ {len(segments)} ä¸ªç‰‡æ®µ")
            self._save_structure_map(segments)
            self.all_segments = segments
            self._build_segment_index()  # æ„å»ºå¿«é€Ÿç´¢å¼•
        else:
            logger.error("âŒ æ–‡æ¡£è§£æå¤±è´¥")
            raise TranslationError("æ–‡æ¡£è§£æå¤±è´¥ï¼Œæœªç”Ÿæˆä»»ä½•å†…å®¹ç‰‡æ®µ")
        
        logger.info(f"âœ… å·²åŠ è½½ {len(self.all_segments)} ä¸ªå†…å®¹ç‰‡æ®µ")
    
    def _initialize_translator(self) -> None:
        """åˆå§‹åŒ–ç¿»è¯‘å™¨å’Œç¼“å­˜ç®¡ç†å™¨"""
        provider = (getattr(self.settings.api, 'translator_provider', 'gemini') or 'gemini').lower()

        if provider == 'gemini':
            # åˆ›å»ºç¼“å­˜ç®¡ç†å™¨ï¼ˆå¦‚æœå¯ç”¨ Gemini Context Cachingï¼‰
            if self.settings.processing.enable_gemini_caching:
                from ..translator.support import CachePersistenceManager
                self.cache_manager = CachePersistenceManager(self.settings)
                logger.info("âœ… Gemini ç¼“å­˜ç®¡ç†å™¨å·²åˆå§‹åŒ–")

            # GeminiTranslator currently does not accept cache_manager in constructor;
            # keep cache_manager on the workflow and pass where needed inside translator.
            self.translator = GeminiTranslator(self.settings)
            logger.info("âœ… Gemini ç¿»è¯‘å™¨å·²åˆå§‹åŒ–")
            return

        if provider in {'deepseek', 'openai', 'openai-compatible', 'openai_compatible'}:
            # OpenAI-compatible provider (DeepSeek)
            self.cache_manager = None
            self.translator = OpenAICompatibleTranslator(self.settings)
            logger.info(f"âœ… OpenAI-compatible ç¿»è¯‘å™¨å·²åˆå§‹åŒ– (provider={provider})")
            return
        
        # Ollamaå·²é›†æˆåˆ°OpenAI-compatible providerä¸­
        # é…ç½®ç¤ºä¾‹ï¼šTRANSLATOR_PROVIDER=openai-compatible, OPENAI_BASE_URL=http://localhost:11434
        
        raise TranslationError(
            f"æœªçŸ¥ translator_provider: {provider}ã€‚"
            f"æ”¯æŒçš„provider: gemini, deepseek, openai, openai-compatibleã€‚"
            f"æ³¨æ„ï¼šOllamaç°å·²é›†æˆåˆ°openai-compatibleä¸­ï¼Œè¯·ä½¿ç”¨OPENAI_BASE_URL=http://localhost:11434"
        )
    
    def _post_translate_titles(self) -> None:
        """
        ç¿»è¯‘å®Œæˆåå¤„ç†ç« èŠ‚æ ‡é¢˜
        
        æ”¾åœ¨æœ€åæ‰§è¡Œçš„ä¼˜åŠ¿ï¼š
        1. å¯ä»¥åˆ©ç”¨å·²ç”Ÿæˆçš„æœ¯è¯­è¡¨ä¿æŒä¸€è‡´æ€§
        2. ä¸éœ€è¦å¤æ‚çš„ mode é…ç½®ï¼ˆæ ‡é¢˜ç¿»è¯‘æœ¬èº«æ˜¯ç®€å•ä»»åŠ¡ï¼‰
        3. ä¸å½±å“ä¸»ç¿»è¯‘æµç¨‹
        """
        logger.info("ğŸ“ å¼€å§‹ç¿»è¯‘ç« èŠ‚æ ‡é¢˜...")
        
        # æå–å¾…ç¿»è¯‘æ ‡é¢˜
        raw_titles = []
        for seg in self.all_segments:
            if (seg.is_new_chapter and seg.chapter_title and
                seg.chapter_title.strip() and not is_likely_chinese(seg.chapter_title)):
                raw_titles.append(seg.chapter_title)
        
        if not raw_titles:
            logger.info("   - æ— éœ€ç¿»è¯‘çš„æ ‡é¢˜")
            return
        
        # å»é‡
        unique_titles = list(dict.fromkeys(raw_titles))
        logger.info(f"   - å‘ç° {len(unique_titles)} ä¸ªå”¯ä¸€æ ‡é¢˜")

        # æ‰¹é‡ç¿»è¯‘ï¼ˆä¸éœ€è¦ mode é…ç½®ï¼Œtranslate_titles æ–¹æ³•æœ¬èº«å·²è¶³å¤Ÿç®€å•ï¼‰
        translation_map = self.translator.translate_titles(unique_titles)
        
        # å›å¡«ç»“æœ
        update_count = 0
        for seg in self.all_segments:
            if seg.is_new_chapter and seg.chapter_title in translation_map:
                translated = translation_map[seg.chapter_title]
                if translated:
                    seg.chapter_title = translated
                    update_count += 1
        
        logger.info(f"   - æ›´æ–°äº† {update_count} ä¸ªæ ‡é¢˜")
        
        # ä¿å­˜æ›´æ–°åçš„ç»“æ„
        self._save_structure_map(self.all_segments)
        logger.info("âœ… æ ‡é¢˜ç¿»è¯‘å®Œæˆ")
    
    def _prompt_glossary_edit(self) -> None:
        """
        æœ¯è¯­è¡¨ç¼–è¾‘äº¤äº’
        
        åœ¨æœ¯è¯­è¡¨ç”Ÿæˆåæš‚åœï¼Œå…è®¸ç”¨æˆ·æŸ¥çœ‹å’Œç¼–è¾‘æœ¯è¯­è¡¨
        """
        glossary_path = self.project_dir / "glossary.json"
        
        logger.info("=" * 60)
        logger.info("ğŸ“š æœ¯è¯­è¡¨å·²ç”Ÿæˆï¼Œå½“å‰åŒ…å« {} æ¡æœ¯è¯­".format(len(self.glossary)))
        logger.info(f"   æ–‡ä»¶ä½ç½®: {glossary_path}")
        logger.info("")
        logger.info("ğŸ’¡ æ‚¨å¯ä»¥:")
        logger.info("   1. æŸ¥çœ‹å¹¶ç¼–è¾‘ä¸Šè¿°æ–‡ä»¶ä¸­çš„æœ¯è¯­ç¿»è¯‘")
        logger.info("   2. æ·»åŠ é—æ¼çš„ä¸“ä¸šæœ¯è¯­")
        logger.info("   3. ä¿®æ­£ä¸å‡†ç¡®çš„ç¿»è¯‘")
        logger.info("")
        
        # æ˜¾ç¤ºéƒ¨åˆ†æœ¯è¯­é¢„è§ˆ
        preview_count = min(10, len(self.glossary))
        if preview_count > 0:
            logger.info(f"ğŸ“‹ æœ¯è¯­é¢„è§ˆ (å‰ {preview_count} æ¡):")
            for i, (term, translation) in enumerate(list(self.glossary.items())[:preview_count]):
                logger.info(f"   {i+1}. {term} â†’ {translation}")
            if len(self.glossary) > preview_count:
                logger.info(f"   ... è¿˜æœ‰ {len(self.glossary) - preview_count} æ¡")
        
        logger.info("")
        logger.info("=" * 60)
        
        try:
            user_input = input("ğŸ“ ç¼–è¾‘å®ŒæˆåæŒ‰ Enter ç»§ç»­ç¿»è¯‘ï¼ˆè¾“å…¥ 'q' å–æ¶ˆï¼‰: ").strip().lower()
            
            if user_input == 'q':
                logger.info("âš ï¸  ç”¨æˆ·å–æ¶ˆï¼Œé€€å‡ºç¿»è¯‘æµç¨‹")
                raise KeyboardInterrupt("ç”¨æˆ·å–æ¶ˆç¿»è¯‘")
            
            # é‡æ–°åŠ è½½å¯èƒ½è¢«ç¼–è¾‘çš„æœ¯è¯­è¡¨
            if glossary_path.exists():
                try:
                    with open(glossary_path, 'r', encoding='utf-8') as f:
                        self.glossary = json.load(f)
                    logger.info(f"âœ… å·²é‡æ–°åŠ è½½æœ¯è¯­è¡¨ ({len(self.glossary)} æ¡)")
                except Exception as e:
                    logger.warning(f"âš ï¸  é‡æ–°åŠ è½½æœ¯è¯­è¡¨å¤±è´¥: {e}")
        
        except EOFError:
            # éäº¤äº’æ¨¡å¼ï¼ˆå¦‚åœ¨ CI/CD ä¸­è¿è¡Œï¼‰ï¼Œç›´æ¥ç»§ç»­
            logger.info("â„¹ï¸  éäº¤äº’æ¨¡å¼ï¼Œè·³è¿‡æœ¯è¯­è¡¨ç¼–è¾‘")
    
    def _generate_glossary(self) -> None:
        """ç”Ÿæˆæˆ–åŠ è½½æœ¯è¯­è¡¨ï¼ˆæ”¯æŒæ¸è¿›å¼æå–ï¼‰"""
        # å‡†å¤‡æœ¯è¯­è¡¨æ–‡ä»¶è·¯å¾„
        glossary_merged_path = self.project_dir / "glossary_merged.json"
        glossary_path = self.project_dir / "glossary.json"
        
        # ä¼˜å…ˆä½¿ç”¨åˆå¹¶çš„æœ¯è¯­è¡¨
        if glossary_merged_path.exists():
            glossary_path = glossary_merged_path
        
        # å°è¯•åŠ è½½å·²æœ‰æœ¯è¯­è¡¨
        glossary_loaded = False
        if glossary_path.exists():
            try:
                with open(glossary_path, 'r', encoding='utf-8') as gf:
                    self.glossary = json.load(gf)
                logger.info(f"ğŸ“š ä»ç¼“å­˜åŠ è½½å·²æœ‰æœ¯è¯­è¡¨ ({len(self.glossary)} æ¡) -> {glossary_path}")
                glossary_loaded = True
                
                # å¦‚æœé…ç½®ä¸ºè·³è¿‡é¢„ç¿»è¯‘ï¼Œç›´æ¥è¿”å›
                if self.settings.processing.skip_pretranslate_if_glossary_exists:
                    logger.info("âœ… å·²æœ‰æœ¯è¯­è¡¨ï¼Œè·³è¿‡é¢„ç¿»è¯‘é˜¶æ®µï¼Œç›´æ¥è¿›å…¥æ­£å¼ç¿»è¯‘")
                    return
                else:
                    logger.warning("âš ï¸ å‘ç°å·²æœ‰æœ¯è¯­è¡¨ï¼Œä½†é…ç½®ä¸ºé‡æ–°ç”Ÿæˆï¼Œå°†è¦†ç›–åŸæœ‰æœ¯è¯­è¡¨")
            except Exception as e:
                logger.warning(f"âš ï¸ åŠ è½½å·²ä¿å­˜çš„æœ¯è¯­è¡¨å¤±è´¥ï¼Œå°†é‡æ–°ç”Ÿæˆ: {e}")
        
        # ========== ç¬¬ä¸€é˜¶æ®µï¼šåˆ›å»ºåŸºç¡€ç¼“å­˜ï¼ˆé¢„ç¿»è¯‘ç”¨ï¼‰ ==========
        if hasattr(self.translator, 'create_base_cache'):
            self.translator.create_base_cache()
        
        # è®¡ç®—é¢„ç¿»è¯‘èŒƒå›´
        try:
            ratio = getattr(self.settings.processing, 'glossary_preamble_ratio', 0.1)
            pre_count = max(1, int(len(self.all_segments) * float(ratio)))
        except Exception:
            pre_count = max(1, int(len(self.all_segments) * 0.1))
        
        if pre_count <= 0:
            logger.info("âšª é¢„ç¿»è¯‘æ•°é‡ä¸º 0ï¼Œè·³è¿‡æœ¯è¯­è¡¨ç”Ÿæˆ")
            self.glossary = {}
            return
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨æ¸è¿›å¼æœ¯è¯­è¡¨æå–
        enable_progressive = getattr(self.settings.processing, 'enable_progressive_glossary', True)
        
        if enable_progressive:
            logger.info("ğŸ”„ å¯ç”¨æ¸è¿›å¼æœ¯è¯­è¡¨æå–æ¨¡å¼")
            self._generate_glossary_progressive(pre_count, glossary_path)
        else:
            logger.info("ğŸ“¦ ä½¿ç”¨ä¼ ç»Ÿæœ¯è¯­è¡¨æå–æ¨¡å¼ï¼ˆé¢„ç¿»è¯‘å®Œæˆåä¸€æ¬¡æ€§æå–ï¼‰")
            self._generate_glossary_traditional(pre_count, glossary_path)
    
    def _generate_glossary_progressive(self, pre_count: int, glossary_path) -> None:
        """æ¸è¿›å¼æœ¯è¯­è¡¨ç”Ÿæˆï¼šæ¯ä¸ª batch ç¿»è¯‘åç«‹å³æå–å¹¶åˆå¹¶"""
        pre_segments = self.all_segments[:pre_count]
        pending_pre = [seg for seg in pre_segments if not seg.is_translated]
        
        if not pending_pre:
            logger.info("âšª æ‰€æœ‰é¢„ç¿»è¯‘ç‰‡æ®µå·²å®Œæˆï¼Œè·³è¿‡")
            return
        
        logger.info(f"ğŸ§ª æ¸è¿›å¼é¢„ç¿»è¯‘ä¸æœ¯è¯­è¡¨æå–")
        logger.info(f"   - è®¡åˆ’é¢„ç¿»è¯‘: {pre_count} ä¸ªç‰‡æ®µ")
        logger.info(f"   - å¾…å¤„ç†: {len(pending_pre)} ä¸ªç‰‡æ®µ")
        logger.info(f"   - æœ€å°‘æœ¯è¯­æ•°: {self.settings.processing.glossary_min_terms}")
        logger.info(f"   - æœ€å¤§æœ¯è¯­æ•°: {self.settings.processing.glossary_max_terms}")
        logger.info(f"   - é¥±å’Œåº¦é˜ˆå€¼: {self.settings.processing.glossary_stop_threshold}")
        
        # åˆå§‹åŒ–æœ¯è¯­è¡¨å’Œç»Ÿè®¡
        self.glossary = {}
        batch_size = self.settings.processing.batch_size
        new_terms_history = []  # è®°å½•æ¯ä¸ª batch æ–°å¢çš„æœ¯è¯­æ•°é‡
        consecutive_low_batches = 0  # è¿ç»­ä½å¢é•¿ batch è®¡æ•°
        min_terms = self.settings.processing.glossary_min_terms
        max_terms = self.settings.processing.glossary_max_terms
        stop_threshold = self.settings.processing.glossary_stop_threshold
        
        for i in range(0, len(pending_pre), batch_size):
            batch = pending_pre[i:i+batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(pending_pre) + batch_size - 1) // batch_size
            
            logger.info(f"ğŸ”„ å¤„ç† Batch {batch_num}/{total_batches} ({len(batch)} ä¸ªç‰‡æ®µ)...")
            
            # 1. ç¿»è¯‘å½“å‰ batch
            context = ""
            if batch:
                context = self._get_context_from_memory(
                    batch[0],
                    self.settings.processing.max_context_length
                )
            
            batch_results = self.translator.translate_batch(batch, context=context)
            for seg, t in zip(batch, batch_results):
                seg.translated_text = t
                # é¢„ç¿»è¯‘é˜¶æ®µä¹Ÿæ ‡è®°å®ŒæˆçŠ¶æ€ï¼ˆå¦‚æœå¯ç”¨äº† checkpointï¼‰
                if self.checkpoint and t and not t.startswith("[Failed") and not t.endswith("Failed]"):
                    self.checkpoint.mark_segment_completed(seg.segment_id)
            
            # 2. ä»å½“å‰ batch æå–æœ¯è¯­è¡¨
            if hasattr(self.translator, 'extract_glossary'):
                try:
                    batch_glossary = self.translator.extract_glossary(batch)
                    
                    # åˆå¹¶åˆ°æ€»æœ¯è¯­è¡¨ï¼Œç»Ÿè®¡æ–°å¢æ•°é‡
                    before_count = len(self.glossary)
                    self.glossary.update(batch_glossary)
                    after_count = len(self.glossary)
                    new_terms = after_count - before_count
                    new_terms_history.append(new_terms)
                    
                    logger.info(f"   âœ… Batch {batch_num} æå–æœ¯è¯­: {len(batch_glossary)} æ¡ï¼Œæ–°å¢: {new_terms} æ¡ï¼Œç´¯è®¡: {after_count} æ¡")
                    
                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§æœ¯è¯­æ•°
                    if after_count >= max_terms:
                        logger.info(f"âœ… æœ¯è¯­è¡¨å·²è¾¾åˆ°æœ€å¤§æ¡ç›®æ•° ({max_terms} æ¡)ï¼Œæå‰ç»“æŸé¢„ç¿»è¯‘")
                        logger.info(f"   - å·²å¤„ç†: {batch_num}/{total_batches} batches ({(batch_num/total_batches)*100:.1f}%)")
                        break
                    
                    # 3. æ£€æŸ¥æ˜¯å¦è¾¾åˆ°åœæ­¢æ¡ä»¶
                    # æ¡ä»¶1: è¾¾åˆ°æœ€å°‘æœ¯è¯­æ•°
                    if after_count >= min_terms:
                        # æ¡ä»¶2: æœ¯è¯­è¡¨é¥±å’Œï¼ˆè¿ç»­ 3 ä¸ª batch æ–°å¢æœ¯è¯­æ•°ä½äºé˜ˆå€¼ï¼‰
                        if len(new_terms_history) >= 3:
                            avg_new_terms = sum(new_terms_history) / len(new_terms_history)
                            recent_avg = sum(new_terms_history[-3:]) / 3
                            
                            if recent_avg < avg_new_terms * stop_threshold:
                                consecutive_low_batches += 1
                                logger.info(f"   ğŸ“Š æœ¯è¯­å¢é•¿æ”¾ç¼“: è¿‘3æ‰¹å¹³å‡ {recent_avg:.1f} < å†å²å¹³å‡ {avg_new_terms:.1f} Ã— {stop_threshold}")
                                
                                if consecutive_low_batches >= 2:  # è¿ç»­ 2 æ¬¡ä½å¢é•¿
                                    logger.info(f"âœ… æœ¯è¯­è¡¨å·²é¥±å’Œï¼ˆ{after_count} æ¡ï¼‰ï¼Œæå‰ç»“æŸé¢„ç¿»è¯‘")
                                    logger.info(f"   - å·²å¤„ç†: {batch_num}/{total_batches} batches ({(batch_num/total_batches)*100:.1f}%)")
                                    logger.info(f"   - èŠ‚çœ: {total_batches - batch_num} batches")
                                    break
                            else:
                                consecutive_low_batches = 0  # é‡ç½®è®¡æ•°
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ Batch {batch_num} æœ¯è¯­æå–å¤±è´¥: {e}")
            
            # 4. ä¿å­˜é˜¶æ®µæ€§ç»“æœ
            self._save_structure_map(self.all_segments)
            if self.checkpoint:
                self.checkpoint.save_checkpoint()
            if self.glossary:
                try:
                    with open(glossary_path, 'w', encoding='utf-8') as gf:
                        json.dump(self.glossary, gf, ensure_ascii=False, indent=2)
                except Exception as e:
                    logger.warning(f"âš ï¸ ä¿å­˜é˜¶æ®µæ€§æœ¯è¯­è¡¨å¤±è´¥: {e}")
        
        # æœ€ç»ˆç»Ÿè®¡
        if self.glossary:
            logger.info(f"ğŸ’¾ æœ¯è¯­è¡¨å·²ä¿å­˜åˆ°: {glossary_path}")
            logger.info(f"ğŸ”¥ æ¸è¿›å¼æå–å®Œæˆï¼Œå…± {len(self.glossary)} æ¡æœ¯è¯­")
            # æ˜¾ç¤ºå‰ 5 æ¡æœ¯è¯­ä½œä¸ºç¤ºä¾‹
            for idx, (k, v) in enumerate(list(self.glossary.items())[:5], 1):
                logger.info(f"   {idx}. '{k}' â†’ '{v}'")
            if len(self.glossary) > 5:
                logger.info("   ... (æ›´å¤šæœ¯è¯­)")
        else:
            logger.info("âšª æœªç”Ÿæˆæœ‰æ•ˆæœ¯è¯­è¡¨")
    
    def _generate_glossary_traditional(self, pre_count: int, glossary_path) -> None:
        """ä¼ ç»Ÿæœ¯è¯­è¡¨ç”Ÿæˆï¼šé¢„ç¿»è¯‘å®Œæˆåä¸€æ¬¡æ€§æå–"""
        pre_segments = self.all_segments[:pre_count]
        pending_pre = [seg for seg in pre_segments if not seg.is_translated]
        
        if pending_pre:
            logger.info(f"ğŸ§ª é¢„ç¿»è¯‘å‰ {pre_count} ä¸ªç‰‡æ®µä»¥æ„å»ºæœ¯è¯­è¡¨...")
            logger.info(f"   - ä½¿ç”¨åŸºç¡€ç¼“å­˜ï¼ˆæ—  modeã€æ—  glossaryï¼‰")
            
            translation_mode_config = self._build_translation_mode_config()
            # ä¸ºé¢„ç¿»è¯‘ç‰‡æ®µæä¾›ä¸Šä¸‹æ–‡ï¼ˆåŒæ­¥æ–¹å¼ï¼Œä½¿ç”¨åŸæ–‡ä½œä¸ºä¸Šä¸‹æ–‡ï¼‰
            translations = []
            batch_size = self.settings.processing.batch_size
            for i in range(0, len(pending_pre), batch_size):
                batch = pending_pre[i:i+batch_size]
                context = ""
                if batch:
                    context = self._get_context_from_memory(
                        batch[0],
                        self.settings.processing.max_context_length
                    )
                batch_results = self.translator.translate_batch(batch, context=context)
                translations.extend(batch_results)
            
            for seg, t in zip(pending_pre, translations):
                seg.translated_text = t
                # ä¼ ç»Ÿé¢„ç¿»è¯‘é˜¶æ®µä¹Ÿæ ‡è®°å®ŒæˆçŠ¶æ€ï¼ˆå¦‚æœå¯ç”¨äº† checkpointï¼‰
                if self.checkpoint and t and not t.startswith("[Failed") and not t.endswith("Failed]"):
                    self.checkpoint.mark_segment_completed(seg.segment_id)
            self._save_structure_map(self.all_segments)
            if self.checkpoint:
                self.checkpoint.save_checkpoint()
        
        # æå–æœ¯è¯­è¡¨ï¼ˆè‹¥ç¿»è¯‘å™¨å®ç°äº†è¯¥æ–¹æ³•ï¼‰
        if hasattr(self.translator, 'extract_glossary'):
            try:
                self.glossary = self.translator.extract_glossary(pre_segments)
            except Exception as e:
                logger.warning(f"âš ï¸ æå–æœ¯è¯­è¡¨å¤±è´¥: {e}")
                self.glossary = {}
        else:
            logger.info("â„¹ï¸ ç¿»è¯‘å™¨ä¸æ”¯æŒæœ¯è¯­è¡¨æå–ï¼Œè·³è¿‡æ­¤æ­¥éª¤")
            self.glossary = {}
        
        # æŒä¹…åŒ–æœ¯è¯­è¡¨
        if self.glossary:
            try:
                with open(glossary_path, 'w', encoding='utf-8') as gf:
                    json.dump(self.glossary, gf, ensure_ascii=False, indent=2)
                logger.info(f"ğŸ’¾ æœ¯è¯­è¡¨å·²ä¿å­˜åˆ°: {glossary_path}")
                logger.info(f"ğŸ”¥ å·²ç”Ÿæˆæœ¯è¯­è¡¨ï¼ŒåŒ…å« {len(self.glossary)} æ¡æœ¯è¯­")
            except Exception as e:
                logger.warning(f"âš ï¸ ä¿å­˜æœ¯è¯­è¡¨å¤±è´¥: {e}")
        else:
            logger.info("âšª æœªç”Ÿæˆæœ‰æ•ˆæœ¯è¯­è¡¨")
    
    def _initialize_checkpoint(self) -> None:
        """åˆå§‹åŒ–æ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨"""
        self.checkpoint = CheckpointManager(self.settings)
        self.checkpoint.update_total_segments(len(self.all_segments))
        logger.info("âœ… æ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨å·²åˆå§‹åŒ–")
        logger.info(f"   ğŸ“‚ æ£€æŸ¥ç‚¹æ–‡ä»¶: {self.checkpoint.checkpoint_file}")
    
    def _run_translation_loop(self) -> None:
        """æ‰§è¡Œç¿»è¯‘å¾ªç¯ï¼ˆæ”¯æŒåŒæ­¥/å¼‚æ­¥æ¨¡å¼ï¼‰"""
        # ========== ç¬¬äºŒé˜¶æ®µï¼šåˆ›å»ºå®Œæ•´ç¼“å­˜ï¼ˆæ­£å¼ç¿»è¯‘ç”¨ï¼‰ ==========
        # åŒ…å« glossary å’Œ mode
        if hasattr(self.translator, 'create_full_cache'):
            self.translator.create_full_cache(glossary=self.glossary)
            logger.info("ğŸ“¦ æ­£å¼ç¿»è¯‘é˜¶æ®µï¼šä½¿ç”¨å®Œæ•´ç¼“å­˜ï¼ˆå« mode å’Œ glossaryï¼‰")
        
        # è·å–å¾…ç¿»è¯‘ç‰‡æ®µï¼ˆæ‰€æœ‰ç‰‡æ®µï¼Œå› ä¸ºæ­£å¼ç¿»è¯‘éœ€è¦é‡æ–°ç¿»è¯‘é¢„ç¿»è¯‘éƒ¨åˆ†ä»¥ä¿è¯ä¸€è‡´æ€§ï¼‰
        # æ³¨æ„ï¼šé¢„ç¿»è¯‘çš„ç‰‡æ®µä¹Ÿéœ€è¦é‡æ–°ç¿»è¯‘ï¼Œå› ä¸ºï¼š
        # 1. é¢„ç¿»è¯‘ä½¿ç”¨çš„æ˜¯æ—  glossary çš„ç¼“å­˜
        # 2. æ­£å¼ç¿»è¯‘æœ‰äº†å®Œæ•´æœ¯è¯­è¡¨ï¼Œå¯ä»¥è·å¾—æ›´ä¸€è‡´çš„ç¿»è¯‘è´¨é‡
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦é‡æ–°ç¿»è¯‘é¢„ç¿»è¯‘éƒ¨åˆ†
        reprocess_pretranslated = getattr(self.settings.processing, 'reprocess_pretranslated', True)
        
        if reprocess_pretranslated:
            # æ ‡è®°é¢„ç¿»è¯‘éƒ¨åˆ†ä¸ºæœªç¿»è¯‘ï¼Œä»¥ä¾¿é‡æ–°å¤„ç†
            try:
                ratio = getattr(self.settings.processing, 'glossary_preamble_ratio', 0.1)
                pre_count = max(1, int(len(self.all_segments) * float(ratio)))
            except Exception:
                pre_count = max(1, int(len(self.all_segments) * 0.1))
            
            for seg in self.all_segments[:pre_count]:
                if seg.is_translated:
                    seg.translated_text = ""  # æ¸…é™¤é¢„ç¿»è¯‘ç»“æœ
            
            logger.info(f"ğŸ”„ å°†é‡æ–°ç¿»è¯‘å‰ {pre_count} ä¸ªé¢„ç¿»è¯‘ç‰‡æ®µï¼ˆä½¿ç”¨å®Œæ•´ç¼“å­˜ï¼‰")
        
        pending_segments = self.checkpoint.get_pending_segments(self.all_segments)
        
        if not pending_segments:
            logger.info("ğŸ‰ æ‰€æœ‰ç‰‡æ®µå‡å·²ç¿»è¯‘å®Œæˆï¼")
            return
        
        logger.info(f"ğŸ”„ å‘ç° {len(pending_segments)} ä¸ªå¾…ç¿»è¯‘ç‰‡æ®µ")
        
        # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨å¼‚æ­¥æ¨¡å¼
        use_async = (
            self.settings.processing.enable_async and 
            len(pending_segments) >= self.settings.processing.async_threshold and
            hasattr(self.translator, 'async_translator')
        )
        
        if use_async:
            self._run_async_translation(pending_segments)
        else:
            self._run_sync_translation(pending_segments)
        
        # ç¿»è¯‘å¾ªç¯ç»“æŸåï¼Œå¼ºåˆ¶ä¿å­˜ä¸€æ¬¡
        logger.info("ğŸ”’ ç¿»è¯‘å¾ªç¯å®Œæˆï¼Œæ‰§è¡Œå¼ºåˆ¶ä¿å­˜...")
        self._save_structure_map(self.all_segments)
        self.checkpoint.save_checkpoint()
        logger.info("âœ… å¼ºåˆ¶ä¿å­˜å®Œæˆ")
    
    def _run_sync_translation(self, pending_segments: SegmentList) -> None:
        """åŒæ­¥ç¿»è¯‘æ¨¡å¼ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰"""
        logger.info("ğŸ”„ ä½¿ç”¨åŒæ­¥æ¨¡å¼ç¿»è¯‘")
        logger.info(f"ğŸ“ å¼€å§‹åŒæ­¥ç¿»è¯‘ {len(pending_segments)} ä¸ªç‰‡æ®µ...")
        
        try:
            # å°è¯•ä½¿ç”¨ rich è¿›åº¦æ¡ï¼Œå¦‚æœå¤±è´¥åˆ™å›é€€åˆ°æ— è¿›åº¦æ¡æ¨¡å¼
            use_rich = self.settings.processing.use_rich_progress
            
            if use_rich:
                try:
                    from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn
                    
                    with Progress(
                        SpinnerColumn(),
                        TextColumn("[bold blue]{task.description}"),
                        BarColumn(),
                        TaskProgressColumn(),
                        console=None,  # ä½¿ç”¨é»˜è®¤console
                    ) as progress:
                        task = progress.add_task("[cyan]åŒæ­¥ç¿»è¯‘ä¸­...", total=len(pending_segments))
                        
                        success_count = 0
                        batch_size = self.settings.processing.batch_size
                        
                        for i in range(0, len(pending_segments), batch_size):
                            batch = pending_segments[i:i+batch_size]
                            # ä¸ºå½“å‰batchçš„ç¬¬ä¸€ä¸ªsegmentè·å–ä¸Šä¸‹æ–‡
                            context = ""
                            if batch:
                                context = self._get_context_from_memory(
                                    batch[0],
                                    self.settings.processing.max_context_length
                                )
                            results = self.translator.translate_batch(batch, context=context)
                            
                            for seg, trans in zip(batch, results):
                                if trans and not trans.startswith("[Failed") and not trans.endswith("Failed]"):
                                    seg.translated_text = trans
                                    self.checkpoint.mark_segment_completed(seg.segment_id)
                                    success_count += 1
                                else:
                                    seg.translated_text = trans if trans else "[Failed: Empty response]"
                                    self.checkpoint.mark_segment_failed(seg.segment_id, trans or "Empty response")
                                
                                progress.update(task, advance=1)
                            
                            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
                            if (i // batch_size + 1) % self.settings.processing.checkpoint_interval == 0:
                                self._save_structure_map(self.all_segments)
                                self.checkpoint.save_checkpoint()
                        
                        logger.info(f"âœ… åŒæ­¥ç¿»è¯‘å®Œæˆ: {success_count}/{len(pending_segments)} æˆåŠŸ")
                        
                except ImportError:
                    logger.warning("âš ï¸ Rich åº“æœªå®‰è£…ï¼Œä½¿ç”¨ç®€å•æ¨¡å¼ï¼ˆæ— è¿›åº¦æ¡ï¼‰")
                    use_rich = False
            
            # å›é€€åˆ°ç®€å•æ¨¡å¼ï¼ˆæ— è¿›åº¦æ¡ï¼‰
            if not use_rich:
                translation_mode_config = self._build_translation_mode_config()
                # ä¸ºæ¯ä¸ªbatchåˆ†åˆ«å¤„ç†ä¸Šä¸‹æ–‡
                success_count = 0
                batch_size = self.settings.processing.batch_size

                for i in range(0, len(pending_segments), batch_size):
                    batch = pending_segments[i:i+batch_size]
                    # ä¸ºå½“å‰batchçš„ç¬¬ä¸€ä¸ªsegmentè·å–ä¸Šä¸‹æ–‡
                    context = ""
                    if batch:
                        context = self._get_context_from_memory(
                            batch[0],
                            self.settings.processing.max_context_length
                        )
                    results = self.translator.translate_batch(batch, context=context)

                    for seg, trans in zip(batch, results):
                        if trans and not trans.startswith("[Failed") and not trans.endswith("Failed]"):
                            seg.translated_text = trans
                            self.checkpoint.mark_segment_completed(seg.segment_id)
                            success_count += 1
                        else:
                            seg.translated_text = trans if trans else "[Failed: Empty response]"
                            self.checkpoint.mark_segment_failed(seg.segment_id, trans or "Empty response")

                    # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
                    if (i // batch_size + 1) % self.settings.processing.checkpoint_interval == 0:
                        self._save_structure_map(self.all_segments)
                        self.checkpoint.save_checkpoint()
                
                logger.info(f"âœ… åŒæ­¥ç¿»è¯‘å®Œæˆ: {success_count}/{len(pending_segments)} æˆåŠŸ")
            
            # æœ€ç»ˆä¿å­˜
            self._save_structure_map(self.all_segments)
            self.checkpoint.save_checkpoint()
            
        except Exception as e:
            logger.error(f"âŒ åŒæ­¥ç¿»è¯‘å¤±è´¥: {e}")
            for seg in pending_segments:
                seg.translated_text = f"[Failed: {str(e)}]"
                self.checkpoint.mark_segment_failed(seg.segment_id, str(e))
            self._save_structure_map(self.all_segments)
            self.checkpoint.save_checkpoint()
            raise
    
    def _run_async_translation(self, pending_segments: SegmentList) -> None:
        """å¼‚æ­¥ç¿»è¯‘æ¨¡å¼ï¼ˆå¤šæ‰¹æ¬¡å¹¶å‘æ‰§è¡Œï¼ŒçœŸæ­£çš„å¹¶è¡Œç¿»è¯‘ï¼‰
        
        æ¶æ„ä¼˜åŒ–ï¼š
        - ä¹‹å‰ï¼šbatch ä¸²è¡Œæ‰§è¡Œï¼Œå®é™…å¹¶å‘åº¦ = 1
        - ç°åœ¨ï¼šå¤šä¸ª batch å¹¶å‘æ‰§è¡Œï¼Œå®é™…å¹¶å‘åº¦ = max_concurrent_batches
        
        è°ƒç”¨é“¾ï¼š
        workflow._run_async_translation (åŒæ­¥å…¥å£)
          â””â”€â”€ asyncio.run(_run_concurrent_batches)  [å•æ¬¡è°ƒç”¨]
                â””â”€â”€ asyncio.gather(*batch_tasks)  [çœŸæ­£çš„å¹¶å‘]
                      â””â”€â”€ _process_single_batch (æ¯ä¸ª batch)
                            â””â”€â”€ async_t.translate_text_batch_async
        """
        logger.info("âš¡ ä½¿ç”¨å¼‚æ­¥æ¨¡å¼ç¿»è¯‘ï¼ˆå¤šæ‰¹æ¬¡å¹¶å‘+å³æ—¶ä¿å­˜ï¼‰")
        
        # æ£€æŸ¥translatoræ˜¯å¦æ”¯æŒå¼‚æ­¥
        if not hasattr(self.translator, 'async_translator') or self.translator.async_translator is None:
            logger.warning("âš ï¸ å½“å‰translatorä¸æ”¯æŒå¼‚æ­¥æ¨¡å¼ï¼Œé™çº§åˆ°åŒæ­¥æ¨¡å¼")
            self._run_sync_translation(pending_segments)
            return
        
        batch_size = self.settings.processing.batch_size
        batches = [
            pending_segments[i:i+batch_size] 
            for i in range(0, len(pending_segments), batch_size)
        ]
        total_batches = len(batches)
        total_segments = len(pending_segments)
        
        # å¹¶å‘åº¦æ§åˆ¶ï¼šåŒæ—¶æ‰§è¡Œå¤šå°‘ä¸ª batch
        max_concurrent = getattr(self.settings.processing, 'async_max_workers', 5)
        
        logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘ç¿»è¯‘ {total_segments} ä¸ªç‰‡æ®µ")
        logger.info(f"   ğŸ“Š å…± {total_batches} æ‰¹æ¬¡ï¼Œæ‰¹å¤§å° {batch_size}ï¼Œå¹¶å‘åº¦ {max_concurrent}")
        
        # ç”¨äºçº¿ç¨‹å®‰å…¨çš„è®¡æ•°å’Œä¿å­˜
        lock = threading.Lock()
        stats = {"success": 0, "processed": 0, "completed_batches": 0}
        
        async def _process_single_batch(batch_idx: int, batch: SegmentList, semaphore: asyncio.Semaphore):
            """å¤„ç†å•ä¸ª batchï¼ˆåœ¨ semaphore æ§åˆ¶ä¸‹ï¼‰"""
            async with semaphore:
                try:
                    # è·å–ä¸Šä¸‹æ–‡ï¼ˆè¯»å–å½“å‰å·²ç¿»è¯‘çš„å†…å®¹ï¼‰
                    context = ""
                    if batch:
                        context = self._get_context_from_memory(
                            batch[0],
                            self.settings.processing.max_context_length
                        )
                    
                    # æ‰§è¡Œç¿»è¯‘
                    async_t = self.translator.async_translator
                    batch_results = await async_t.translate_text_batch_async(
                        batch, context, self.glossary
                    )
                    
                    # å¤„ç†ç»“æœï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
                    batch_success = 0
                    with lock:
                        for seg, trans in zip(batch, batch_results):
                            if trans and not (isinstance(trans, str) and (trans.startswith("[Failed") or trans.endswith("Failed]"))):
                                seg.translated_text = trans
                                self.checkpoint.mark_segment_completed(seg.segment_id)
                                stats["success"] += 1
                                batch_success += 1
                            else:
                                seg.translated_text = trans if trans else "[Failed: Empty response]"
                                self.checkpoint.mark_segment_failed(seg.segment_id, trans or "Empty response")
                            stats["processed"] += 1
                        
                        stats["completed_batches"] += 1
                        
                        # æ¯å®Œæˆä¸€ä¸ª batch å°±ä¿å­˜
                        self._save_structure_map(self.all_segments)
                        self.checkpoint.save_checkpoint()
                    
                    logger.info(f"âœ… æ‰¹æ¬¡ {batch_idx}/{total_batches} å®Œæˆ (æœ¬æ‰¹æˆåŠŸ: {batch_success}/{len(batch)}, æ€»è¿›åº¦: {stats['completed_batches']}/{total_batches})")
                    return batch_idx, True
                    
                except Exception as e:
                    logger.error(f"âŒ æ‰¹æ¬¡ {batch_idx} å¤±è´¥: {e}")
                    
                    # æ ‡è®°å¤±è´¥ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
                    with lock:
                        for seg in batch:
                            seg.translated_text = f"[Failed: {str(e)}]"
                            self.checkpoint.mark_segment_failed(seg.segment_id, str(e))
                            stats["processed"] += 1
                        
                        stats["completed_batches"] += 1
                        
                        # ä¿å­˜å¤±è´¥çŠ¶æ€
                        try:
                            self._save_structure_map(self.all_segments)
                            self.checkpoint.save_checkpoint()
                        except Exception as save_exc:
                            logger.error(f"ä¿å­˜å¤±è´¥çŠ¶æ€æ—¶å‡ºé”™: {save_exc}")
                    
                    return batch_idx, False
        
        async def _run_concurrent_batches():
            """å¹¶å‘æ‰§è¡Œæ‰€æœ‰ batch"""
            semaphore = asyncio.Semaphore(max_concurrent)
            
            # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
            tasks = [
                _process_single_batch(idx, batch, semaphore)
                for idx, batch in enumerate(batches, 1)
            ]
            
            # å¹¶å‘æ‰§è¡Œï¼ˆä½¿ç”¨ as_completed è·å–å®æ—¶è¿›åº¦ï¼‰
            results = []
            for coro in asyncio.as_completed(tasks):
                try:
                    batch_idx, success = await coro
                    results.append((batch_idx, success))
                except Exception as e:
                    logger.error(f"æ‰¹æ¬¡ä»»åŠ¡å¼‚å¸¸: {e}")
            
            return results
        
        # ä½¿ç”¨ Rich è¿›åº¦æ¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        import time
        start_time = time.time()
        
        if RICH_AVAILABLE:
            console = Console()
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                TextColumn("â€¢"),
                TextColumn("{task.completed}/{task.total}"),
                TimeElapsedColumn(),
                console=console,
                transient=False,
                refresh_per_second=2
            ) as progress:
                batch_task = progress.add_task(f"[cyan]æ‰¹æ¬¡è¿›åº¦ (å¹¶å‘:{max_concurrent})", total=total_batches)
                segment_task = progress.add_task("[green]ç‰‡æ®µè¿›åº¦", total=total_segments)
                
                # å¯åŠ¨åå°ä»»åŠ¡æ›´æ–°è¿›åº¦æ¡
                async def _run_with_progress():
                    semaphore = asyncio.Semaphore(max_concurrent)
                    tasks = [
                        _process_single_batch(idx, batch, semaphore)
                        for idx, batch in enumerate(batches, 1)
                    ]
                    
                    for coro in asyncio.as_completed(tasks):
                        try:
                            await coro
                            # æ›´æ–°è¿›åº¦æ¡
                            progress.update(batch_task, completed=stats["completed_batches"])
                            progress.update(segment_task, completed=stats["processed"])
                        except Exception as e:
                            logger.error(f"æ‰¹æ¬¡ä»»åŠ¡å¼‚å¸¸: {e}")
                
                # æ‰§è¡Œå¹¶å‘ç¿»è¯‘
                asyncio.run(_run_with_progress())
                
                # æœ€ç»ˆæ›´æ–°
                progress.update(batch_task, completed=total_batches)
                progress.update(segment_task, completed=stats["processed"])
        else:
            # æ—  Rich çš„ç®€å•æ¨¡å¼
            asyncio.run(_run_concurrent_batches())
        
        # è¾“å‡ºç»Ÿè®¡
        elapsed = time.time() - start_time
        logger.info(f"â±ï¸  ç¿»è¯‘å®Œæˆï¼è€—æ—¶: {elapsed:.1f}s")
        logger.info(f"ğŸ“Š æˆåŠŸ: {stats['success']}/{stats['processed']} ç‰‡æ®µ")
        logger.info(f"ğŸ“Š é€Ÿåº¦: {stats['processed']/elapsed:.2f} segments/s")
        logger.info(f"ğŸ“Š å¹¶å‘æ•ˆç‡: {max_concurrent} batches åŒæ—¶è¿è¡Œ")
    
    def _cleanup_resources(self) -> None:
        """æ¸…ç†èµ„æº"""
        try:
            if hasattr(self.translator, '_async_translator') and self.translator._async_translator:
                self.translator._async_translator.cleanup()
            if hasattr(self.translator, 'cache_manager') and self.translator.cache_manager:
                self.translator.cache_manager.cleanup_all_caches()
            logger.info("âœ… èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            logger.debug(f"æ¸…ç†èµ„æºæ—¶å‡ºç°è­¦å‘Š: {e}")
    
    def _render_output(self) -> None:
        """Render: ç”Ÿæˆæœ€ç»ˆæ–‡æ¡£ï¼ˆMarkdown + PDFï¼‰"""
        logger.info("ğŸ“„ å¼€å§‹æ¸²æŸ“æœ€ç»ˆæ–‡æ¡£...")
        
        # å†³å®šæœ€ç»ˆè¾“å‡ºç›®å½•
        if self.settings.files.final_output_dir:
            final_dir = self.settings.files.final_output_dir
            final_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"   - è‡ªå®šä¹‰è¾“å‡ºç›®å½•: {final_dir}")
        else:
            # é»˜è®¤è¾“å‡ºåˆ°æºæ–‡ä»¶æ‰€åœ¨ç›®å½•
            final_dir = self.settings.files.document_path.parent
            logger.info(f"   - è¾“å‡ºåˆ°æºæ–‡ä»¶ç›®å½•: {final_dir}")
        
        # 1. ç”Ÿæˆ Markdown
        md_renderer = MarkdownRenderer(self.settings)
        md_output_path = final_dir / f"{Path(self.file_path.name).stem}_Translated.md"
        md_renderer.render_to_file(self.all_segments, md_output_path, f"åŸæ–‡: {self.file_path.name}")
        logger.info(f"âœ… Markdown å·²ä¿å­˜åˆ°: {md_output_path}")
        
        # 2. ç”Ÿæˆ PDFï¼ˆå¯é€‰ï¼Œå¦‚æœä¾èµ–å¯ç”¨ï¼‰
        try:
            from ..renderer.pdf import PDFRenderer
            pdf_renderer = PDFRenderer(self.settings)
            
            pdf_path = final_dir / f"{Path(self.file_path.name).stem}_Translated.pdf"
            pdf_renderer.render_to_file(self.all_segments, pdf_path, f"åŸæ–‡: {self.file_path.name}")
            logger.info(f"âœ… PDF å·²ä¿å­˜åˆ°: {pdf_path}")
        except ImportError:
            logger.info("â„¹ï¸  è·³è¿‡ PDF ç”Ÿæˆï¼ˆæœªå®‰è£…ç›¸å…³ä¾èµ–ï¼‰")
        except Exception as e:
            logger.warning(f"âš ï¸  PDF ç”Ÿæˆå¤±è´¥: {e}")
            logger.info("ğŸ’¡ å·²ç”Ÿæˆ Markdown æ–‡ä»¶ï¼Œå¯æ‰‹åŠ¨è½¬æ¢ä¸º PDF")
        
        logger.info("âœ… æ–‡æ¡£æ¸²æŸ“å®Œæˆ")
    
    def _save_structure_map(self, segments: SegmentList) -> None:
        """
        Save: ä¿å­˜å®Œæ•´çš„æ–‡æ¡£ç»“æ„çŠ¶æ€åˆ° JSON æ–‡ä»¶
        è¿™æ˜¯å•ä¸€çœŸç†æºçš„æŒä¹…åŒ–
        """
        try:
            self.structure_path.parent.mkdir(parents=True, exist_ok=True)
            
            # åºåˆ—åŒ–ä¸ºå­—å…¸åˆ—è¡¨
            data = [seg.model_dump() for seg in segments]
            
            # å¼ºåˆ¶å†™å…¥å¹¶åˆ·æ–°
            with open(self.structure_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
                f.flush()  # å¼ºåˆ¶åˆ·æ–°ç¼“å†²åŒº
                os.fsync(f.fileno())  # å¼ºåˆ¶åŒæ­¥åˆ°ç£ç›˜
            
            # ç»Ÿè®¡å·²ç¿»è¯‘æ•°é‡ï¼ˆç”¨äºè¿›åº¦æ˜¾ç¤ºï¼‰
            translated_count = sum(1 for seg in segments if seg.is_translated)
            logger.info(f"ğŸ’¾ Structure map å·²ä¿å­˜åˆ°: {self.structure_path}")
            logger.info(f"ğŸ’¾ ç¿»è¯‘è¿›åº¦: {translated_count}/{len(segments)} ä¸ªç‰‡æ®µå·²å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç»“æ„çŠ¶æ€å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise
    
    def _get_context_from_memory(self, current_segment: ContentSegment, max_length: int) -> str:
        """
        ä»å†…å­˜ä¸­è·å–ç¿»è¯‘ä¸Šä¸‹æ–‡ï¼ˆO(1) å¿«é€ŸæŸ¥æ‰¾ï¼‰
        
        è®¾è®¡è¯´æ˜ï¼š
        ä½¿ç”¨ä¸Šä¸€ä¸ª segment çš„**åŸæ–‡**ï¼ˆè€Œéè¯‘æ–‡ï¼‰ä½œä¸ºä¸Šä¸‹æ–‡
        
        è¿™ç§è®¾è®¡çš„ä¼˜åŠ¿ï¼š
        1. å¼‚æ­¥å‹å¥½ï¼šåŸæ–‡åœ¨ç¿»è¯‘å‰å°±å·²ç¡®å®šï¼Œå¤šä¸ª batch å¯å¹¶å‘æ‰§è¡Œï¼Œæ— éœ€ç­‰å¾…å‰åºç¿»è¯‘å®Œæˆ
        2. ç¡®å®šæ€§å¼ºï¼šcontext åœ¨ç¿»è¯‘å¼€å§‹å‰å°±å›ºå®šï¼Œä¾¿äºå¤ç°å’Œè°ƒè¯•
        3. é¢„è®¡ç®—å¯è¡Œï¼šå¯ä»¥åœ¨åˆ†å‘å¼‚æ­¥ä»»åŠ¡å‰é¢„å…ˆè®¡ç®—æ‰€æœ‰ segment çš„ context
        
        åŠ£åŠ¿ï¼š
        - æ— æ³•åˆ©ç”¨å·²ç¿»è¯‘å†…å®¹çš„æœ¯è¯­è¡¨ä¸€è‡´æ€§ï¼ˆä½†é€šè¿‡ glossary æœºåˆ¶å¼¥è¡¥ï¼‰
        
        Args:
            current_segment: å½“å‰å¾…ç¿»è¯‘çš„ segment
            max_length: ä¸Šä¸‹æ–‡æœ€å¤§é•¿åº¦é™åˆ¶
            
        Returns:
            å‰ä¸€ä¸ª segment åŸæ–‡çš„å 25% éƒ¨åˆ†ï¼ˆä¸è¶…è¿‡ max_lengthï¼‰
        """
        # ä½¿ç”¨ç´¢å¼•å¿«é€ŸæŸ¥æ‰¾ï¼ˆO(1)ï¼‰ï¼Œé¿å…éå†
        current_idx = self._segment_index.get(current_segment.segment_id, -1)
        if current_idx <= 0:
            return ""

        # è·å–ä¸Šä¸€ä¸ªç‰‡æ®µçš„åŸæ–‡ä½œä¸ºä¸Šä¸‹æ–‡
        prev_seg = self.all_segments[current_idx - 1]
        if not prev_seg.original_text or not prev_seg.original_text.strip():
            return ""
        
        # è®¡ç®—ä¸Šä¸‹æ–‡é•¿åº¦ï¼š25% çš„ MAX_CHUNK_SIZE
        context_length = int(self.settings.processing.max_chunk_size * 0.25)

        # å¦‚æœåŸæ–‡é•¿åº¦è¶…è¿‡ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ï¼Œå–å25%çš„å†…å®¹
        original_text = prev_seg.original_text.strip()
        if len(original_text) > context_length:
            # ä»åŸæ–‡æœ«å°¾å‘å‰å–æŒ‡å®šé•¿åº¦
            context_text = original_text[-context_length:].strip()
        else:
            context_text = original_text

        # ç¡®ä¿ä¸è¶…è¿‡max_lengthå‚æ•°
        if len(context_text) > max_length:
            context_text = context_text[-max_length:].strip()

        return context_text
    
    def _build_segment_index(self) -> None:
        """æ„å»º segment_id -> index çš„å¿«é€Ÿç´¢å¼•"""
        self._segment_index = {
            seg.segment_id: idx 
            for idx, seg in enumerate(self.all_segments)
        }
        logger.debug(f"ğŸ“‡ å·²æ„å»º segment ç´¢å¼• ({len(self._segment_index)} æ¡)")
