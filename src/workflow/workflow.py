"""
ç¿»è¯‘å·¥ä½œæµæ¨¡å—
"""
import asyncio
import json
from pathlib import Path
from typing import Dict, Optional, List

from ..core.schema import Settings, SegmentList, ContentSegment
from ..core.exceptions import TranslationError
from ..utils.logger import logger
from ..utils.file import create_output_directory, get_file_hash
from ..translator import GeminiTranslator, OpenAICompatibleTranslator, CheckpointManager
from ..parser.loader import load_document_structure as parse_document
from ..parser.helpers import is_likely_chinese
from ..renderer.markdown import MarkdownRenderer

# å°è¯•å¯¼å…¥ Rich è¿›åº¦æ˜¾ç¤º
try:
    from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn, TimeElapsedColumn
    from rich.console import Console
    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False


class TranslationWorkflow:
    """
    ç¿»è¯‘å·¥ä½œæµç±» - å°è£…å®Œæ•´çš„æ–‡æ¡£ç¿»è¯‘ä¸šåŠ¡é€»è¾‘
    
    èŒè´£ï¼š
    - æ–‡æ¡£åŠ è½½å’Œè§£æ
    - æ ‡é¢˜é¢„ç¿»è¯‘
    - æœ¯è¯­è¡¨ç”Ÿæˆå’Œç®¡ç†
    - æ‰¹é‡ç¿»è¯‘æ‰§è¡Œï¼ˆåŒæ­¥/å¼‚æ­¥ï¼‰
    - è¿›åº¦ç®¡ç†å’Œæ–­ç‚¹ç»­ä¼ 
    - æœ€ç»ˆæ–‡æ¡£æ¸²æŸ“
    """
    
    def __init__(self, settings: Settings):
        """
        åˆå§‹åŒ–ç¿»è¯‘å·¥ä½œæµ
        
        Args:
            settings: å…¨å±€è®¾ç½®å¯¹è±¡ï¼ŒåŒ…å«æ‰€æœ‰é…ç½®ä¿¡æ¯
        """
        self.settings = settings
        self.file_path = settings.files.document_path
        self.file_hash = get_file_hash(self.file_path)
        self.project_name = self.file_hash
        
        # å‡†å¤‡å·¥ä½œç›®å½•
        self.project_dir = create_output_directory(
            settings.files.output_base_dir,
            self.project_name
        )
        self.structure_path = self.project_dir / "structure_map.json"
        
        # æ ¸å¿ƒç»„ä»¶ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self.all_segments: Optional[SegmentList] = None
        self.translator: Optional[GeminiTranslator] = None
        self.cache_manager = None
        self.checkpoint: Optional[CheckpointManager] = None
        self.glossary: Optional[Dict[str, str]] = None
        
    def execute(self) -> None:
        """æ‰§è¡Œå®Œæ•´çš„ç¿»è¯‘å·¥ä½œæµ"""
        logger.info(f"ğŸš€ å¼€å§‹å¤„ç†æ–‡æ¡£: {self.file_path.name}")
        logger.info(f"   - ç¿»è¯‘æ¨¡å¼: {self.settings.processing.translation_mode_entity.name}")
        logger.info(f"   - é¡¹ç›®æ ‡è¯† (Hash): {self.project_name}")
        
        try:
            # 1. åŠ è½½æ–‡æ¡£ç»“æ„
            self._load_document()
            
            # 2. åˆå§‹åŒ–ç¿»è¯‘å™¨å’Œç¼“å­˜
            self._initialize_translator()
            
            # 3. é¢„ç¿»è¯‘æ ‡é¢˜
            self._pre_translate_titles()
            
            # 4. ç”Ÿæˆæœ¯è¯­è¡¨
            self._generate_glossary()
            
            # 5. åˆå§‹åŒ–æ–­ç‚¹ç»­ä¼ 
            self._initialize_checkpoint()
            
            # 6. æ‰§è¡Œç¿»è¯‘å¾ªç¯
            self._run_translation_loop()
            
            # 7. æ¸…ç†èµ„æº
            self._cleanup_resources()
            
            # 8. æ¸²æŸ“æœ€ç»ˆæ–‡æ¡£
            self._render_output()
            
        except Exception as e:
            logger.error(f"âŒ ç¿»è¯‘å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
            raise
    
    def _load_document(self) -> None:
        """
        Load é˜¶æ®µï¼šåŠ è½½æ–‡æ¡£ç»“æ„åˆ°å†…å­˜

        ä¼˜å…ˆçº§ï¼š
        1. ä» structure_map.json åŠ è½½ï¼ˆåŒ…å«ç¿»è¯‘çŠ¶æ€ï¼‰
        2. è§£æåŸå§‹æ–‡æ¡£ç”Ÿæˆæ–°ç»“æ„
        """
        logger.info("ğŸ“– åŠ è½½æ–‡æ¡£ç»“æ„...")
        
        # 1. å°è¯•ä» structure_map.json åŠ è½½
        if self.structure_path.exists() and self.settings.processing.enable_cache:
            try:
                with open(self.structure_path, 'r', encoding='utf-8') as f:
                    raw_data = json.load(f)
                    segments = [ContentSegment(**item) for item in raw_data]
                    logger.info(f"ğŸ“¦ ä»ç»“æ„æ–‡ä»¶åŠ è½½ {len(segments)} ä¸ªç‰‡æ®µ")
                    self.all_segments = segments
                    logger.info(f"âœ… å·²åŠ è½½ {len(self.all_segments)} ä¸ªå†…å®¹ç‰‡æ®µ")
                    return
            except Exception as e:
                logger.warning(f"âš ï¸ structure_map.json æŸåï¼Œå°†é‡æ–°è§£æ: {e}")
        
        # 2. é‡æ–°è§£ææ–‡æ¡£
        logger.info("âš™ï¸ è§£ææ–‡æ¡£ç»“æ„...")
        segments = parse_document(self.file_path, self.structure_path, self.settings)
        
        if segments:
            logger.info(f"âœ… è§£æå®Œæˆï¼Œç”Ÿæˆ {len(segments)} ä¸ªç‰‡æ®µ")
            self._save_structure_map(segments)
            self.all_segments = segments
        else:
            logger.error("âŒ æ–‡æ¡£è§£æå¤±è´¥")
            raise TranslationError("æ–‡æ¡£è§£æå¤±è´¥ï¼Œæœªç”Ÿæˆä»»ä½•å†…å®¹ç‰‡æ®µ")
        
        logger.info(f"âœ… å·²åŠ è½½ {len(self.all_segments)} ä¸ªå†…å®¹ç‰‡æ®µ")
    
    def _initialize_translator(self) -> None:
        """åˆå§‹åŒ–ç¿»è¯‘å™¨å’Œç¼“å­˜ç®¡ç†å™¨"""
        provider = (getattr(self.settings.api, 'translator_provider', 'gemini') or 'gemini').lower()

        if provider == 'gemini':
            # åˆ›å»ºç¼“å­˜ç®¡ç†å™¨ï¼ˆå¦‚æœå¯ç”¨ Gemini Context Cachingï¼‰
            if self.settings.processing.enable_gemini_caching:
                from ..translator.support import CachePersistenceManager
                self.cache_manager = CachePersistenceManager(self.settings)
                logger.info("âœ… Gemini ç¼“å­˜ç®¡ç†å™¨å·²åˆå§‹åŒ–")

            self.translator = GeminiTranslator(
                self.settings,
                cache_manager=self.cache_manager
            )
            logger.info("âœ… Gemini ç¿»è¯‘å™¨å·²åˆå§‹åŒ–")
            return

        if provider in {'deepseek', 'openai', 'openai-compatible', 'openai_compatible'}:
            # OpenAI-compatible provider (DeepSeek)
            self.cache_manager = None
            self.translator = OpenAICompatibleTranslator(self.settings)
            logger.info(f"âœ… OpenAI-compatible ç¿»è¯‘å™¨å·²åˆå§‹åŒ– (provider={provider})")
            return

        raise TranslationError(f"æœªçŸ¥ translator_provider: {provider}")
    
    def _pre_translate_titles(self) -> None:
        """é¢„ç¿»è¯‘ç« èŠ‚æ ‡é¢˜"""
        logger.info("ğŸ“ å¼€å§‹é¢„ç¿»è¯‘æ ‡é¢˜...")
        
        # æå–å¾…ç¿»è¯‘æ ‡é¢˜
        raw_titles = []
        for seg in self.all_segments:
            if (seg.is_new_chapter and seg.chapter_title and
                seg.chapter_title.strip() and not is_likely_chinese(seg.chapter_title)):
                raw_titles.append(seg.chapter_title)
        
        if not raw_titles:
            logger.info("   - æ— éœ€ç¿»è¯‘çš„æ ‡é¢˜")
            return
        
        # å»é‡
        unique_titles = list(dict.fromkeys(raw_titles))
        logger.info(f"   - å‘ç° {len(unique_titles)} ä¸ªå”¯ä¸€æ ‡é¢˜")
        
        # æ‰¹é‡ç¿»è¯‘
        translation_map = self.translator.translate_titles(unique_titles)
        
        # å›å¡«ç»“æœ
        update_count = 0
        for seg in self.all_segments:
            if seg.is_new_chapter and seg.chapter_title in translation_map:
                translated = translation_map[seg.chapter_title]
                if translated:
                    seg.chapter_title = translated
                    update_count += 1
        
        logger.info(f"   - æ›´æ–°äº† {update_count} ä¸ªæ ‡é¢˜")
        
        # ä¿å­˜æ›´æ–°åçš„ç»“æ„
        self._save_structure_map(self.all_segments)
        logger.info("âœ… æ ‡é¢˜é¢„ç¿»è¯‘å®Œæˆ")
    
    def _generate_glossary(self) -> None:
        """ç”Ÿæˆæˆ–åŠ è½½æœ¯è¯­è¡¨"""
        # å‡†å¤‡æœ¯è¯­è¡¨æ–‡ä»¶è·¯å¾„
        glossary_merged_path = self.project_dir / "glossary_merged.json"
        glossary_path = self.project_dir / "glossary.json"
        
        # ä¼˜å…ˆä½¿ç”¨åˆå¹¶çš„æœ¯è¯­è¡¨
        if glossary_merged_path.exists():
            glossary_path = glossary_merged_path
        
        # å°è¯•åŠ è½½å·²æœ‰æœ¯è¯­è¡¨
        if glossary_path.exists():
            try:
                with open(glossary_path, 'r', encoding='utf-8') as gf:
                    self.glossary = json.load(gf)
                logger.info(f"ğŸ“š ä»ç¼“å­˜åŠ è½½å·²æœ‰æœ¯è¯­è¡¨ ({len(self.glossary)} æ¡) -> {glossary_path}")
                return
            except Exception as e:
                logger.warning(f"âš ï¸ åŠ è½½å·²ä¿å­˜çš„æœ¯è¯­è¡¨å¤±è´¥ï¼Œå°†é‡æ–°ç”Ÿæˆ: {e}")
        
        # ç”Ÿæˆæ–°çš„æœ¯è¯­è¡¨ï¼ˆé€šè¿‡é¢„ç¿»è¯‘éƒ¨åˆ†æ–‡æ¡£ï¼‰
        try:
            ratio = getattr(self.settings.processing, 'glossary_preamble_ratio', 0.1)
            pre_count = max(1, int(len(self.all_segments) * float(ratio)))
        except Exception:
            pre_count = max(1, int(len(self.all_segments) * 0.1))
        
        if pre_count > 0:
            pre_segments = self.all_segments[:pre_count]
            pending_pre = [seg for seg in pre_segments if not seg.is_translated]
            
            if pending_pre:
                logger.info(f"ğŸ§ª é¢„ç¿»è¯‘å‰ {pre_count} ä¸ªç‰‡æ®µä»¥æ„å»ºæœ¯è¯­è¡¨...")
                translations = self.translator.translate_batch(pending_pre, context="")
                for seg, t in zip(pending_pre, translations):
                    seg.translated_text = t
                self._save_structure_map(self.all_segments)
            
            # æå–æœ¯è¯­è¡¨
            self.glossary = self.translator.extract_glossary(pre_segments)
            
            # æŒä¹…åŒ–æœ¯è¯­è¡¨
            if self.glossary:
                try:
                    with open(glossary_path, 'w', encoding='utf-8') as gf:
                        json.dump(self.glossary, gf, ensure_ascii=False, indent=2)
                    logger.info(f"ğŸ’¾ æœ¯è¯­è¡¨å·²ä¿å­˜åˆ°: {glossary_path}")
                    logger.info(f"ğŸ”¥ å·²ç”Ÿæˆæœ¯è¯­è¡¨ï¼ŒåŒ…å« {len(self.glossary)} æ¡æœ¯è¯­")
                except Exception as e:
                    logger.warning(f"âš ï¸ ä¿å­˜æœ¯è¯­è¡¨å¤±è´¥: {e}")
            else:
                logger.info("âšª æœªç”Ÿæˆæœ‰æ•ˆæœ¯è¯­è¡¨")
    
    def _initialize_checkpoint(self) -> None:
        """åˆå§‹åŒ–æ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨"""
        self.checkpoint = CheckpointManager(self.settings)
        self.checkpoint.update_total_segments(len(self.all_segments))
        logger.info("âœ… æ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨å·²åˆå§‹åŒ–")
    
    def _run_translation_loop(self) -> None:
        """æ‰§è¡Œç¿»è¯‘å¾ªç¯ï¼ˆæ”¯æŒåŒæ­¥/å¼‚æ­¥æ¨¡å¼ï¼‰"""
        # è·å–å¾…ç¿»è¯‘ç‰‡æ®µ
        pending_segments = self.checkpoint.get_pending_segments(self.all_segments)
        
        if not pending_segments:
            logger.info("ğŸ‰ æ‰€æœ‰ç‰‡æ®µå‡å·²ç¿»è¯‘å®Œæˆï¼")
            return
        
        logger.info(f"ğŸ”„ å‘ç° {len(pending_segments)} ä¸ªå¾…ç¿»è¯‘ç‰‡æ®µ")
        
        # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨å¼‚æ­¥æ¨¡å¼
        use_async = (
            self.settings.processing.enable_async and 
            len(pending_segments) >= self.settings.processing.async_threshold and
            hasattr(self.translator, 'async_translator')
        )
        
        if use_async:
            self._run_async_translation(pending_segments)
        else:
            self._run_sync_translation(pending_segments)
    
    def _run_sync_translation(self, pending_segments: SegmentList) -> None:
        """åŒæ­¥ç¿»è¯‘æ¨¡å¼"""
        logger.info("ğŸ”„ ä½¿ç”¨åŒæ­¥æ¨¡å¼ç¿»è¯‘")
        logger.info(f"ğŸ“ å¼€å§‹åŒæ­¥ç¿»è¯‘ {len(pending_segments)} ä¸ªç‰‡æ®µ...")
        
        try:
            results = self.translator.translate_batch(
                pending_segments,
                context="",
                glossary=self.glossary
            )
            
            # å¤„ç†ç¿»è¯‘ç»“æœ
            success_count = 0
            for seg, trans in zip(pending_segments, results):
                if trans and not trans.startswith("[Failed"):
                    seg.translated_text = trans
                    self.checkpoint.mark_segment_completed(seg.segment_id)
                    success_count += 1
                else:
                    seg.translated_text = trans if trans else "[Failed: Empty response]"
                    self.checkpoint.mark_segment_failed(seg.segment_id, trans or "Empty response")
            
            # ä¿å­˜ç»“æœ
            self._save_structure_map(self.all_segments)
            self.checkpoint.save_checkpoint()
            
            logger.info(f"âœ… åŒæ­¥ç¿»è¯‘å®Œæˆ: {success_count}/{len(pending_segments)} æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ åŒæ­¥ç¿»è¯‘å¤±è´¥: {e}")
            for seg in pending_segments:
                seg.translated_text = f"[Failed: {str(e)}]"
                self.checkpoint.mark_segment_failed(seg.segment_id, str(e))
            self._save_structure_map(self.all_segments)
            self.checkpoint.save_checkpoint()
            raise
    
    def _run_async_translation(self, pending_segments: SegmentList) -> None:
        """å¼‚æ­¥ç¿»è¯‘æ¨¡å¼"""
        logger.info("âš¡ ä½¿ç”¨å¼‚æ­¥æ¨¡å¼ç¿»è¯‘ï¼ˆæå‡é€Ÿåº¦ï¼‰")
        
        try:
            batch_size = self.settings.processing.batch_size
            batches = [
                pending_segments[i:i+batch_size] 
                for i in range(0, len(pending_segments), batch_size)
            ]
            total_batches = len(batches)
            
            logger.info(f"ğŸš€ å¼€å§‹å¼‚æ­¥ç¿»è¯‘ {len(pending_segments)} ä¸ªç‰‡æ®µï¼ˆ{total_batches} æ‰¹æ¬¡ï¼Œæ‰¹å¤§å° {batch_size}ï¼‰...")
            
            async def translate_all_batches_with_progress():
                """æ‰€æœ‰æ‰¹æ¬¡å¹¶å‘æ‰§è¡Œï¼Œå¸¦è¿›åº¦æ˜¾ç¤º"""
                tasks = [
                    self.translator.async_translator.translate_text_batch_async(
                        batch, "", self.glossary
                    )
                    for batch in batches
                ]
                
                # ä½¿ç”¨Richè¿›åº¦æ¡
                if RICH_AVAILABLE:
                    console = Console()
                    with Progress(
                        SpinnerColumn(),
                        TextColumn("[progress.description]{task.description}"),
                        BarColumn(),
                        TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                        TextColumn("â€¢"),
                        TextColumn("{task.completed}/{task.total} æ‰¹æ¬¡"),
                        TimeElapsedColumn(),
                        console=console
                    ) as progress:
                        task_id = progress.add_task("[cyan]å¼‚æ­¥ç¿»è¯‘è¿›åº¦", total=total_batches)
                        
                        results = [None] * total_batches
                        for i, task in enumerate(asyncio.as_completed(tasks)):
                            result = await task
                            results[i] = result
                            progress.update(task_id, advance=1)
                        
                        return results
                else:
                    return await asyncio.gather(*tasks)
            
            # æ‰§è¡Œå¹¶å‘ç¿»è¯‘
            all_results = asyncio.run(translate_all_batches_with_progress())
            
            # å¤„ç†ç¿»è¯‘ç»“æœ
            success_count = 0
            batch_idx = 0
            for batch, batch_results in zip(batches, all_results):
                for seg, trans in zip(batch, batch_results):
                    if trans and not trans.startswith("[Failed"):
                        seg.translated_text = trans
                        self.checkpoint.mark_segment_completed(seg.segment_id)
                        success_count += 1
                    else:
                        seg.translated_text = trans if trans else "[Failed: Empty response]"
                        self.checkpoint.mark_segment_failed(seg.segment_id, trans or "Empty response")
                
                # æ¯5ä¸ªbatchä¿å­˜ä¸€æ¬¡
                batch_idx += 1
                if batch_idx % 5 == 0:
                    self._save_structure_map(self.all_segments)
                    self.checkpoint.save_checkpoint()
            
            # æœ€ç»ˆä¿å­˜
            self._save_structure_map(self.all_segments)
            self.checkpoint.save_checkpoint()
            
            logger.info(f"âœ… å¼‚æ­¥ç¿»è¯‘å®Œæˆ: {success_count}/{len(pending_segments)} æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ å¼‚æ­¥ç¿»è¯‘å¤±è´¥ï¼Œé™çº§åˆ°åŒæ­¥æ¨¡å¼: {e}")
            self._run_sync_translation(pending_segments)
    
    def _cleanup_resources(self) -> None:
        """æ¸…ç†èµ„æº"""
        try:
            if hasattr(self.translator, '_async_translator') and self.translator._async_translator:
                self.translator._async_translator.cleanup()
            if hasattr(self.translator, 'cache_manager') and self.translator.cache_manager:
                self.translator.cache_manager.cleanup_all_caches()
            logger.info("âœ… èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            logger.debug(f"æ¸…ç†èµ„æºæ—¶å‡ºç°è­¦å‘Š: {e}")
    
    def _render_output(self) -> None:
        """Render: ç”Ÿæˆæœ€ç»ˆæ–‡æ¡£ï¼ˆMarkdown + PDFï¼‰"""
        logger.info("ğŸ“„ å¼€å§‹æ¸²æŸ“æœ€ç»ˆæ–‡æ¡£...")
        
        # å†³å®šæœ€ç»ˆè¾“å‡ºç›®å½•
        if self.settings.files.final_output_dir:
            final_dir = self.settings.files.final_output_dir
            final_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"   - è‡ªå®šä¹‰è¾“å‡ºç›®å½•: {final_dir}")
        else:
            # é»˜è®¤è¾“å‡ºåˆ°æºæ–‡ä»¶æ‰€åœ¨ç›®å½•
            final_dir = self.settings.files.document_path.parent
            logger.info(f"   - è¾“å‡ºåˆ°æºæ–‡ä»¶ç›®å½•: {final_dir}")
        
        # 1. ç”Ÿæˆ Markdown
        md_renderer = MarkdownRenderer(self.settings)
        md_output_path = final_dir / f"{Path(self.file_path.name).stem}_Translated.md"
        md_renderer.render_to_file(self.all_segments, md_output_path, f"åŸæ–‡: {self.file_path.name}")
        logger.info(f"âœ… Markdown å·²ä¿å­˜åˆ°: {md_output_path}")
        
        # 2. ç”Ÿæˆ PDFï¼ˆå¯é€‰ï¼Œå¦‚æœä¾èµ–å¯ç”¨ï¼‰
        try:
            from ..renderer.pdf import PDFRenderer
            pdf_renderer = PDFRenderer(self.settings)
            
            pdf_path = final_dir / f"{Path(self.file_path.name).stem}_Translated.pdf"
            pdf_renderer.render_to_file(self.all_segments, pdf_path, f"åŸæ–‡: {self.file_path.name}")
            logger.info(f"âœ… PDF å·²ä¿å­˜åˆ°: {pdf_path}")
        except ImportError:
            logger.info("â„¹ï¸  è·³è¿‡ PDF ç”Ÿæˆï¼ˆæœªå®‰è£…ç›¸å…³ä¾èµ–ï¼‰")
        except Exception as e:
            logger.warning(f"âš ï¸  PDF ç”Ÿæˆå¤±è´¥: {e}")
            logger.info("ğŸ’¡ å·²ç”Ÿæˆ Markdown æ–‡ä»¶ï¼Œå¯æ‰‹åŠ¨è½¬æ¢ä¸º PDF")
        
        logger.info("âœ… æ–‡æ¡£æ¸²æŸ“å®Œæˆ")
    
    def _save_structure_map(self, segments: SegmentList) -> None:
        """
        Save: ä¿å­˜å®Œæ•´çš„æ–‡æ¡£ç»“æ„çŠ¶æ€åˆ° JSON æ–‡ä»¶
        è¿™æ˜¯å•ä¸€çœŸç†æºçš„æŒä¹…åŒ–
        """
        try:
            self.structure_path.parent.mkdir(parents=True, exist_ok=True)
            
            # åºåˆ—åŒ–ä¸ºå­—å…¸åˆ—è¡¨
            data = [seg.model_dump() for seg in segments]
            
            with open(self.structure_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            logger.debug(f"ğŸ’¾ ç»“æ„çŠ¶æ€å·²ä¿å­˜: {len(segments)} ä¸ªç‰‡æ®µ")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç»“æ„çŠ¶æ€å¤±è´¥: {e}")
            raise
    
    def _get_context_from_memory(self, current_segment: ContentSegment, max_length: int) -> str:
        """
        ä»å†…å­˜ä¸­è·å–ç¿»è¯‘ä¸Šä¸‹æ–‡
        é€šè¿‡ segment_id åœ¨ all_segments ä¸­æŸ¥æ‰¾å‰æ–‡å·²ç¿»è¯‘ç‰‡æ®µ
        """
        # æ‰¾åˆ°å½“å‰ç‰‡æ®µçš„ä½ç½®
        current_idx = next((i for i, seg in enumerate(self.all_segments) if seg.segment_id == current_segment.segment_id), -1)
        if current_idx == -1:
            return ""
        
        # è·å–å‰å‡ ä¸ªå·²ç¿»è¯‘çš„ç‰‡æ®µå†…å®¹
        context_parts = []
        context_length = 0
        
        # å‘å‰æŸ¥æ‰¾å·²ç¿»è¯‘çš„ç‰‡æ®µ
        for i in range(current_idx - 1, -1, -1):
            seg = self.all_segments[i]
            if seg.is_translated and seg.translated_text:
                # ä¼°ç®—é•¿åº¦ï¼ˆä¸­æ–‡å­—ç¬¦æŒ‰2å­—èŠ‚ç®—ï¼‰
                text_length = len(seg.translated_text.encode('utf-8'))
                if context_length + text_length > max_length:
                    break
                
                context_parts.insert(0, seg.translated_text)  # ä¿æŒé¡ºåº
                context_length += text_length
        
        return " ".join(context_parts).strip()
