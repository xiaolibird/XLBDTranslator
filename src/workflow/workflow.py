"""
ç¿»è¯‘å·¥ä½œæµæ¨¡å—
"""
import asyncio
import json
from pathlib import Path
from typing import Dict, Optional, List

from ..core.schema import Settings, SegmentList, ContentSegment
from ..core.exceptions import TranslationError
from ..utils.logger import logger
from ..utils.file import create_output_directory, get_file_hash
from ..translator import GeminiTranslator, OpenAICompatibleTranslator, CheckpointManager
from ..parser.loader import load_document_structure as parse_document
from ..parser.helpers import is_likely_chinese
from ..renderer.markdown import MarkdownRenderer

# å°è¯•å¯¼å…¥ Rich è¿›åº¦æ˜¾ç¤º
try:
    from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn, TimeElapsedColumn
    from rich.console import Console
    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False


class TranslationWorkflow:
    """
    ç¿»è¯‘å·¥ä½œæµç±» - å°è£…å®Œæ•´çš„æ–‡æ¡£ç¿»è¯‘ä¸šåŠ¡é€»è¾‘
    
    èŒè´£ï¼š
    - æ–‡æ¡£åŠ è½½å’Œè§£æ
    - æ ‡é¢˜é¢„ç¿»è¯‘
    - æœ¯è¯­è¡¨ç”Ÿæˆå’Œç®¡ç†
    - æ‰¹é‡ç¿»è¯‘æ‰§è¡Œï¼ˆåŒæ­¥/å¼‚æ­¥ï¼‰
    - è¿›åº¦ç®¡ç†å’Œæ–­ç‚¹ç»­ä¼ 
    - æœ€ç»ˆæ–‡æ¡£æ¸²æŸ“
    """
    
    def __init__(self, settings: Settings):
        """
        åˆå§‹åŒ–ç¿»è¯‘å·¥ä½œæµ
        
        Args:
            settings: å…¨å±€è®¾ç½®å¯¹è±¡ï¼ŒåŒ…å«æ‰€æœ‰é…ç½®ä¿¡æ¯
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° (stage, progress, message)
        """
        self.settings = settings
        self.file_path = settings.files.document_path
        self.file_hash = get_file_hash(self.file_path)
        self.project_name = self.file_hash
        
        # å‡†å¤‡å·¥ä½œç›®å½•
        self.project_dir = create_output_directory(
            settings.files.output_base_dir,
            self.project_name
        )
        self.structure_path = self.project_dir / "structure_map.json"
        
        # æ ¸å¿ƒç»„ä»¶ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self.all_segments: Optional[SegmentList] = None
        self.translator: Optional[GeminiTranslator] = None
        self.cache_manager = None
        self.checkpoint: Optional[CheckpointManager] = None
        self.glossary: Optional[Dict[str, str]] = None

    # Deprecated: progress reporting removed for a cleaner workflow interface.
    # Progress callbacks were removed to simplify the TranslationWorkflow class.

    def _optimize_batch_size_for_provider(self):
        """æ ¹æ®translator providerä¼˜åŒ–batch_sizeï¼Œå¹³è¡¡é€Ÿåº¦å’Œcontexté™åˆ¶
        
        Returns:
            tuple: (æ˜¯å¦ä¼˜åŒ–, åŸå§‹batch_size, ä¼˜åŒ–åbatch_size, åŸå› )
        """
        provider = getattr(self.settings.api, 'translator_provider', 'gemini').lower()
        original_batch_size = self.settings.processing.batch_size
        max_chunk_size = self.settings.processing.max_chunk_size
        
        # ä¼°ç®—æ¯æ‰¹æ¬¡çš„å­—ç¬¦é‡ï¼ˆä¸å«promptï¼‰
        estimated_chars_per_batch = original_batch_size * max_chunk_size
        
        # æ ¹æ®providerè°ƒæ•´batch_size
        if provider in {'deepseek', 'openai', 'openai-compatible'}:
            # äº‘ç«¯APIä½¿ç”¨å®Œæ•´ç‰ˆpromptï¼Œå­—ç¬¦æ•°è¾ƒå¤šï¼Œéœ€è¦å‡å°‘batch_size
            # DeepSeek/OpenAI context limit çº¦32K-128Kï¼Œä½†è€ƒè™‘promptå¼€é”€ï¼Œä¿å®ˆè®¾ç½®ä¸º60Kä¸Šé™
            max_safe_chars = 60000
            prompt_overhead = 2000  # ä¼°ç®—å®Œæ•´ç‰ˆpromptçš„å­—ç¬¦æ•°
            
            # è®¡ç®—å®‰å…¨çš„batch_size
            safe_batch_size = max(1, (max_safe_chars - prompt_overhead) // max_chunk_size)
            optimized_batch_size = min(original_batch_size, safe_batch_size, 3)  # æœ€å¤š3ä½œä¸ºç¡¬ä¸Šé™
            
            logger.info(f"ğŸ”§ å‚æ•°ä¼˜åŒ–åˆ†æ ({provider.upper()}):")
            logger.info(f"   ğŸ“Š å½“å‰é…ç½®: batch_size={original_batch_size}, max_chunk_size={max_chunk_size}")
            logger.info(f"   ğŸ“ ä¼°ç®—å­—ç¬¦é‡: {estimated_chars_per_batch:,} å­—ç¬¦/æ‰¹æ¬¡ (ä¸å«prompt)")
            logger.info(f"   âš ï¸  å®‰å…¨ä¸Šé™: {max_safe_chars:,} å­—ç¬¦/æ‰¹æ¬¡ (å«{prompt_overhead:,}å­—ç¬¦promptå¼€é”€)")
            logger.info(f"   ğŸ¯ ä¼˜åŒ–ç»“æœ: batch_size {original_batch_size} â†’ {optimized_batch_size}")
            
            if optimized_batch_size < original_batch_size:
                new_estimated_chars = optimized_batch_size * max_chunk_size + prompt_overhead
                logger.info(f"   âœ… æ–°é…ç½®å­—ç¬¦é‡: {new_estimated_chars:,} å­—ç¬¦/æ‰¹æ¬¡ (å®‰å…¨èŒƒå›´å†…)")
                logger.info(f"   ğŸ’¡ å¤‡é€‰æ–¹æ¡ˆ: å¯è€ƒè™‘å‡å°‘ max_chunk_size è‡³ {max_chunk_size//2} ä»¥å¢åŠ batch_size")
            else:
                logger.info(f"   âœ… å½“å‰é…ç½®å·²å®‰å…¨: {estimated_chars_per_batch + prompt_overhead:,} å­—ç¬¦/æ‰¹æ¬¡")
            
            reason = f"äº‘ç«¯APIä½¿ç”¨å®Œæ•´ç‰ˆpromptï¼Œå‡å°‘batch_sizeé¿å…è¶…å‡ºcontexté™åˆ¶"
            
        elif provider == 'gemini':
            # Geminiä½¿ç”¨å®Œæ•´ç‰ˆpromptï¼Œä½†context windowè¾ƒå¤§ (1M+ tokens)
            # ä¿å®ˆè®¾ç½®ä¸Šé™ä¸º20ä¸‡å­—ç¬¦
            max_safe_chars = 200000
            prompt_overhead = 2000
            
            safe_batch_size = max(1, (max_safe_chars - prompt_overhead) // max_chunk_size)
            optimized_batch_size = min(original_batch_size, safe_batch_size, 4)  # æœ€å¤š4
            
            logger.info(f"ğŸ”§ å‚æ•°ä¼˜åŒ–åˆ†æ (GEMINI):")
            logger.info(f"   ğŸ“Š å½“å‰é…ç½®: batch_size={original_batch_size}, max_chunk_size={max_chunk_size}")
            logger.info(f"   ğŸ“ ä¼°ç®—å­—ç¬¦é‡: {estimated_chars_per_batch:,} å­—ç¬¦/æ‰¹æ¬¡ (ä¸å«prompt)")
            logger.info(f"   âš ï¸  å®‰å…¨ä¸Šé™: {max_safe_chars:,} å­—ç¬¦/æ‰¹æ¬¡ (å«{prompt_overhead:,}å­—ç¬¦promptå¼€é”€)")
            logger.info(f"   ğŸ¯ ä¼˜åŒ–ç»“æœ: batch_size {original_batch_size} â†’ {optimized_batch_size}")
            
            if optimized_batch_size < original_batch_size:
                new_estimated_chars = optimized_batch_size * max_chunk_size + prompt_overhead
                logger.info(f"   âœ… æ–°é…ç½®å­—ç¬¦é‡: {new_estimated_chars:,} å­—ç¬¦/æ‰¹æ¬¡ (å®‰å…¨èŒƒå›´å†…)")
                logger.info(f"   ğŸ’¡ å¤‡é€‰æ–¹æ¡ˆ: å¯è€ƒè™‘å‡å°‘ max_chunk_size è‡³ {max_chunk_size//2} ä»¥å¢åŠ batch_size")
            else:
                logger.info(f"   âœ… å½“å‰é…ç½®å·²å®‰å…¨: {estimated_chars_per_batch + prompt_overhead:,} å­—ç¬¦/æ‰¹æ¬¡")
            
            reason = f"Geminiä½¿ç”¨å®Œæ•´ç‰ˆpromptï¼Œé€‚åº¦å‡å°‘batch_sizeä¿è¯ç¨³å®šæ€§"
            
        else:
            # æœ¬åœ°æ¨¡å‹ä½¿ç”¨ç®€åŒ–ç‰ˆpromptï¼Œå¯ä»¥ä¿æŒè¾ƒå¤§batch_size
            # æœ¬åœ°æ¨¡å‹é€šå¸¸æœ‰æ›´å¤§çš„context window
            max_safe_chars = 100000  # æœ¬åœ°æ¨¡å‹é€šå¸¸æ”¯æŒæ›´å¤§çš„ä¸Šä¸‹æ–‡
            prompt_overhead = 500     # ç®€åŒ–ç‰ˆpromptå¼€é”€è¾ƒå°
            
            safe_batch_size = max(1, (max_safe_chars - prompt_overhead) // max_chunk_size)
            optimized_batch_size = min(original_batch_size, safe_batch_size)
            
            logger.info(f"ğŸ”§ å‚æ•°ä¼˜åŒ–åˆ†æ ({provider.upper()}):")
            logger.info(f"   ğŸ“Š å½“å‰é…ç½®: batch_size={original_batch_size}, max_chunk_size={max_chunk_size}")
            logger.info(f"   ğŸ“ ä¼°ç®—å­—ç¬¦é‡: {estimated_chars_per_batch:,} å­—ç¬¦/æ‰¹æ¬¡ (ä¸å«prompt)")
            logger.info(f"   âš ï¸  å®‰å…¨ä¸Šé™: {max_safe_chars:,} å­—ç¬¦/æ‰¹æ¬¡ (å«{prompt_overhead:,}å­—ç¬¦promptå¼€é”€)")
            
            if optimized_batch_size < original_batch_size:
                logger.info(f"   ğŸ¯ ä¼˜åŒ–ç»“æœ: batch_size {original_batch_size} â†’ {optimized_batch_size}")
                new_estimated_chars = optimized_batch_size * max_chunk_size + prompt_overhead
                logger.info(f"   âœ… æ–°é…ç½®å­—ç¬¦é‡: {new_estimated_chars:,} å­—ç¬¦/æ‰¹æ¬¡ (å®‰å…¨èŒƒå›´å†…)")
                logger.info(f"   ğŸ’¡ å¤‡é€‰æ–¹æ¡ˆ: å¯è€ƒè™‘å‡å°‘ max_chunk_size è‡³ {max_chunk_size//2} ä»¥å¢åŠ batch_size")
            else:
                logger.info(f"   âœ… å‚æ•°ä¿æŒ: batch_size = {original_batch_size} (å½“å‰é…ç½®å·²å®‰å…¨)")
                logger.info(f"   ğŸ“ å½“å‰å­—ç¬¦é‡: {estimated_chars_per_batch + prompt_overhead:,} å­—ç¬¦/æ‰¹æ¬¡")
            
            reason = f"æœ¬åœ°æ¨¡å‹ä½¿ç”¨ç®€åŒ–ç‰ˆpromptï¼Œä¿æŒåŸæœ‰batch_size"
        
        # åº”ç”¨ä¼˜åŒ–åçš„batch_size
        optimized = optimized_batch_size != original_batch_size
        if optimized:
            self.settings.processing.batch_size = optimized_batch_size
        
        return optimized, original_batch_size, optimized_batch_size, reason

    def _build_translation_mode_config(self) -> Dict[str, str]:
        """æ„å»ºç”¨äºè°ƒç”¨ç¿»è¯‘å™¨çš„ translation_mode_config å­—å…¸"""
        mode_entity = getattr(self.settings.processing, 'translation_mode_entity', None)
        if mode_entity:
            return {
                'name': getattr(mode_entity, 'name', 'Auto'),
                'style': getattr(mode_entity, 'style', 'Fluent and precise'),
                'role_desc': getattr(mode_entity, 'role_desc', 'Expert translator')
            }
        return {
            'name': str(getattr(self.settings.processing, 'translation_mode', 'Default')),
            'style': 'Fluent and precise',
            'role_desc': 'Expert translator'
        }
        
    def execute(self) -> None:
        """æ‰§è¡Œå®Œæ•´çš„ç¿»è¯‘å·¥ä½œæµ"""
        logger.info(f"ğŸš€ å¼€å§‹å¤„ç†æ–‡æ¡£: {self.file_path.name}")
        mode_name = getattr(self.settings.processing.translation_mode_entity, 'name', 'Default') if self.settings.processing.translation_mode_entity else 'Default'
        logger.info(f"   - ç¿»è¯‘æ¨¡å¼: {mode_name}")
        logger.info(f"   - é¡¹ç›®æ ‡è¯† (Hash): {self.project_name}")
        
        try:
            # 0. å‚æ•°ä¼˜åŒ–ï¼šæ ¹æ®translator providerè°ƒæ•´batch_size
            self._optimize_batch_size_for_provider()

            # 1. åŠ è½½æ–‡æ¡£ç»“æ„
            self._load_document()
            segment_count = len(self.all_segments) if self.all_segments else 0

            # 2. åˆå§‹åŒ–ç¿»è¯‘å™¨å’Œç¼“å­˜
            self._initialize_translator()

            # 3. é¢„ç¿»è¯‘æ ‡é¢˜
            self._pre_translate_titles()

            # 4. ç”Ÿæˆæœ¯è¯­è¡¨
            self._generate_glossary()
            glossary_size = len(self.glossary) if self.glossary else 0

            # 5. åˆå§‹åŒ–æ–­ç‚¹ç»­ä¼ 
            self._initialize_checkpoint()

            # 6. æ‰§è¡Œç¿»è¯‘å¾ªç¯
            self._run_translation_loop()

            # 7. æ¸…ç†èµ„æº
            self._cleanup_resources()

            # 8. æ¸²æŸ“æœ€ç»ˆæ–‡æ¡£
            self._render_output()

        except Exception as e:
            logger.error(f"âŒ ç¿»è¯‘å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
            raise
    
    def _load_document(self) -> None:
        """
        Load é˜¶æ®µï¼šåŠ è½½æ–‡æ¡£ç»“æ„åˆ°å†…å­˜

        ä¼˜å…ˆçº§ï¼š
        1. ä» structure_map.json åŠ è½½ï¼ˆåŒ…å«ç¿»è¯‘çŠ¶æ€ï¼‰
        2. è§£æåŸå§‹æ–‡æ¡£ç”Ÿæˆæ–°ç»“æ„
        """
        logger.info("ğŸ“– åŠ è½½æ–‡æ¡£ç»“æ„...")
        
        # 1. å°è¯•ä» structure_map.json åŠ è½½
        if self.structure_path.exists() and self.settings.processing.enable_cache:
            try:
                with open(self.structure_path, 'r', encoding='utf-8') as f:
                    raw_data = json.load(f)
                    segments = [ContentSegment(**item) for item in raw_data]
                    logger.info(f"ğŸ“¦ ä»ç»“æ„æ–‡ä»¶åŠ è½½ {len(segments)} ä¸ªç‰‡æ®µ")
                    self.all_segments = segments
                    logger.info(f"âœ… å·²åŠ è½½ {len(self.all_segments)} ä¸ªå†…å®¹ç‰‡æ®µ")
                    return
            except Exception as e:
                logger.warning(f"âš ï¸ structure_map.json æŸåï¼Œå°†é‡æ–°è§£æ: {e}")
        
        # 2. é‡æ–°è§£ææ–‡æ¡£
        logger.info("âš™ï¸ è§£ææ–‡æ¡£ç»“æ„...")
        segments = parse_document(self.file_path, self.structure_path, self.settings)
        
        if segments:
            logger.info(f"âœ… è§£æå®Œæˆï¼Œç”Ÿæˆ {len(segments)} ä¸ªç‰‡æ®µ")
            self._save_structure_map(segments)
            self.all_segments = segments
        else:
            logger.error("âŒ æ–‡æ¡£è§£æå¤±è´¥")
            raise TranslationError("æ–‡æ¡£è§£æå¤±è´¥ï¼Œæœªç”Ÿæˆä»»ä½•å†…å®¹ç‰‡æ®µ")
        
        logger.info(f"âœ… å·²åŠ è½½ {len(self.all_segments)} ä¸ªå†…å®¹ç‰‡æ®µ")
    
    def _initialize_translator(self) -> None:
        """åˆå§‹åŒ–ç¿»è¯‘å™¨å’Œç¼“å­˜ç®¡ç†å™¨"""
        provider = (getattr(self.settings.api, 'translator_provider', 'gemini') or 'gemini').lower()

        if provider == 'gemini':
            # åˆ›å»ºç¼“å­˜ç®¡ç†å™¨ï¼ˆå¦‚æœå¯ç”¨ Gemini Context Cachingï¼‰
            if self.settings.processing.enable_gemini_caching:
                from ..translator.support import CachePersistenceManager
                self.cache_manager = CachePersistenceManager(self.settings)
                logger.info("âœ… Gemini ç¼“å­˜ç®¡ç†å™¨å·²åˆå§‹åŒ–")

            # GeminiTranslator currently does not accept cache_manager in constructor;
            # keep cache_manager on the workflow and pass where needed inside translator.
            self.translator = GeminiTranslator(self.settings)
            logger.info("âœ… Gemini ç¿»è¯‘å™¨å·²åˆå§‹åŒ–")
            return

        if provider in {'deepseek', 'openai', 'openai-compatible', 'openai_compatible'}:
            # OpenAI-compatible provider (DeepSeek)
            self.cache_manager = None
            self.translator = OpenAICompatibleTranslator(self.settings)
            logger.info(f"âœ… OpenAI-compatible ç¿»è¯‘å™¨å·²åˆå§‹åŒ– (provider={provider})")
            return
        
        # Ollamaå·²é›†æˆåˆ°OpenAI-compatible providerä¸­
        # é…ç½®ç¤ºä¾‹ï¼šTRANSLATOR_PROVIDER=openai-compatible, OPENAI_BASE_URL=http://localhost:11434
        
        raise TranslationError(
            f"æœªçŸ¥ translator_provider: {provider}ã€‚"
            f"æ”¯æŒçš„provider: gemini, deepseek, openai, openai-compatibleã€‚"
            f"æ³¨æ„ï¼šOllamaç°å·²é›†æˆåˆ°openai-compatibleä¸­ï¼Œè¯·ä½¿ç”¨OPENAI_BASE_URL=http://localhost:11434"
        )
    
    def _pre_translate_titles(self) -> None:
        """é¢„ç¿»è¯‘ç« èŠ‚æ ‡é¢˜"""
        logger.info("ğŸ“ å¼€å§‹é¢„ç¿»è¯‘æ ‡é¢˜...")
        
        # æå–å¾…ç¿»è¯‘æ ‡é¢˜
        raw_titles = []
        for seg in self.all_segments:
            if (seg.is_new_chapter and seg.chapter_title and
                seg.chapter_title.strip() and not is_likely_chinese(seg.chapter_title)):
                raw_titles.append(seg.chapter_title)
        
        if not raw_titles:
            logger.info("   - æ— éœ€ç¿»è¯‘çš„æ ‡é¢˜")
            return
        
        # å»é‡
        unique_titles = list(dict.fromkeys(raw_titles))
        logger.info(f"   - å‘ç° {len(unique_titles)} ä¸ªå”¯ä¸€æ ‡é¢˜")
        
        # æ„å»º translation_mode_configï¼ˆä¼˜å…ˆä½¿ç”¨å·²è®¾ç½®çš„å®ä½“ï¼‰
        translation_mode_config = None
        mode_entity = getattr(self.settings.processing, 'translation_mode_entity', None)
        if mode_entity:
            translation_mode_config = {
                'name': getattr(mode_entity, 'name', 'Auto'),
                'style': getattr(mode_entity, 'style', 'Fluent and precise'),
                'role_desc': getattr(mode_entity, 'role_desc', 'Expert translator')
            }
        else:
            translation_mode_config = {
                'name': str(getattr(self.settings.processing, 'translation_mode', 'Default')),
                'style': 'Fluent and precise',
                'role_desc': 'Expert translator'
            }

        # æ‰¹é‡ç¿»è¯‘
        translation_map = self.translator.translate_titles(unique_titles)
        
        # å›å¡«ç»“æœ
        update_count = 0
        for seg in self.all_segments:
            if seg.is_new_chapter and seg.chapter_title in translation_map:
                translated = translation_map[seg.chapter_title]
                if translated:
                    seg.chapter_title = translated
                    update_count += 1
        
        logger.info(f"   - æ›´æ–°äº† {update_count} ä¸ªæ ‡é¢˜")
        
        # ä¿å­˜æ›´æ–°åçš„ç»“æ„
        self._save_structure_map(self.all_segments)
        logger.info("âœ… æ ‡é¢˜é¢„ç¿»è¯‘å®Œæˆ")
    
    def _generate_glossary(self) -> None:
        """ç”Ÿæˆæˆ–åŠ è½½æœ¯è¯­è¡¨"""
        # å‡†å¤‡æœ¯è¯­è¡¨æ–‡ä»¶è·¯å¾„
        glossary_merged_path = self.project_dir / "glossary_merged.json"
        glossary_path = self.project_dir / "glossary.json"
        
        # ä¼˜å…ˆä½¿ç”¨åˆå¹¶çš„æœ¯è¯­è¡¨
        if glossary_merged_path.exists():
            glossary_path = glossary_merged_path
        
        # å°è¯•åŠ è½½å·²æœ‰æœ¯è¯­è¡¨
        if glossary_path.exists():
            try:
                with open(glossary_path, 'r', encoding='utf-8') as gf:
                    self.glossary = json.load(gf)
                logger.info(f"ğŸ“š ä»ç¼“å­˜åŠ è½½å·²æœ‰æœ¯è¯­è¡¨ ({len(self.glossary)} æ¡) -> {glossary_path}")
                return
            except Exception as e:
                logger.warning(f"âš ï¸ åŠ è½½å·²ä¿å­˜çš„æœ¯è¯­è¡¨å¤±è´¥ï¼Œå°†é‡æ–°ç”Ÿæˆ: {e}")
        
        # ç”Ÿæˆæ–°çš„æœ¯è¯­è¡¨ï¼ˆé€šè¿‡é¢„ç¿»è¯‘éƒ¨åˆ†æ–‡æ¡£ï¼‰
        try:
            ratio = getattr(self.settings.processing, 'glossary_preamble_ratio', 0.1)
            pre_count = max(1, int(len(self.all_segments) * float(ratio)))
        except Exception:
            pre_count = max(1, int(len(self.all_segments) * 0.1))
        
        if pre_count > 0:
            pre_segments = self.all_segments[:pre_count]
            pending_pre = [seg for seg in pre_segments if not seg.is_translated]
            
            if pending_pre:
                logger.info(f"ğŸ§ª é¢„ç¿»è¯‘å‰ {pre_count} ä¸ªç‰‡æ®µä»¥æ„å»ºæœ¯è¯­è¡¨...")
                translation_mode_config = self._build_translation_mode_config()
                # ä¸ºé¢„ç¿»è¯‘ç‰‡æ®µæä¾›ä¸Šä¸‹æ–‡ï¼ˆè™½ç„¶å¯¹æœ¯è¯­è¡¨æ„å»ºå½±å“è¾ƒå°ï¼Œä½†ä¿æŒä¸€è‡´æ€§ï¼‰
                translations = []
                batch_size = self.settings.processing.batch_size
                for i in range(0, len(pending_pre), batch_size):
                    batch = pending_pre[i:i+batch_size]
                    context = ""
                    if batch:
                        context = self._get_context_from_memory(
                            batch[0],
                            self.settings.processing.max_context_length
                        )
                    batch_results = self.translator.translate_batch(batch, context=context)
                    translations.extend(batch_results)
                
                for seg, t in zip(pending_pre, translations):
                    seg.translated_text = t
                self._save_structure_map(self.all_segments)
            
            # æå–æœ¯è¯­è¡¨ï¼ˆè‹¥ç¿»è¯‘å™¨å®ç°äº†è¯¥æ–¹æ³•ï¼‰
            if hasattr(self.translator, 'extract_glossary'):
                try:
                    self.glossary = self.translator.extract_glossary(pre_segments)
                except Exception as e:
                    logger.warning(f"âš ï¸ æå–æœ¯è¯­è¡¨å¤±è´¥: {e}")
                    self.glossary = {}
            else:
                logger.info("â„¹ï¸ ç¿»è¯‘å™¨ä¸æ”¯æŒæœ¯è¯­è¡¨æå–ï¼Œè·³è¿‡æ­¤æ­¥éª¤")
                self.glossary = {}
            
            # æŒä¹…åŒ–æœ¯è¯­è¡¨
            if self.glossary:
                try:
                    with open(glossary_path, 'w', encoding='utf-8') as gf:
                        json.dump(self.glossary, gf, ensure_ascii=False, indent=2)
                    logger.info(f"ğŸ’¾ æœ¯è¯­è¡¨å·²ä¿å­˜åˆ°: {glossary_path}")
                    logger.info(f"ğŸ”¥ å·²ç”Ÿæˆæœ¯è¯­è¡¨ï¼ŒåŒ…å« {len(self.glossary)} æ¡æœ¯è¯­")
                except Exception as e:
                    logger.warning(f"âš ï¸ ä¿å­˜æœ¯è¯­è¡¨å¤±è´¥: {e}")
            else:
                logger.info("âšª æœªç”Ÿæˆæœ‰æ•ˆæœ¯è¯­è¡¨")
    
    def _initialize_checkpoint(self) -> None:
        """åˆå§‹åŒ–æ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨"""
        self.checkpoint = CheckpointManager(self.settings)
        self.checkpoint.update_total_segments(len(self.all_segments))
        logger.info("âœ… æ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨å·²åˆå§‹åŒ–")
        logger.info(f"   ğŸ“‚ æ£€æŸ¥ç‚¹æ–‡ä»¶: {self.checkpoint.checkpoint_file}")
    
    def _run_translation_loop(self) -> None:
        """æ‰§è¡Œç¿»è¯‘å¾ªç¯ï¼ˆæ”¯æŒåŒæ­¥/å¼‚æ­¥æ¨¡å¼ï¼‰"""
        # è·å–å¾…ç¿»è¯‘ç‰‡æ®µ
        pending_segments = self.checkpoint.get_pending_segments(self.all_segments)
        
        if not pending_segments:
            logger.info("ğŸ‰ æ‰€æœ‰ç‰‡æ®µå‡å·²ç¿»è¯‘å®Œæˆï¼")
            return
        
        logger.info(f"ğŸ”„ å‘ç° {len(pending_segments)} ä¸ªå¾…ç¿»è¯‘ç‰‡æ®µ")
        
        # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨å¼‚æ­¥æ¨¡å¼
        use_async = (
            self.settings.processing.enable_async and 
            len(pending_segments) >= self.settings.processing.async_threshold and
            hasattr(self.translator, 'async_translator')
        )
        
        if use_async:
            self._run_async_translation(pending_segments)
        else:
            self._run_sync_translation(pending_segments)
    
    def _run_sync_translation(self, pending_segments: SegmentList) -> None:
        """åŒæ­¥ç¿»è¯‘æ¨¡å¼ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰"""
        logger.info("ğŸ”„ ä½¿ç”¨åŒæ­¥æ¨¡å¼ç¿»è¯‘")
        logger.info(f"ğŸ“ å¼€å§‹åŒæ­¥ç¿»è¯‘ {len(pending_segments)} ä¸ªç‰‡æ®µ...")
        
        try:
            # å°è¯•ä½¿ç”¨ rich è¿›åº¦æ¡ï¼Œå¦‚æœå¤±è´¥åˆ™å›é€€åˆ°æ— è¿›åº¦æ¡æ¨¡å¼
            use_rich = self.settings.processing.use_rich_progress
            
            if use_rich:
                try:
                    from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn
                    
                    with Progress(
                        SpinnerColumn(),
                        TextColumn("[bold blue]{task.description}"),
                        BarColumn(),
                        TaskProgressColumn(),
                        console=None,  # ä½¿ç”¨é»˜è®¤console
                    ) as progress:
                        task = progress.add_task("[cyan]åŒæ­¥ç¿»è¯‘ä¸­...", total=len(pending_segments))
                        
                        success_count = 0
                        batch_size = self.settings.processing.batch_size
                        
                        for i in range(0, len(pending_segments), batch_size):
                            batch = pending_segments[i:i+batch_size]
                            # ä¸ºå½“å‰batchçš„ç¬¬ä¸€ä¸ªsegmentè·å–ä¸Šä¸‹æ–‡
                            context = ""
                            if batch:
                                context = self._get_context_from_memory(
                                    batch[0],
                                    self.settings.processing.max_context_length
                                )
                            results = self.translator.translate_batch(batch, context=context)
                            
                            for seg, trans in zip(batch, results):
                                if trans and not trans.startswith("[Failed") and not trans.endswith("Failed]"):
                                    seg.translated_text = trans
                                    self.checkpoint.mark_segment_completed(seg.segment_id)
                                    success_count += 1
                                else:
                                    seg.translated_text = trans if trans else "[Failed: Empty response]"
                                    self.checkpoint.mark_segment_failed(seg.segment_id, trans or "Empty response")
                                
                                progress.update(task, advance=1)
                            
                            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
                            if (i // batch_size + 1) % self.settings.processing.checkpoint_interval == 0:
                                self._save_structure_map(self.all_segments)
                                self.checkpoint.save_checkpoint()
                        
                        logger.info(f"âœ… åŒæ­¥ç¿»è¯‘å®Œæˆ: {success_count}/{len(pending_segments)} æˆåŠŸ")
                        
                except ImportError:
                    logger.warning("âš ï¸ Rich åº“æœªå®‰è£…ï¼Œä½¿ç”¨ç®€å•æ¨¡å¼ï¼ˆæ— è¿›åº¦æ¡ï¼‰")
                    use_rich = False
            
            # å›é€€åˆ°ç®€å•æ¨¡å¼ï¼ˆæ— è¿›åº¦æ¡ï¼‰
            if not use_rich:
                translation_mode_config = self._build_translation_mode_config()
                # ä¸ºæ¯ä¸ªbatchåˆ†åˆ«å¤„ç†ä¸Šä¸‹æ–‡
                success_count = 0
                batch_size = self.settings.processing.batch_size

                for i in range(0, len(pending_segments), batch_size):
                    batch = pending_segments[i:i+batch_size]
                    # ä¸ºå½“å‰batchçš„ç¬¬ä¸€ä¸ªsegmentè·å–ä¸Šä¸‹æ–‡
                    context = ""
                    if batch:
                        context = self._get_context_from_memory(
                            batch[0],
                            self.settings.processing.max_context_length
                        )
                    results = self.translator.translate_batch(batch, context=context)

                    for seg, trans in zip(batch, results):
                        if trans and not trans.startswith("[Failed") and not trans.endswith("Failed]"):
                            seg.translated_text = trans
                            self.checkpoint.mark_segment_completed(seg.segment_id)
                            success_count += 1
                        else:
                            seg.translated_text = trans if trans else "[Failed: Empty response]"
                            self.checkpoint.mark_segment_failed(seg.segment_id, trans or "Empty response")

                    # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
                    if (i // batch_size + 1) % self.settings.processing.checkpoint_interval == 0:
                        self._save_structure_map(self.all_segments)
                        self.checkpoint.save_checkpoint()
                
                logger.info(f"âœ… åŒæ­¥ç¿»è¯‘å®Œæˆ: {success_count}/{len(pending_segments)} æˆåŠŸ")
            
            # æœ€ç»ˆä¿å­˜
            self._save_structure_map(self.all_segments)
            self.checkpoint.save_checkpoint()
            
        except Exception as e:
            logger.error(f"âŒ åŒæ­¥ç¿»è¯‘å¤±è´¥: {e}")
            for seg in pending_segments:
                seg.translated_text = f"[Failed: {str(e)}]"
                self.checkpoint.mark_segment_failed(seg.segment_id, str(e))
            self._save_structure_map(self.all_segments)
            self.checkpoint.save_checkpoint()
            raise
    
    def _run_async_translation(self, pending_segments: SegmentList) -> None:
        """å¼‚æ­¥ç¿»è¯‘æ¨¡å¼ï¼ˆä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨è‡ªåŠ¨èµ„æºæ¸…ç†ï¼‰"""
        logger.info("âš¡ ä½¿ç”¨å¼‚æ­¥æ¨¡å¼ç¿»è¯‘ï¼ˆæå‡é€Ÿåº¦ï¼‰")
        
        # æ£€æŸ¥translatoræ˜¯å¦æ”¯æŒå¼‚æ­¥
        if not hasattr(self.translator, 'async_translator') or self.translator.async_translator is None:
            logger.warning("âš ï¸ å½“å‰translatorä¸æ”¯æŒå¼‚æ­¥æ¨¡å¼ï¼Œé™çº§åˆ°åŒæ­¥æ¨¡å¼")
            self._run_sync_translation(pending_segments)
            return
        
        try:
            batch_size = self.settings.processing.batch_size
            batches = [
                pending_segments[i:i+batch_size] 
                for i in range(0, len(pending_segments), batch_size)
            ]
            total_batches = len(batches)
            
            logger.info(f"ğŸš€ å¼€å§‹å¼‚æ­¥ç¿»è¯‘ {len(pending_segments)} ä¸ªç‰‡æ®µï¼ˆ{total_batches} æ‰¹æ¬¡ï¼Œæ‰¹å¤§å° {batch_size}ï¼‰...")
            
            async def translate_all_batches_with_progress():
                """æ‰€æœ‰æ‰¹æ¬¡å¹¶å‘æ‰§è¡Œï¼Œå¸¦è¿›åº¦æ˜¾ç¤ºå’Œä¸Šä¸‹æ–‡ç®¡ç†å™¨è‡ªåŠ¨èµ„æºæ¸…ç†"""
                # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¡®ä¿èµ„æºè‡ªåŠ¨æ¸…ç†
                async with self.translator.async_translator as async_t:
                    # åˆ›å»ºä»»åŠ¡å’Œç´¢å¼•æ˜ å°„
                    task_to_index = {}
                    tasks = []
                    
                    for i, batch in enumerate(batches):
                        # ä¸ºå½“å‰batchçš„ç¬¬ä¸€ä¸ªsegmentè·å–ä¸Šä¸‹æ–‡
                        context = ""
                        if batch:
                            context = self._get_context_from_memory(
                                batch[0],
                                self.settings.processing.max_context_length
                            )
                        coro = async_t.translate_text_batch_async(
                            batch, context, self.glossary
                        )
                        task = asyncio.create_task(coro)
                        tasks.append(task)
                        task_to_index[task] = i
                    
                    # ä½¿ç”¨Richè¿›åº¦æ¡
                    if RICH_AVAILABLE:
                        console = Console()
                        with Progress(
                            SpinnerColumn(),
                            TextColumn("[progress.description]{task.description}"),
                            BarColumn(),
                            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                            TextColumn("â€¢"),
                            TextColumn("{task.completed}/{task.total} æ‰¹æ¬¡"),
                            TimeElapsedColumn(),
                            console=console
                        ) as progress:
                            task_id = progress.add_task("[cyan]å¼‚æ­¥ç¿»è¯‘è¿›åº¦", total=total_batches)
                            
                            # æ”¶é›†ç»“æœï¼Œä¿æŒé¡ºåº
                            results = [None] * total_batches
                            completed_count = 0
                            
                            # ä½¿ç”¨ wait æ¥é€æ­¥è·å–ç»“æœå¹¶æ›´æ–°è¿›åº¦
                            pending = set(tasks)
                            
                            while pending:
                                # ç­‰å¾…è‡³å°‘ä¸€ä¸ªä»»åŠ¡å®Œæˆ
                                done, pending = await asyncio.wait(pending, return_when=asyncio.FIRST_COMPLETED)

                                # å¤„ç†å·²å®Œæˆçš„ä»»åŠ¡ï¼ˆæ•è·å•ä¸ªä»»åŠ¡å¼‚å¸¸ï¼Œé¿å…æ•´ä¸ªå¹¶å‘æµç¨‹ä¸­æ–­ï¼‰
                                for completed_task in done:
                                    task_index = task_to_index.get(completed_task, None)
                                    try:
                                        result = completed_task.result()
                                    except Exception as task_exc:
                                        logger.error(f"âŒ å¼‚æ­¥æ‰¹æ¬¡ä»»åŠ¡å¤±è´¥: {task_exc}")
                                        # æ ‡è®°è¯¥æ‰¹æ¬¡ä¸ºå¤±è´¥ï¼ˆç”¨ä¸åŒæ­¥è·¯å¾„å…¼å®¹çš„å ä½ç¬¦ï¼‰
                                        result = [f"[Failed: {str(task_exc)}]"] * (len(batches[task_index]) if task_index is not None else 1)

                                    # é€šè¿‡ä»»åŠ¡å¯¹è±¡æ‰¾åˆ°å¯¹åº”çš„ç´¢å¼•å¹¶å­˜å‚¨ç»“æœ
                                    if task_index is not None:
                                        results[task_index] = result
                                    completed_count += 1
                                    # é€æ­¥æ›´æ–°è¿›åº¦æ¡
                                    progress.update(task_id, completed=completed_count)
                            
                            return results
                    else:
                        # æ— è¿›åº¦æ¡æ¨¡å¼ï¼šä½¿ç”¨gatherä¿æŒé¡ºåºï¼ˆreturn_exceptions=Trueé¿å…å•ä»»åŠ¡å¼‚å¸¸ä¸­æ–­æ•´ä½“ï¼‰
                        raw_results = await asyncio.gather(*tasks, return_exceptions=True)
                        # ç»Ÿä¸€å¤„ç†å¼‚å¸¸ï¼Œè½¬æ¢ä¸ºå¤±è´¥å ä½ç¬¦
                        results = []
                        for i, res in enumerate(raw_results):
                            if isinstance(res, Exception):
                                logger.error(f"âŒ å¼‚æ­¥æ‰¹æ¬¡ {i} ä»»åŠ¡å¤±è´¥: {res}")
                                results.append([f"[Failed: {str(res)}]"] * len(batches[i]))
                            else:
                                results.append(res)
                        return results
            
            # æ‰§è¡Œå¹¶å‘ç¿»è¯‘ï¼ˆæ£€æŸ¥äº‹ä»¶å¾ªç¯å…¼å®¹æ€§ï¼‰
            try:
                # æ£€æŸ¥æ˜¯å¦å·²åœ¨äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œ
                loop = asyncio.get_running_loop()
                # å¦‚æœå·²æœ‰è¿è¡Œä¸­çš„å¾ªç¯ï¼Œä¸èƒ½ç”¨asyncio.runï¼Œéœ€è¦ç›´æ¥awaitï¼ˆä½†è¿™é‡Œæ˜¯åŒæ­¥å‡½æ•°ï¼Œè®°å½•è­¦å‘Šï¼‰
                logger.warning("âš ï¸ æ£€æµ‹åˆ°å·²è¿è¡Œçš„äº‹ä»¶å¾ªç¯ï¼Œå¼‚æ­¥ç¿»è¯‘å¯èƒ½å—é™ã€‚å»ºè®®åœ¨ç‹¬ç«‹ç¯å¢ƒè¿è¡Œã€‚")
                # é™çº§åˆ°åŒæ­¥æ¨¡å¼
                raise RuntimeError("å·²å­˜åœ¨è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œæ— æ³•ä½¿ç”¨asyncio.run")
            except RuntimeError:
                # æ²¡æœ‰è¿è¡Œä¸­çš„å¾ªç¯ï¼ˆè¿™æ˜¯æ­£å¸¸æƒ…å†µï¼‰ï¼Œå¯ä»¥å®‰å…¨ä½¿ç”¨asyncio.run
                all_results = asyncio.run(translate_all_batches_with_progress())

            # å¤„ç†ç¿»è¯‘ç»“æœå¹¶å®æ—¶ä¿å­˜checkpoint
            success_count = 0
            batch_idx = 0
            for batch, batch_results in zip(batches, all_results):
                # batch_results æœ‰å¯èƒ½æ˜¯å¼‚å¸¸å ä½ï¼Œç¡®ä¿å¯è¿­ä»£
                if not isinstance(batch_results, (list, tuple)):
                    batch_results = [batch_results] * len(batch)

                for seg, trans in zip(batch, batch_results):
                    try:
                        if trans and not (isinstance(trans, str) and (trans.startswith("[Failed") or trans.endswith("Failed]"))):
                            seg.translated_text = trans
                            self.checkpoint.mark_segment_completed(seg.segment_id)
                            success_count += 1
                        else:
                            seg.translated_text = trans if trans else "[Failed: Empty response]"
                            self.checkpoint.mark_segment_failed(seg.segment_id, trans or "Empty response")
                    except Exception as proc_exc:
                        logger.error(f"âŒ å¤„ç†å¼‚æ­¥ç¿»è¯‘ç»“æœæ—¶å‡ºé”™: {proc_exc}")
                        seg.translated_text = f"[Failed: {proc_exc}]"
                        self.checkpoint.mark_segment_failed(seg.segment_id, str(proc_exc))

                # æ¯ä¸ªbatchå®Œæˆåç«‹å³ä¿å­˜checkpointï¼ˆé˜²æ­¢æ•°æ®ä¸¢å¤±ï¼‰
                batch_idx += 1
                try:
                    self._save_structure_map(self.all_segments)
                    self.checkpoint.save_checkpoint()
                    logger.debug(f"ğŸ’¾ å·²ä¿å­˜æ‰¹æ¬¡ {batch_idx}/{total_batches} çš„checkpoint")
                except Exception as save_exc:
                    logger.error(f"âŒ ä¿å­˜æ‰¹æ¬¡ {batch_idx} checkpointå¤±è´¥: {save_exc}")
                    # ä¸ä¸­æ–­ç¿»è¯‘æµç¨‹ï¼Œç»§ç»­å¤„ç†åç»­æ‰¹æ¬¡

            logger.info(f"âœ… å¼‚æ­¥ç¿»è¯‘å®Œæˆ: {success_count}/{len(pending_segments)} æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ å¼‚æ­¥ç¿»è¯‘å¤±è´¥ï¼Œé™çº§åˆ°åŒæ­¥æ¨¡å¼: {e}")
            # é™çº§å‰å…ˆä¿å­˜å½“å‰è¿›åº¦
            try:
                self._save_structure_map(self.all_segments)
                self.checkpoint.save_checkpoint()
                logger.info("ğŸ’¾ å·²ä¿å­˜å¼‚æ­¥ç¿»è¯‘ä¸­æ–­å‰çš„è¿›åº¦")
            except Exception as save_exc:
                logger.error(f"âŒ ä¿å­˜ä¸­æ–­è¿›åº¦å¤±è´¥: {save_exc}")
            # é™çº§åˆ°åŒæ­¥æ¨¡å¼
            self._run_sync_translation(pending_segments)
        finally:
            # æœ€ç»ˆä¿è¯ï¼šç¡®ä¿åœ¨ä»»ä½•æƒ…å†µä¸‹éƒ½å°è¯•ä¿å­˜å½“å‰ç»“æ„ä¸æ£€æŸ¥ç‚¹
            try:
                self._save_structure_map(self.all_segments)
                if self.checkpoint:
                    self.checkpoint.save_checkpoint()
                logger.debug("âœ… å¼‚æ­¥ç¿»è¯‘finallyå—ï¼šå·²ä¿å­˜æœ€ç»ˆcheckpoint")
            except Exception as final_exc:
                logger.warning(f"âš ï¸ æœ€ç»ˆä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {final_exc}")
    
    def _cleanup_resources(self) -> None:
        """æ¸…ç†èµ„æº"""
        try:
            if hasattr(self.translator, '_async_translator') and self.translator._async_translator:
                self.translator._async_translator.cleanup()
            if hasattr(self.translator, 'cache_manager') and self.translator.cache_manager:
                self.translator.cache_manager.cleanup_all_caches()
            logger.info("âœ… èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            logger.debug(f"æ¸…ç†èµ„æºæ—¶å‡ºç°è­¦å‘Š: {e}")
    
    def _render_output(self) -> None:
        """Render: ç”Ÿæˆæœ€ç»ˆæ–‡æ¡£ï¼ˆMarkdown + PDFï¼‰"""
        logger.info("ğŸ“„ å¼€å§‹æ¸²æŸ“æœ€ç»ˆæ–‡æ¡£...")
        
        # å†³å®šæœ€ç»ˆè¾“å‡ºç›®å½•
        if self.settings.files.final_output_dir:
            final_dir = self.settings.files.final_output_dir
            final_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"   - è‡ªå®šä¹‰è¾“å‡ºç›®å½•: {final_dir}")
        else:
            # é»˜è®¤è¾“å‡ºåˆ°æºæ–‡ä»¶æ‰€åœ¨ç›®å½•
            final_dir = self.settings.files.document_path.parent
            logger.info(f"   - è¾“å‡ºåˆ°æºæ–‡ä»¶ç›®å½•: {final_dir}")
        
        # 1. ç”Ÿæˆ Markdown
        md_renderer = MarkdownRenderer(self.settings)
        md_output_path = final_dir / f"{Path(self.file_path.name).stem}_Translated.md"
        md_renderer.render_to_file(self.all_segments, md_output_path, f"åŸæ–‡: {self.file_path.name}")
        logger.info(f"âœ… Markdown å·²ä¿å­˜åˆ°: {md_output_path}")
        
        # 2. ç”Ÿæˆ PDFï¼ˆå¯é€‰ï¼Œå¦‚æœä¾èµ–å¯ç”¨ï¼‰
        try:
            from ..renderer.pdf import PDFRenderer
            pdf_renderer = PDFRenderer(self.settings)
            
            pdf_path = final_dir / f"{Path(self.file_path.name).stem}_Translated.pdf"
            pdf_renderer.render_to_file(self.all_segments, pdf_path, f"åŸæ–‡: {self.file_path.name}")
            logger.info(f"âœ… PDF å·²ä¿å­˜åˆ°: {pdf_path}")
        except ImportError:
            logger.info("â„¹ï¸  è·³è¿‡ PDF ç”Ÿæˆï¼ˆæœªå®‰è£…ç›¸å…³ä¾èµ–ï¼‰")
        except Exception as e:
            logger.warning(f"âš ï¸  PDF ç”Ÿæˆå¤±è´¥: {e}")
            logger.info("ğŸ’¡ å·²ç”Ÿæˆ Markdown æ–‡ä»¶ï¼Œå¯æ‰‹åŠ¨è½¬æ¢ä¸º PDF")
        
        logger.info("âœ… æ–‡æ¡£æ¸²æŸ“å®Œæˆ")
    
    def _save_structure_map(self, segments: SegmentList) -> None:
        """
        Save: ä¿å­˜å®Œæ•´çš„æ–‡æ¡£ç»“æ„çŠ¶æ€åˆ° JSON æ–‡ä»¶
        è¿™æ˜¯å•ä¸€çœŸç†æºçš„æŒä¹…åŒ–
        """
        try:
            self.structure_path.parent.mkdir(parents=True, exist_ok=True)
            
            # åºåˆ—åŒ–ä¸ºå­—å…¸åˆ—è¡¨
            data = [seg.model_dump() for seg in segments]
            
            with open(self.structure_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            logger.debug(f"ğŸ’¾ ç»“æ„çŠ¶æ€å·²ä¿å­˜: {len(segments)} ä¸ªç‰‡æ®µ")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç»“æ„çŠ¶æ€å¤±è´¥: {e}")
            raise
    
    def _get_context_from_memory(self, current_segment: ContentSegment, max_length: int) -> str:
        """
        ä»å†…å­˜ä¸­è·å–ç¿»è¯‘ä¸Šä¸‹æ–‡
        ä»ä¸Šä¸€ä¸ªsegmentçš„åŸæ–‡ä¸­é€‰å–25%å·¦å³çš„MAX_CHUNK_SIZEä½œä¸ºä¸Šä¸‹æ–‡
        """
        # æ‰¾åˆ°å½“å‰ç‰‡æ®µçš„ä½ç½®
        current_idx = next((i for i, seg in enumerate(self.all_segments) if seg.segment_id == current_segment.segment_id), -1)
        if current_idx == -1:
            return ""

        # è·å–ä¸Šä¸€ä¸ªç‰‡æ®µçš„åŸæ–‡ä½œä¸ºä¸Šä¸‹æ–‡
        if current_idx > 0:
            prev_seg = self.all_segments[current_idx - 1]
            if prev_seg.original_text and prev_seg.original_text.strip():
                # è®¡ç®—ä¸Šä¸‹æ–‡é•¿åº¦ï¼š25% çš„ MAX_CHUNK_SIZE
                context_length = int(self.settings.processing.max_chunk_size * 0.25)

                # å¦‚æœåŸæ–‡é•¿åº¦è¶…è¿‡ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ï¼Œå–å25%çš„å†…å®¹
                original_text = prev_seg.original_text.strip()
                if len(original_text) > context_length:
                    # ä»åŸæ–‡æœ«å°¾å‘å‰å–æŒ‡å®šé•¿åº¦
                    context_text = original_text[-context_length:].strip()
                else:
                    context_text = original_text

                # ç¡®ä¿ä¸è¶…è¿‡max_lengthå‚æ•°
                if len(context_text) > max_length:
                    context_text = context_text[-max_length:].strip()

                return context_text

        return ""
