"""
æ–‡æ¡£è§£æå™¨é›†åˆ
åŒ…å«æ‰€æœ‰æ–‡æ¡£æ ¼å¼çš„è§£æå™¨å®ç°ï¼šBaseDocPipeline, PDFParser, EPUBParser
"""
import os
import csv
import json
import fitz  # PyMuPDF
import ebooklib
from abc import ABC, abstractmethod
from pathlib import Path
from typing import List, Dict, Any, Iterator, Tuple

from ebooklib import epub
from bs4 import BeautifulSoup

from ..core.schema import ContentSegment, Settings
from ..core.exceptions import DocumentParseError
from .helpers import process_unified_toc, extract_text_from_html
from ..utils.logger import get_logger

logger = get_logger(__name__)


# ============================================================================
# åŸºç¡€æŠ½è±¡ç±»
# ============================================================================

class BaseDocPipeline(ABC):
    """
    æ–‡æ¡£å¤„ç†æµæ°´çº¿çš„æŠ½è±¡åŸºç±»ã€‚
    è´Ÿè´£å°†æ–‡æ¡£æµè½¬æ¢ä¸º List[ContentSegment] å¯¹è±¡æµã€‚
    """
    def __init__(self, file_path: Path, cache_path: Path, settings: Settings):
        self.file_path = file_path
        self.cache_path = cache_path
        self.settings = settings

        # ç»“æœå®¹å™¨
        self.all_segments: List[ContentSegment] = []
        self.global_id_counter: int = 0

        # æ–‡æœ¬ç¼“å†²åŒº
        self.rolling_buffer: List[str] = []
        self.current_buffer_length: int = 0

        # ä¸Šä¸‹æ–‡çŠ¶æ€
        self.current_chapter_title: str = "å‰è¨€/æœªå‘½åç« èŠ‚"
        self.current_page_index: int = 0
        self.pending_new_chapter: bool = False
        self.current_toc_level: int = 1
        # ç« èŠ‚æ˜ å°„è¡¨ {UnitKey: ChapterTitle}
        self.chapter_map: Dict[Any, Dict[str, Any]] = {}

    def run(self) -> List[ContentSegment]:
        """ä¸»æµç¨‹ï¼šè¿­ä»£å•å…ƒ -> ç»´æŠ¤çŠ¶æ€ -> ç”Ÿæˆå¯¹è±¡"""
        logger.info(f"Starting pipeline '{self.__class__.__name__}' for {self.file_path.name}")

        self._load_metadata()

        # éå†å†…å®¹å•å…ƒ (UnitKey é€šå¸¸æ˜¯ é¡µç  æˆ– æ–‡ä»¶å)
        for unit_key, content, content_type in self._iter_content_units():

            # A. è§†è§‰/å›¾ç‰‡æ¨¡å¼å¤„ç†
            if content_type == "image":
                self._flush_buffer()

                seg = ContentSegment(
                    segment_id=self.global_id_counter,
                    original_text="",
                    content_type="image",
                    image_path=content,
                    page_index=unit_key if isinstance(unit_key, int) else 0,
                    chapter_title=self.current_chapter_title,
                    toc_level=self.current_toc_level,
                    is_new_chapter=False
                )
                self.all_segments.append(seg)
                self.global_id_counter += 1
                continue

            # B. çº¯æ–‡æœ¬æ¨¡å¼å¤„ç†
            if not content or not content.strip():
                continue

            # 1. æ£€æŸ¥ç« èŠ‚å˜æ›´
            chap_info = self.chapter_map.get(unit_key)

            if chap_info:
                new_title = chap_info.get("title", "Untitled")
                new_level = chap_info.get("level", 1)

                if new_title != self.current_chapter_title:
                    self._flush_buffer()
                    self.current_chapter_title = new_title
                    self.current_toc_level = new_level
                    logger.debug(f"New chapter detected: {new_title}")
                    self.pending_new_chapter = True

            # 2. æ›´æ–°å½“å‰é¡µç  (é’ˆå¯¹ PDF)
            if isinstance(unit_key, int):
                self.current_page_index = unit_key

            # 3. ç´¯ç§¯æ–‡æœ¬
            self.rolling_buffer.append(content)
            self.current_buffer_length += len(content)

            # 4. æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†å—
            if self.current_buffer_length >= self.settings.processing.max_chunk_size:
                self._flush_buffer()

        # å¤„ç†å‰©ä½™å†…å®¹
        self._flush_buffer()

        self._save_cache()
        logger.info(f"Pipeline finished. Generated {len(self.all_segments)} segments.")
        return self.all_segments

    def _flush_buffer(self):
        """å°†å½“å‰ç¼“å†²åŒºæ‰“åŒ…æˆä¸€ä¸ª Segment"""
        if not self.rolling_buffer:
            return

        full_text = "\n\n".join(self.rolling_buffer)

        seg = ContentSegment(
            segment_id=self.global_id_counter,
            original_text=full_text,
            content_type="text",
            chapter_title=self.current_chapter_title,
            toc_level=self.current_toc_level,
            is_new_chapter=self.pending_new_chapter,
            page_index=self.current_page_index
        )

        self.all_segments.append(seg)
        self.global_id_counter += 1

        # é‡ç½®çŠ¶æ€
        self.rolling_buffer = []
        self.current_buffer_length = 0
        self.pending_new_chapter = False

    def _save_cache(self):
        """ä¿å­˜ä¸º JSON"""
        try:
            data = [seg.model_dump() for seg in self.all_segments]
            with open(self.cache_path, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.warning(f"Failed to save cache: {e}")

    @abstractmethod
    def _load_metadata(self):
        pass

    @abstractmethod
    def _iter_content_units(self) -> Iterator[Tuple[Any, str, str]]:
        """
        Yields: (unit_key, content, content_type)
        content_type: 'text' | 'image'
        """
        pass


# ============================================================================
# PDF è§£æå™¨
# ============================================================================

class PDFParser(BaseDocPipeline):
    """PDF æ–‡æ¡£è§£æå™¨"""

    def __init__(self, file_path: Path, cache_path: Path, settings: Settings):
        super().__init__(file_path, cache_path, settings)
        self.doc: fitz.Document = None

    def _load_metadata(self):
        """
        åŠ è½½å…ƒæ•°æ®å¹¶é€‚é… process_unified_toc æ¶æ„ã€‚
        æ”¯æŒï¼šCSV è‡ªå®šä¹‰ -> PDF åŸç”Ÿ TOC -> çº¯é¡µç å›é€€ã€‚
        """
        self.doc = fitz.open(str(self.file_path))

        # 1. å®šä¹‰ä¸­é—´å±‚ï¼šæ ‡å‡†ä¸‰å…ƒç»„åˆ—è¡¨
        # æ¯ä¸€é¡¹ç»“æ„: {'level': int, 'title': str, 'key': int}
        standardized_items = []

        # =========================================================
        # åˆ†æ”¯ A: å°è¯•åŠ è½½ CSV è‡ªå®šä¹‰ç›®å½• (ä¼˜å…ˆçº§æœ€é«˜)
        # =========================================================
        # ä¿®æ­£: ä» settings.document è¯»å–æœ€ç»ˆç”Ÿæ•ˆçš„ TOC è·¯å¾„
        if self.settings.document.custom_toc_path and self.settings.document.custom_toc_path.exists():
            logger.info(f"Loading custom TOC from CSV: {self.settings.document.custom_toc_path}")
            try:
                # utf-8-sig å…¼å®¹ Excel ä¿å­˜çš„ CSV
                with open(self.settings.document.custom_toc_path, 'r', encoding='utf-8-sig') as f:
                    reader = csv.DictReader(f)
                    for row in reader:
                        # å¥å£®æ€§è¯»å–ï¼šå¤„ç† CSV åˆ—åå¤§å°å†™æˆ–ç©ºæ ¼
                        # å‡è®¾æ ‡å‡†åˆ—å: Page, Title, Level (å¯é€‰)
                        row_lower = {k.lower().strip(): v for k, v in row.items()}

                        page_str = row_lower.get('page') or row_lower.get('é¡µç ')
                        if not page_str: continue

                        p_idx = int(page_str) - 1 # ç”¨æˆ·ä¹ æƒ¯ 1-based, å†…éƒ¨é€»è¾‘ 0-based

                        title = row_lower.get('title') or row_lower.get('æ ‡é¢˜') or f"Page {p_idx+1}"
                        level_str = row_lower.get('level') or row_lower.get('å±‚çº§') or "1"

                        if p_idx >= 0:
                            standardized_items.append({
                                'level': int(level_str),
                                'title': title.strip(),
                                'key': p_idx
                            })
            except Exception as e:
                logger.error(f"Failed to parse CSV TOC: {e}. Falling back to native TOC.")
                standardized_items = [] # è§£æå¤±è´¥ï¼Œæ¸…ç©ºä»¥è§¦å‘å›é€€

        # =========================================================
        # åˆ†æ”¯ B: å°è¯•åŠ è½½ PDF åŸç”Ÿ TOC (å¦‚æœ CSV ä¸ºç©º)
        # =========================================================
        if not standardized_items:
            # fitz.get_toc() è¿”å›: [[lvl, title, page, ...], ...]
            raw_toc = self.doc.get_toc()
            if raw_toc:
                logger.info("Loading native PDF TOC.")
                for item in raw_toc:
                    lvl = item[0]
                    title = item[1]
                    page_num = item[2]

                    p_idx = page_num - 1
                    if p_idx >= 0:
                        standardized_items.append({
                            'level': lvl,
                            'title': title,
                            'key': p_idx
                        })

        # =========================================================
        # åˆ†æ”¯ C: çº¯é¡µç å›é€€æ¨¡å¼ (å¦‚æœä»¥ä¸Šéƒ½ä¸ºç©º)
        # =========================================================
        is_fallback_mode = False
        if not standardized_items:
            logger.info("No TOC found. Falling back to Page-as-Chapter mode.")
            is_fallback_mode = True
            # æ¯ä¸€é¡µéƒ½ä½œä¸ºä¸€ä¸ª Level 1 çš„ç« èŠ‚
            for i in range(len(self.doc)):
                standardized_items.append({
                    'level': 1,
                    'title': f"Page {i+1}",
                    'key': i
                })

        # =========================================================
        # 2. ç»Ÿä¸€è°ƒç”¨æ ¸å¿ƒç­–ç•¥
        # =========================================================

        # è·å–é¢åŒ…å±‘å¼€å…³ (é»˜è®¤å¼€å¯)
        use_bc = self.settings.processing.use_breadcrumb

        # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæ˜¯çº¯é¡µç å›é€€æ¨¡å¼ï¼Œå¼ºåˆ¶å…³é—­é¢åŒ…å±‘
        # å¦åˆ™ä¼šå˜æˆ "Page 1 > Page 2 > Page 3..." è¿™ç§è’è°¬çš„å±‚çº§
        if is_fallback_mode:
            use_bc = False

        # è°ƒç”¨ process_unified_toc ç”Ÿæˆæœ€ç»ˆ Map
        # ç»“æœæ ¼å¼: { 0: {"title": "...", "level": 1}, 5: {"title": "...", "level": 2} }
        self.chapter_map = process_unified_toc(standardized_items, use_breadcrumb=use_bc)

        # (å¯é€‰) ä¿å­˜ raw items ä¾› process_flow è¿›è¡Œé¢„ç¿»è¯‘ä½¿ç”¨
        self.raw_toc_entries = standardized_items

        logger.info(f"Metadata loaded. Chapter Map contains {len(self.chapter_map)} entries.")

    def _iter_content_units(self) -> Iterator[Tuple[int, str, str]]:
        """
        æ ¹æ®æ¨¡å¼ Yield å†…å®¹ã€‚
        Vision æ¨¡å¼ -> type='image', content=path
        Text æ¨¡å¼ -> type='text', content=string
        """
        use_vision = self.settings.processing.use_vision_mode
        # --- Text æ¨¡å¼ ---
        # æ ¹æ® settings.document.page_range è¿›è¡Œé¡µé¢åˆ‡å‰²
        actual_start_page = 0
        actual_end_page = len(self.doc) # æ€»é¡µæ•°

        if self.settings.document.page_range:
            user_start, user_end = self.settings.document.page_range
            
            # å°†ç”¨æˆ·è¾“å…¥çš„ 1-based è½¬æ¢ä¸º 0-based ç´¢å¼•
            potential_start_idx = user_start - 1
            potential_end_idx = user_end # range æ˜¯ exclusive çš„ï¼Œæ‰€ä»¥ç›´æ¥ç”¨ user_end
            
            # ç¡®ä¿èŒƒå›´ä¸è¶…å‡ºæ–‡æ¡£å®é™…é¡µæ•°
            actual_start_page = max(0, potential_start_idx)
            actual_end_page = min(len(self.doc), potential_end_idx)

            logger.info(f"ğŸ“„ é¡µé¢èŒƒå›´åˆ‡å‰²: ç”¨æˆ·è¯·æ±‚ {user_start}-{user_end}ï¼Œå®é™…å¤„ç† {actual_start_page + 1}-{actual_end_page}")
            if actual_start_page >= actual_end_page:
                logger.warning(f"âš ï¸ è®¾å®šçš„é¡µé¢èŒƒå›´ {user_start}-{user_end} æ— æ•ˆæˆ–è¶…å‡ºæ–‡æ¡£èŒƒå›´ï¼Œå°†è·³è¿‡é¡µé¢è§£æã€‚")
                return # èŒƒå›´æ— æ•ˆï¼Œä¸ç”Ÿæˆä»»ä½•å†…å®¹

        for i in range(actual_start_page, actual_end_page):
            page = self.doc[i]

            if use_vision:
                # --- Vision æ¨¡å¼ ---
                img_path = self._save_page_image(page, i)
                if img_path:
                    # yield (é¡µç , å›¾ç‰‡è·¯å¾„, ç±»å‹)
                    yield i, img_path, "image"
            else:
                # --- Text æ¨¡å¼ ---
                text = self._extract_text(page, i)
                # yield (é¡µç , æ–‡æœ¬å†…å®¹, ç±»å‹)
                yield i, text, "text"

    def _save_page_image(self, page, page_idx) -> str:
        """
        ä»¥ 200 DPI æ¸²æŸ“è£åˆ‡åçš„é¡µé¢å›¾ç‰‡ã€‚
        ç›´æ¥åœ¨æ¸²æŸ“é˜¶æ®µæ ¹æ® Settings é‡Œçš„ Margin å‚æ•°ç§»é™¤é¡µçœ‰é¡µè„šã€‚
        """
        try:
            # 1. å‡†å¤‡ç›®å½•
            project_dir = self.cache_path.parent

            # åœ¨é¡¹ç›®ç›®å½•ä¸‹åˆ›å»º images æ–‡ä»¶å¤¹
            image_dir = project_dir / "images"
            image_dir.mkdir(parents=True, exist_ok=True)

            # 2. ç”Ÿæˆæ–‡ä»¶å
            filename = f"page_{page_idx + 1:04d}.jpg"
            full_path = image_dir / filename

            if full_path.exists():
                return str(full_path.resolve())

            # 4. è®¡ç®—è£åˆ‡åŒºåŸŸ
            clip_rect = self._get_crop_rect(page)

            # 5. æ¸²æŸ“å›¾ç‰‡
            zoom = 200 / 72  # 200 DPI
            mat = fitz.Matrix(zoom, zoom)
            pix = page.get_pixmap(matrix=mat, clip=clip_rect, alpha=False)

            # 6. ä¿å­˜
            pix.save(str(full_path))
            return str(full_path.resolve())

        except Exception as e:
            logger.error(f"Failed to render image for page {page_idx}: {e}")
            return ""

    def _extract_text(self, page, page_idx) -> str:
        """
        æå–é¡µé¢æ–‡æœ¬ï¼Œæ ¹æ® settings é‡Œçš„ç™¾åˆ†æ¯”å‚æ•°è¿›è¡Œè£åˆ‡ (Clip)ã€‚
        """
        try:
            # è®¡ç®—è£åˆ‡åŒºåŸŸ
            clip_rect = self._get_crop_rect(page)

            # æå–æ–‡æœ¬
            text = page.get_text("text", clip=clip_rect, sort=True)
            return text.strip()

        except Exception as e:
            logger.error(f"Failed to extract text from page {page_idx}: {e}")
            return ""

    def _get_crop_rect(self, page: fitz.Page) -> fitz.Rect:
        """è®¡ç®—è£åˆ‡çŸ©å½¢"""
        w = page.rect.width
        h = page.rect.height

        # è·å–è¾¹è·è®¾ç½®
        m_top = self.settings.document.margin_top or 0.0
        m_bottom = self.settings.document.margin_bottom or 0.0
        m_left = self.settings.document.margin_left or 0.0
        m_right = self.settings.document.margin_right or 0.0

        # è®¡ç®—åæ ‡
        x0 = w * m_left
        y0 = h * m_top
        x1 = w * (1.0 - m_right)
        y1 = h * (1.0 - m_bottom)

        # å®‰å…¨æ£€æŸ¥
        if x0 >= x1 or y0 >= y1:
            return page.rect  # è¿”å›å…¨é¡µ

        return fitz.Rect(x0, y0, x1, y1)


# ============================================================================
# EPUB è§£æå™¨
# ============================================================================

class EPUBParser(BaseDocPipeline):
    """EPUB æ–‡æ¡£è§£æå™¨"""

    def __init__(self, file_path: Path, cache_path: Path, settings: Settings):
        super().__init__(file_path, cache_path, settings)
        self.book: epub.EpubBook = None

    def _load_metadata(self):
        """è§£æ EPUB å…ƒæ•°æ®å’Œç›®å½•ç»“æ„"""
        logger.info("Parsing EPUB metadata.")
        # 1. è¯»å– EPUB
        self.book = epub.read_epub(str(self.file_path))

        # 2. å°è¯•ä» NCX/NAV è·å–ç›®å½• (Flatten)
        standardized_items = self._flatten_epub_to_standard(self.book.toc)

        # 3. å…œåº•é€»è¾‘ï¼šå¦‚æœç›®å½•ä¸ºç©ºï¼Œä½¿ç”¨ Spine
        if not standardized_items:
            logger.warning("âš ï¸ EPUB TOC is empty. Falling back to Spine (linear reading order).")

            for item_id, linear in self.book.spine:
                item = self.book.get_item_with_id(item_id)

                if item:
                    if item.get_type() != ebooklib.ITEM_DOCUMENT:
                        continue

                    if 'nav' in (item.get_name() or "").lower():
                        continue

                    file_name = item.get_name()
                    standardized_items.append({
                        'level': 1,
                        'title': f"Section: {file_name}",
                        'key': file_name
                    })

        # 4. ç»Ÿä¸€å¤„ç†
        use_bc = self.settings.processing.use_breadcrumb
        self.chapter_map = process_unified_toc(standardized_items, use_breadcrumb=use_bc)
        logger.info(f"âœ… Metadata loaded. Chapter Map size: {len(self.chapter_map)}")

    def _iter_content_units(self):
        """æŒ‰ç…§ EPUB Spine éå†ï¼Œå¹¶è§£æ HTML å—çº§å…ƒç´ """
        BLOCK_TAGS = ['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'blockquote', 'pre']

        for item_id, linear in self.book.spine:
            item = self.book.get_item_with_id(item_id)

            if not item: continue
            if item.get_type() != ebooklib.ITEM_DOCUMENT: continue

            try:
                # 1. è§£æ HTML
                raw_content = item.get_content()
                soup = BeautifulSoup(raw_content, 'html.parser')

                # è·å–æ–‡ä»¶åä½œä¸º Key
                unit_key = item.get_name()

                # 2. æ‰¾åˆ° Body
                root = soup.find('body') or soup

                # 3. éå†æ‰€æœ‰å—çº§å…ƒç´ 
                for tag in root.find_all(BLOCK_TAGS):
                    # 4. æå–çº¯æ–‡æœ¬
                    text = tag.get_text(separator=' ', strip=True)

                    # 5. è¿‡æ»¤æ‰ç©ºæ ‡ç­¾
                    if not text:
                        continue

                    # 6. Yield å•ä¸ªæ®µè½
                    yield unit_key, text, "text"

            except Exception as e:
                logger.error(f"Failed to parse HTML structure for {item_id}: {e}")
                continue

    def _flatten_epub_to_standard(self, toc, level=1):
        """è§£æ EPUB ç›®å½•ç»“æ„"""
        items = []
        for node in toc:
            # å…¼å®¹ ebooklib çš„ä¸¤ç§èŠ‚ç‚¹æ ¼å¼
            entry = node[0] if isinstance(node, (list, tuple)) else node
            children = node[1] if isinstance(node, (list, tuple)) and len(node) > 1 else []

            if hasattr(entry, 'href') and entry.href:
                items.append({
                    'level': level,
                    'title': entry.title or "Untitled",
                    'key': entry.href.split('#')[0]
                })

            if children:
                items.extend(self._flatten_epub_to_standard(children, level + 1))
        return items
