#!/usr/bin/env python3
"""
ä¸»ç¨‹åºå…¥å£ç‚¹ã€‚

è¯¥è„šæœ¬è´Ÿè´£ï¼š
1. åˆå§‹åŒ–é…ç½®å’Œæ—¥å¿—ã€‚
2. è·å–ç”¨æˆ·è¾“å…¥ï¼ˆç¿»è¯‘æ¨¡å¼ã€æ–‡æ¡£å¤„ç†ç­–ç•¥ï¼‰ã€‚
3. è°ƒç”¨æ–‡æ¡£å¤„ç†æµæ°´çº¿ï¼ˆpipelineï¼‰ç”Ÿæˆç»“æ„åŒ–æ–‡æœ¬ã€‚
4. å¯åŠ¨ç¿»è¯‘å¾ªç¯ï¼Œå¤„ç†æ–‡æœ¬å¹¶ä¿å­˜ç»“æœã€‚
5. ç»Ÿä¸€çš„é”™è¯¯å¤„ç†å’Œç¨‹åºé€€å‡ºé€»è¾‘ã€‚
"""
import os, time, json
import sys
import traceback
from pathlib import Path
from tqdm import tqdm
from dataclasses import asdict
from typing import List, Dict, Any

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from src.config import Settings, modes
from src.errors import TranslationError
from src.logging_config import setup_logging
from src.ui import get_mode_selection, get_user_strategy
from src.file_io import get_last_checkpoint_id, create_output_directory, recover_context_from_file, is_likely_chinese
from src.translator import GEMINITranslator
from src.pipeline import ContentSegment, compile_structure, MarkdownRenderer


# åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
logger = setup_logging()

def main():
    """ä¸»å‡½æ•°ï¼Œåè°ƒæ•´ä¸ªç¿»è¯‘æµç¨‹ã€‚"""
    try:
        logger.info("=" * 60)
        logger.info("ğŸ“š æ–‡æ¡£ç¿»è¯‘ç³»ç»Ÿå¯åŠ¨")
        logger.info("=" * 60)
        
        # --- 1. åŠ è½½é…ç½® ---
        # Settings() ä¼šè‡ªåŠ¨ä» .env æ–‡ä»¶å’Œç¯å¢ƒå˜é‡ä¸­åŠ è½½é…ç½®
        settings = Settings()
        logger.info(f"ğŸ“„ æ–‡æ¡£è·¯å¾„: {settings.document_path}")
        logger.info(f"ğŸ­ é»˜è®¤ç¿»è¯‘æ¨¡å¼ID: {settings.translation_mode}")
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {settings.output_base_dir}")
        
        # --- 2. è·å–ç”¨æˆ·é€‰æ‹© ---
        selected_mode = get_mode_selection(modes)
        user_strategy = get_user_strategy(str(settings.document_path), settings)

        # --- 3. ç»„åˆæœ€ç»ˆé…ç½® ---
        project_config = {
            **selected_mode.model_dump(),  # ä½¿ç”¨ Pydantic V2 çš„ model_dump()
            **user_strategy
        }
        
        # --- 4. ç»Ÿä¸€å¤„ç†æµç¨‹ ---
        process_document_flow(settings, project_config)
        
        logger.info("=" * 60)
        logger.info("ğŸ‰ ç¿»è¯‘ä»»åŠ¡æˆåŠŸå®Œæˆï¼")
        logger.info("=" * 60)
        
    except TranslationError as e:
        logger.error(f"âŒ ç¿»è¯‘æµç¨‹å‡ºç°å·²çŸ¥é”™è¯¯: {e}", exc_info=True)
        logger.error(f"ğŸ’¡ å»ºè®®: {e.suggestion}" if e.suggestion else "è¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯è¯¦æƒ…ã€‚")
        sys.exit(1)
    except Exception as e:
        logger.critical(f"ğŸ’¥ å‘ç”Ÿæœªé¢„æœŸçš„ä¸¥é‡é”™è¯¯: {e}", exc_info=True)
        logger.critical(traceback.format_exc())
        sys.exit(1)
    finally:
        logger.info("ç³»ç»Ÿå…³é—­ã€‚")

def process_document_flow(settings: Settings, project_config: dict):
    """
    åè°ƒæ–‡æ¡£ä»è§£æåˆ°ç¿»è¯‘çš„æ•´ä¸ªæµç¨‹ã€‚
    é€‚é… ContentSegment å¯¹è±¡æ¶æ„ã€‚
    """
    file_path = str(settings.document_path)
    logger.info(f"ğŸš€ å¼€å§‹å¤„ç†æ–‡æ¡£: {os.path.basename(file_path)}")
    logger.info(f"   - ç¿»è¯‘æ¨¡å¼: {project_config['name']}")
    
    # --- å‡†å¤‡å·¥ä½œåŒº ---
    project_dir = create_output_directory(
        file_path, 
        project_config['name'],
        settings.output_base_dir
    )
    cache_path = os.path.join(project_dir, "structure_map.json")
    final_md_path = os.path.join(project_dir, "Full_Book.md")
    
    # --- ç¼–è¯‘æ–‡æ¡£ç»“æ„ (å¦‚æœç¼“å­˜ä¸å­˜åœ¨) ---
    all_segments: list[ContentSegment] = [] # ç±»å‹æç¤ºæ›´æ–°
    
    if settings.enable_cache and os.path.exists(cache_path):
        logger.info("ğŸ“¦ å‘ç°ç»“æ„ç¼“å­˜ï¼Œæ­£åœ¨åŠ è½½...")
        try:
            with open(cache_path, "r", encoding="utf-8") as f:
                raw_data = json.load(f)
                # ã€å…³é”®ä¿®æ”¹ã€‘: å°†å­—å…¸åˆ—è¡¨è½¬æ¢å› ContentSegment å¯¹è±¡åˆ—è¡¨
                all_segments = [ContentSegment(**item) for item in raw_data]
            logger.info(f"   âœ… æˆåŠŸåŠ è½½ {len(all_segments)} ä¸ªæ–‡æœ¬ç‰‡æ®µã€‚")
        except (json.JSONDecodeError, IOError, TypeError) as e:
            logger.warning(f"   âš ï¸ ç¼“å­˜æ–‡ä»¶æŸåæˆ–æ ¼å¼ä¸åŒ¹é…: {e}ã€‚å°†é‡æ–°ç¼–è¯‘æ–‡æ¡£ã€‚")
            all_segments = []

    if not all_segments:
        logger.info("âš™ï¸ æœªæ‰¾åˆ°ç¼“å­˜æˆ–ç¼“å­˜å·²ç¦ç”¨ï¼Œå¼€å§‹ç¼–è¯‘æ–‡æ¡£ç»“æ„...")
        # compile_structure ç°åœ¨ç›´æ¥è¿”å› List[ContentSegment]
        all_segments = compile_structure(
            file_path=file_path,
            cache_path=cache_path,
            settings=settings,
            project_config=project_config
        )
    
    if not all_segments:
        raise TranslationError("æ–‡æ¡£ç¼–è¯‘åæœªç”Ÿæˆä»»ä½•æ–‡æœ¬ç‰‡æ®µï¼Œæ— æ³•ç»§ç»­ã€‚")
    
    # --- åˆå§‹åŒ–è¾“å‡ºæ–‡ä»¶ ---
    if not os.path.exists(final_md_path):
        logger.info(f"ğŸ“ åˆ›å»ºæ–°çš„è¾“å‡ºæ–‡ä»¶: {final_md_path}")
        with open(final_md_path, "w", encoding="utf-8") as f:
            f.write(f"# åŸæ–‡: {os.path.basename(file_path)}\n")
            f.write(f"> ä½¿ç”¨ **{project_config['name']}** æ¨¡å¼ç¿»è¯‘\n\n---\n\n")
    
    # --- å¯åŠ¨ç¿»è¯‘å¾ªç¯ ---
    translator = GEMINITranslator(settings)
    pre_translate_chapter_titles(all_segments, translator, project_config)

    try:
        logger.info("ğŸ’¾ æ­£åœ¨æ›´æ–°ç»“æ„ç¼“å­˜ï¼ˆä¿å­˜å·²ç¿»è¯‘çš„ç« èŠ‚æ ‡é¢˜ï¼‰...")
        # å°†å¯¹è±¡åˆ—è¡¨è½¬å›å­—å…¸åˆ—è¡¨
        data_to_save = [asdict(seg) for seg in all_segments]
        with open(cache_path, "w", encoding="utf-8") as f:
            json.dump(data_to_save, f, ensure_ascii=False, indent=2)
        logger.info("âœ… ç¼“å­˜æ›´æ–°æˆåŠŸã€‚")
    except Exception as e:
        logger.warning(f"âš ï¸ æ— æ³•æ›´æ–°ç¼“å­˜ï¼Œä½†ä¸å½±å“åç»­æµç¨‹: {e}")

    run_translation_loop(all_segments, 
        final_md_path, 
        translator,
        project_config)

def run_translation_loop(
    all_segments: list[ContentSegment], # ç±»å‹æç¤ºæ›´æ–°
    output_file: str,
    translator: GEMINITranslator,
    project_config: dict
):
    """
    æ‰§è¡Œç¿»è¯‘ä¸»å¾ªç¯ã€‚
    é€‚é… ContentSegment å¯¹è±¡å±æ€§è®¿é—®å’Œæ–°çš„ MarkdownRendererã€‚
    """
    # --- 0. å®ä¾‹åŒ–æ¸²æŸ“å™¨ ---
    renderer = MarkdownRenderer(translator.settings)

    # --- 1. æ–­ç‚¹ç»­ä¼  --- 
    last_id = get_last_checkpoint_id(output_file)
    
    # ã€å…³é”®ä¿®æ”¹ã€‘: ä½¿ç”¨ .segment_id è®¿é—®å±æ€§
    segments_to_do = [s for s in all_segments if s.segment_id > last_id]
    
    if not segments_to_do:
        logger.info("ğŸ‰ æ‰€æœ‰ç‰‡æ®µå‡å·²ç¿»è¯‘å®Œæˆï¼")
        return

    logger.info(f"ğŸ”„ ä»ç‰‡æ®µ ID {last_id + 1} ç»§ç»­ï¼Œå‰©ä½™ {len(segments_to_do)} ä¸ªç‰‡æ®µå¾…å¤„ç†ã€‚")
    
    # --- 2. æ¢å¤ä¸Šä¸‹æ–‡ ---
    context_length = translator.settings.max_context_length
    context_buffer = recover_context_from_file(output_file, context_length)

    # --- 3. åˆ†æ‰¹å¤„ç† ---
    batch_size = translator.settings.batch_size
    progress_bar = tqdm(range(0, len(segments_to_do), batch_size), desc="Translating Batches")
    
    for i in progress_bar:
        batch = segments_to_do[i : i + batch_size]
        
        # ã€å…³é”®ä¿®æ”¹ã€‘: ä½¿ç”¨ .segment_id
        progress_bar.set_postfix({
            "Batch": f"{i // batch_size + 1}/{len(progress_bar)}",
            "IDs": f"{batch[0].segment_id}-{batch[-1].segment_id}"
        })
        
        try:
            # --- è°ƒç”¨ç¿»è¯‘ ---
            # è¿™é‡Œçš„ translate_batch å†…éƒ¨éœ€è¦é€‚é…ï¼šå®ƒä¼šæ¥æ”¶ List[ContentSegment]
            # å¦‚æœä½ çš„ translator è¿˜æ²¡æ”¹ï¼Œå¯èƒ½éœ€è¦åœ¨è¿™é‡Œæå– batch_texts = [s.original_text for s in batch]
            translations = translator.translate_batch(batch, project_config, context=context_buffer)
            
            # --- å¥å£®æ€§æ£€æŸ¥ ---
            if len(translations) != len(batch):
                logger.error(f"      âŒ æ‰¹æ¬¡ {i // batch_size + 1} æ•°é‡ä¸åŒ¹é… (Req: {len(batch)}, Res: {len(translations)})")
                continue

            # --- å®æ—¶å†™å…¥ ---
            with open(output_file, "a", encoding="utf-8") as f:
                for idx, trans_text in enumerate(translations):
                    seg = batch[idx]
                    
                    # ã€å…³é”®ä¿®æ”¹ã€‘: å°†ç¿»è¯‘ç»“æœå¡«å…¥å¯¹è±¡
                    seg.translated_text = trans_text
                    
                    # ã€å…³é”®ä¿®æ”¹ã€‘: è°ƒç”¨æ–°çš„æ¸²æŸ“å™¨ç±»
                    # æ³¨æ„ï¼šMetadata (Chapter/Page) å·²ç»åœ¨ seg å¯¹è±¡é‡Œäº†ï¼Œæ¸²æŸ“å™¨ä¼šè‡ªåŠ¨å¤„ç†
                    markdown_chunk = renderer.render_segment(seg)
                    f.write(markdown_chunk)
                f.flush()
            
            # --- æ›´æ–°ä¸Šä¸‹æ–‡ ---
            if translations:
                full_translation_text = " ".join(t.replace('\n', ' ') for t in translations)
                context_buffer = full_translation_text[-context_length:]

        except Exception as e: # æ•è·æ›´å®½æ³›çš„å¼‚å¸¸ä»¥é˜²å¯¹è±¡å±æ€§é”™è¯¯
            logger.error(f"      âŒ æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}", exc_info=True)
            continue
        
        # --- é€Ÿç‡æ§åˆ¶ ---
        time.sleep(translator.settings.rate_limit_delay)

def pre_translate_chapter_titles(all_segments: List[ContentSegment], 
        translator, 
        project_config):
    
    """
    [é¢„å¤„ç†] æå–æ‰€æœ‰ç« èŠ‚æ ‡é¢˜ï¼Œæ‰¹é‡ç¿»è¯‘ï¼Œå¹¶æ›´æ–° Segment å¯¹è±¡ã€‚
    ä¼˜åŒ–ï¼šåªå¤„ç†çœŸæ­£çš„ç« èŠ‚å¼€å¤´ (is_new_chapter=True)ã€‚
    """
    logger.info("--- å¼€å§‹ç« èŠ‚æ ‡é¢˜é¢„ç¿»è¯‘ ---")
    
    # 1. æå–æ ‡é¢˜ (ä»…é’ˆå¯¹ç« èŠ‚èµ·å§‹ç‚¹)
    # ã€ä¼˜åŒ–ç‚¹ã€‘å¢åŠ  if seg.is_new_chapter åˆ¤æ–­
    # raw_titles = [
    #     seg.chapter_title 
    #     for seg in all_segments 
    #     if seg.is_new_chapter and seg.chapter_title and seg.chapter_title.strip()
    # ]
    raw_titles = []
    for seg in all_segments:
        if seg.is_new_chapter and seg.chapter_title and seg.chapter_title.strip():
            # ã€ç®€å•æ£€æµ‹ã€‘å¦‚æœæ ‡é¢˜é‡ŒåŒ…å«ä¸­æ–‡å­—ç¬¦ï¼Œå¤§æ¦‚ç‡æ˜¯å·²ç»ç¿»è¯‘è¿‡äº†ï¼Œè·³è¿‡
            # æˆ–è€…ä½ å¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€æ±‚ï¼Œå†³å®šæ˜¯å¦è¦é‡æ–°ç¿»è¯‘
            if is_likely_chinese(seg.chapter_title):
                continue
            raw_titles.append(seg.chapter_title)
    
    # 2. æœ‰åºå»é‡
    unique_titles = list(dict.fromkeys(raw_titles))
    
    if not unique_titles:
        logger.info("No new chapter headers found to translate.")
        return

    logger.info(f"Found {len(unique_titles)} unique headers. Translating...")

    # 3. æ‰¹é‡ç¿»è¯‘
    translation_map = translator.translate_plain_text_list(unique_titles, project_config)
    
    # 4. å›å¡«ç»“æœ
    update_count = 0
    for seg in all_segments:
        # ã€ä¼˜åŒ–ç‚¹ã€‘åªä¿®æ”¹ä½œä¸ºæ–°ç« èŠ‚å¼€å¤´çš„é‚£ä¸ª segment
        if seg.is_new_chapter and seg.chapter_title in translation_map:
            translated = translation_map[seg.chapter_title]
            if translated:
                seg.chapter_title = translated
                update_count += 1
    
    logger.info(f"Updated {update_count} chapter headers.")

if __name__ == "__main__":
    main()