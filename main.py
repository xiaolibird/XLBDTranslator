#!/usr/bin/env python3
"""
XLBD ç¿»è¯‘å™¨ä¸»å…¥å£
åŸºäºçŠ¶æ€é©±åŠ¨çš„ç°ä»£åŒ–æ¶æ„
"""
import os
import sys
import json
import time
from pathlib import Path
from typing import List, Dict, Any

from src.core.schema import Settings, ContentSegment, SegmentList, DocumentConfig
from src.core.exceptions import TranslationError, APIError, APITimeoutError, JSONParseError
from src.utils.logger import setup_logging, logger
from src.utils.file import create_output_directory, get_file_hash
from src.ui import get_mode_selection, get_user_strategy
from src.parsers.manager import compile_structure
from src.parsers.tools import is_likely_chinese
from src.translator.client import GeminiTranslator
from src.renderer.markdown import MarkdownRenderer
from src.config import modes

# å…¨å±€è®¾ç½®å’Œæ—¥å¿—åˆå§‹åŒ–ï¼ˆåœ¨ main å‡½æ•°ä¸­å®Œæˆï¼‰


def process_document_flow(settings: Settings, translation_mode_config: dict):
    """
    çŠ¶æ€é©±åŠ¨çš„æ–‡æ¡£å¤„ç†æµç¨‹

    æ ¸å¿ƒæ¶æ„ï¼š
    - Source of Truth: å†…å­˜ä¸­çš„ List[ContentSegment]
    - Load: åŠ è½½ structure_map.json æˆ–è§£ææ–‡æ¡£
    - Gap Analysis: æ‰¾å‡ºæœªç¿»è¯‘ç‰‡æ®µ
    - Translation Loop: æ‰¹é‡ç¿»è¯‘ + å®æ—¶ä¿å­˜
    - Render: ç”Ÿæˆæœ€ç»ˆæ–‡æ¡£
    """
    file_path = settings.files.document_path
    logger.info(f"ğŸš€ å¼€å§‹å¤„ç†æ–‡æ¡£: {file_path.name}")
    logger.info(f"   - ç¿»è¯‘æ¨¡å¼: {translation_mode_config['name']}")
    
    # åŸºäºæ–‡ä»¶å†…å®¹çš„ MD5 å“ˆå¸Œåˆ›å»ºå”¯ä¸€çš„é¡¹ç›®æ ‡è¯†
    file_hash = get_file_hash(file_path)
    project_name = file_hash
    logger.info(f"   - é¡¹ç›®æ ‡è¯† (Hash): {project_name}")

    # å‡†å¤‡å·¥ä½œç›®å½•å’Œè·¯å¾„
    project_dir = create_output_directory(
        project_name, settings.files.output_base_dir
    )
    structure_path = project_dir / "structure_map.json"

    # 1. Load: åŠ è½½æ–‡æ¡£ç»“æ„åˆ°å†…å­˜
    all_segments = load_document_structure(file_path, structure_path, settings)
    if not all_segments:
        raise TranslationError("æ–‡æ¡£è§£æå¤±è´¥ï¼Œæœªç”Ÿæˆä»»ä½•å†…å®¹ç‰‡æ®µ")
    
    # 2. é¢„ç¿»è¯‘ç« èŠ‚æ ‡é¢˜
    translator = GeminiTranslator(settings)
    pre_translate_titles(all_segments, translator, translation_mode_config)

    # ä¿å­˜æ ‡é¢˜ç¿»è¯‘åçš„ç»“æ„
    save_structure_map(structure_path, all_segments)

    # 3. Gap Analysis: æ‰¾å‡ºå¾…ç¿»è¯‘ç‰‡æ®µ
    pending_segments = find_untranslated_segments(all_segments)

    if not pending_segments:
        logger.info("ğŸ‰ æ‰€æœ‰ç‰‡æ®µå‡å·²ç¿»è¯‘å®Œæˆï¼")
        # ç”Ÿæˆæœ€ç»ˆæ–‡æ¡£
        render_final_document(all_segments, file_path.name, settings)
        return

    logger.info(f"ğŸ”„ å‘ç° {len(pending_segments)} ä¸ªå¾…ç¿»è¯‘ç‰‡æ®µ")
    
    # 4. Translation Loop: çŠ¶æ€é©±åŠ¨ç¿»è¯‘
    run_state_driven_translation_loop(
        pending_segments, all_segments, structure_path, translator, translation_mode_config, settings
    )

    # 5. Render: ç”Ÿæˆæœ€ç»ˆæ–‡æ¡£
    render_final_document(all_segments, file_path.name, settings)


def load_document_structure(file_path: Path, structure_path: Path, settings: Settings) -> SegmentList:
    """
    Load é˜¶æ®µï¼šåŠ è½½æ–‡æ¡£ç»“æ„åˆ°å†…å­˜

    ä¼˜å…ˆçº§ï¼š
    1. ä» structure_map.json åŠ è½½ï¼ˆåŒ…å«ç¿»è¯‘çŠ¶æ€ï¼‰
    2. è§£æåŸå§‹æ–‡æ¡£ç”Ÿæˆæ–°ç»“æ„
    """
    # 1. å°è¯•ä» structure_map.json åŠ è½½
    if structure_path.exists() and settings.processing.enable_cache:
        try:
            with open(structure_path, 'r', encoding='utf-8') as f:
                raw_data = json.load(f)
                segments = [ContentSegment(**item) for item in raw_data]
                logger.info(f"ğŸ“¦ ä»ç»“æ„æ–‡ä»¶åŠ è½½ {len(segments)} ä¸ªç‰‡æ®µ")
                return segments
        except Exception as e:
            logger.warning(f"âš ï¸ structure_map.json æŸåï¼Œå°†é‡æ–°è§£æ: {e}")
            
    # 2. é‡æ–°è§£ææ–‡æ¡£
    logger.info("âš™ï¸ è§£ææ–‡æ¡£ç»“æ„...")
    segments = compile_structure(file_path, structure_path, settings)

    if segments:
        logger.info(f"âœ… è§£æå®Œæˆï¼Œç”Ÿæˆ {len(segments)} ä¸ªç‰‡æ®µ")
        save_structure_map(structure_path, segments)
    else:
        logger.error("âŒ æ–‡æ¡£è§£æå¤±è´¥")

    return segments or []


def pre_translate_titles(segments, translator: GeminiTranslator, translation_mode_config: dict):
    """é¢„ç¿»è¯‘ç« èŠ‚æ ‡é¢˜"""
    logger.info("ğŸ“ é¢„ç¿»è¯‘ç« èŠ‚æ ‡é¢˜...")
    
    # æå–å¾…ç¿»è¯‘æ ‡é¢˜
    raw_titles = []
    for seg in segments:
        if (seg.is_new_chapter and seg.chapter_title and
            seg.chapter_title.strip() and not is_likely_chinese(seg.chapter_title)):
            raw_titles.append(seg.chapter_title)
    
    if not raw_titles:
        logger.info("   - æ— éœ€ç¿»è¯‘çš„æ ‡é¢˜")
        return

    # å»é‡
    unique_titles = list(dict.fromkeys(raw_titles))
    logger.info(f"   - å‘ç° {len(unique_titles)} ä¸ªå”¯ä¸€æ ‡é¢˜")

    # æ‰¹é‡ç¿»è¯‘
    translation_map = translator.translate_titles(unique_titles, translation_mode_config)
    
    # å›å¡«ç»“æœ
    update_count = 0
    for seg in segments:
        if seg.is_new_chapter and seg.chapter_title in translation_map:
            translated = translation_map[seg.chapter_title]
            if translated:
                seg.chapter_title = translated
                update_count += 1
    
    logger.info(f"   - æ›´æ–°äº† {update_count} ä¸ªæ ‡é¢˜")


def find_untranslated_segments(all_segments: SegmentList) -> SegmentList:
    """
    Gap Analysis: æ‰¾å‡ºæ‰€æœ‰æœªç¿»è¯‘çš„ç‰‡æ®µ

    åŸºäºå†…å­˜çŠ¶æ€åˆ†æï¼Œä¸ä¾èµ–æ–‡ä»¶æ£€æŸ¥
    """
    untranslated = [seg for seg in all_segments if not seg.is_translated]
    logger.info(f"ğŸ” åˆ†æç»“æœ: {len(untranslated)}/{len(all_segments)} ç‰‡æ®µå¾…ç¿»è¯‘")
    return untranslated


def run_state_driven_translation_loop(
    pending_segments: SegmentList,
    all_segments: SegmentList,
    structure_path: Path,
    translator: GeminiTranslator,
    translation_mode_config: dict,
    settings: Settings
):
    """
    Translation Loop: çŠ¶æ€é©±åŠ¨ç¿»è¯‘ä¸»å¾ªç¯

    æ ¸å¿ƒç‰¹å¾ï¼š
    - åªéå†å¾…ç¿»è¯‘åˆ—è¡¨
    - é€šè¿‡å†…å­˜ç´¢å¼•è·å–ä¸Šä¸‹æ–‡ï¼ˆä¸è¯»æ–‡ä»¶ï¼‰
    - æ¯æ‰¹ç¿»è¯‘åç«‹å³ä¿å­˜å®Œæ•´çŠ¶æ€
    - ä¸å®æ—¶å†™å…¥ Markdownï¼ˆç­‰å…¨éƒ¨å®Œæˆåæ¸²æŸ“ï¼‰
    """
    from tqdm import tqdm

    batch_size = settings.processing.batch_size
    total_batches = (len(pending_segments) + batch_size - 1) // batch_size

    logger.info(f"ğŸ”„ å¼€å§‹ç¿»è¯‘å¾ªç¯: {total_batches} æ‰¹æ¬¡ï¼Œæ‰¹å¤§å° {batch_size}")

    progress_bar = tqdm(
        range(0, len(pending_segments), batch_size),
        desc="ç¿»è¯‘è¿›åº¦",
        unit="æ‰¹"
    )

    for batch_start in progress_bar:
        batch_end = min(batch_start + batch_size, len(pending_segments))
        current_batch = pending_segments[batch_start:batch_end]

        # æ›´æ–°è¿›åº¦æ¡
        batch_num = batch_start // batch_size + 1
        progress_bar.set_postfix({
            "æ‰¹æ¬¡": f"{batch_num}/{total_batches}",
            "ç‰‡æ®µ": f"{current_batch[0].segment_id}-{current_batch[-1].segment_id}",
            "è¿›åº¦": f"{batch_end}/{len(pending_segments)}"
        })

        try:
            # è·å–ä¸Šä¸‹æ–‡ï¼šç›´æ¥ä»å†…å­˜ä¸­è·å–å‰æ–‡ç¿»è¯‘
            context_text = get_context_from_memory(current_batch[0], all_segments, settings.processing.max_context_length)

            # æ‰§è¡Œç¿»è¯‘
            translations = translator.translate_batch(current_batch, translation_mode_config, context_text)

            # éªŒè¯ç¿»è¯‘ç»“æœ
            if len(translations) != len(current_batch):
                logger.error(f"âŒ æ‰¹æ¬¡ç¿»è¯‘ç»“æœæ•°é‡ä¸åŒ¹é…: æœŸæœ› {len(current_batch)}, å¾—åˆ° {len(translations)}")
                continue

            # æ›´æ–°å†…å­˜ä¸­çš„ç‰‡æ®µçŠ¶æ€
            for seg, trans_text in zip(current_batch, translations):
                seg.translated_text = trans_text

            # Save: ç«‹å³ä¿å­˜å®Œæ•´çŠ¶æ€åˆ° structure_map.json
            save_structure_map(structure_path, all_segments)

            logger.debug(f"âœ… æ‰¹æ¬¡ {batch_num} å®Œæˆï¼Œå·²ä¿å­˜çŠ¶æ€")

        except (APIError, APITimeoutError) as e:
            logger.error(f"âŒ æ‰¹æ¬¡ {batch_num} å‘ç”Ÿ API é”™è¯¯: {e}")
            logger.info("   å°†ç»§ç»­ä¸‹ä¸€ä¸ªæ‰¹æ¬¡ã€‚")
            continue
        except JSONParseError as e:
            logger.error(f"âŒ æ‰¹æ¬¡ {batch_num} å‘ç”Ÿ JSON è§£æé”™è¯¯: {e}")
            logger.info("   å°†ç»§ç»­ä¸‹ä¸€ä¸ªæ‰¹æ¬¡ã€‚")
            continue
        except Exception as e:
            logger.error(f"âŒ æ‰¹æ¬¡ {batch_num} å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            # ç»§ç»­ä¸‹ä¸€æ‰¹ï¼Œä¸ä¸­æ–­æ•´ä¸ªæµç¨‹
            continue

        # é€Ÿç‡æ§åˆ¶
        time.sleep(settings.processing.rate_limit_delay)


def get_context_from_memory(current_segment: ContentSegment, all_segments: SegmentList, max_length: int) -> str:
    """
    ä»å†…å­˜ä¸­è·å–ç¿»è¯‘ä¸Šä¸‹æ–‡

    é€šè¿‡ segment_id åœ¨ all_segments ä¸­æŸ¥æ‰¾å‰æ–‡å·²ç¿»è¯‘ç‰‡æ®µ
    """
    # æ‰¾åˆ°å½“å‰ç‰‡æ®µçš„ä½ç½®
    current_idx = next((i for i, seg in enumerate(all_segments) if seg.segment_id == current_segment.segment_id), -1)
    if current_idx == -1:
        return ""

    # è·å–å‰å‡ ä¸ªå·²ç¿»è¯‘çš„ç‰‡æ®µå†…å®¹
    context_parts = []
    context_length = 0

    # å‘å‰æŸ¥æ‰¾å·²ç¿»è¯‘çš„ç‰‡æ®µ
    for i in range(current_idx - 1, -1, -1):
        seg = all_segments[i]
        if seg.is_translated and seg.translated_text:
            # ä¼°ç®—é•¿åº¦ï¼ˆä¸­æ–‡å­—ç¬¦æŒ‰2å­—èŠ‚ç®—ï¼‰
            text_length = len(seg.translated_text.encode('utf-8'))
            if context_length + text_length > max_length:
                break

            context_parts.insert(0, seg.translated_text)  # ä¿æŒé¡ºåº
            context_length += text_length

    return " ".join(context_parts).strip()


def save_structure_map(structure_path: Path, segments: SegmentList):
    """
    Save: ä¿å­˜å®Œæ•´çš„æ–‡æ¡£ç»“æ„çŠ¶æ€åˆ° JSON æ–‡ä»¶

    è¿™æ˜¯å•ä¸€çœŸç†æºçš„æŒä¹…åŒ–
    """
    try:
        structure_path.parent.mkdir(parents=True, exist_ok=True)

        # åºåˆ—åŒ–ä¸ºå­—å…¸åˆ—è¡¨
        data = [seg.model_dump() for seg in segments]

        with open(structure_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        logger.debug(f"ğŸ’¾ ç»“æ„çŠ¶æ€å·²ä¿å­˜: {len(segments)} ä¸ªç‰‡æ®µ")
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜ç»“æ„çŠ¶æ€å¤±è´¥: {e}")
        raise


def render_final_document(segments: SegmentList, doc_name: str, settings: Settings):
    """Render: ç”Ÿæˆæœ€ç»ˆæ–‡æ¡£ï¼ˆMarkdown + PDFï¼‰"""
    logger.info("ğŸ“„ ç”Ÿæˆæœ€ç»ˆæ–‡æ¡£...")

    # å†³å®šæœ€ç»ˆè¾“å‡ºç›®å½•
    if settings.files.final_output_dir:
        final_dir = settings.files.final_output_dir
        final_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"   - è‡ªå®šä¹‰è¾“å‡ºç›®å½•: {final_dir}")
    else:
        # é»˜è®¤è¾“å‡ºåˆ°æºæ–‡ä»¶æ‰€åœ¨ç›®å½•
        final_dir = settings.files.document_path.parent
        logger.info(f"   - è¾“å‡ºåˆ°æºæ–‡ä»¶ç›®å½•: {final_dir}")

    # 1. ç”Ÿæˆ Markdown
    md_renderer = MarkdownRenderer(settings)
    md_output_path = final_dir / f"{Path(doc_name).stem}_Translated.md"
    md_renderer.render_to_file(segments, md_output_path, f"åŸæ–‡: {doc_name}")
    logger.info(f"âœ… Markdown å·²ä¿å­˜åˆ°: {md_output_path}")

    # 2. ç”Ÿæˆ PDFï¼ˆå¯é€‰ï¼Œå¦‚æœä¾èµ–å¯ç”¨ï¼‰
    try:
        from src.renderer.pdf import PDFRenderer
        pdf_renderer = PDFRenderer(settings)

        pdf_path = final_dir / f"{Path(doc_name).stem}_Translated.pdf"
        pdf_renderer.render_to_file(segments, pdf_path, f"åŸæ–‡: {doc_name}")
        logger.info(f"âœ… PDF å·²ä¿å­˜åˆ°: {pdf_path}")

    except OSError as e:
        if "cannot load library" in str(e) or "cannot open shared object file" in str(e) or "no library called" in str(e).lower():
            logger.error("âŒ PDF ç”Ÿæˆå¤±è´¥ï¼šç¼ºå°‘ WeasyPrint è¿è¡Œæ‰€éœ€çš„ç³»ç»Ÿä¾èµ–åº“ã€‚")
            logger.info("   - Windows: è¯·è®¿é—® https://doc.courtbouillon.org/weasyprint/stable/first_steps.html#windows å®‰è£… GTK3ã€‚")
            logger.info("   - macOS: è¯·è¿è¡Œ `brew install pango cairo gdk-pixbuf libffi`ã€‚")
            logger.info("   - Debian/Ubuntu: è¯·è¿è¡Œ `sudo apt-get install libpango-1.0-0 libcairo2 libpangoft2-1.0-0 libgdk-pixbuf2.0-0 libffi-dev`ã€‚")
            logger.warning("âš ï¸ PDF ç”Ÿæˆå·²è·³è¿‡ï¼Œä½† Markdown æ–‡ä»¶å·²æˆåŠŸç”Ÿæˆã€‚")
        else:
            logger.warning(f"âš ï¸ PDF ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿæ–‡ä»¶é”™è¯¯: {e}")
            logger.info("ğŸ“„ Markdown æ–‡ä»¶å·²æˆåŠŸç”Ÿæˆ")
    except Exception as e:
        logger.warning(f"âš ï¸ PDF ç”Ÿæˆè·³è¿‡: {e}")
        logger.info("ğŸ“„ Markdown æ–‡ä»¶å·²æˆåŠŸç”Ÿæˆ")


def main():
    """ä¸»å‡½æ•°ï¼Œåè°ƒæ•´ä¸ªç¿»è¯‘æµç¨‹"""
    try:
        # åˆå§‹åŒ–è®¾ç½®å’Œæ—¥å¿—
        settings = Settings.from_env_file()
        setup_logging(settings)

        logger.info("=" * 60)
        logger.info("ğŸ“š XLBD æ–‡æ¡£ç¿»è¯‘ç³»ç»Ÿå¯åŠ¨")
        logger.info("=" * 60)

        # --- 1. åŠ è½½é…ç½® ---
        logger.info(f"ğŸ“„ æ–‡æ¡£è·¯å¾„: {settings.files.document_path}")
        logger.info(f"ğŸ­ é»˜è®¤ç¿»è¯‘æ¨¡å¼ID: {settings.processing.translation_mode}")
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {settings.files.output_base_dir}")

        # --- 2. è·å–ç”¨æˆ·é€‰æ‹© ---
        # æ£€æŸ¥æ˜¯å¦åœ¨äº¤äº’ç¯å¢ƒä¸­
        is_interactive = os.isatty(0)  # æ£€æŸ¥ stdin æ˜¯å¦è¿æ¥åˆ°ç»ˆç«¯

        if is_interactive:
            selected_mode = get_mode_selection(modes)
            get_user_strategy(settings)
        else:
            # éäº¤äº’æ¨¡å¼ï¼šä½¿ç”¨é»˜è®¤å€¼
            logger.info("ğŸ”„ éäº¤äº’æ¨¡å¼ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            selected_mode = modes.get(settings.processing.translation_mode, modes["1"])
            logger.info(f"âœ… ä½¿ç”¨ç¿»è¯‘æ¨¡å¼: {selected_mode.name}")

        # --- 3. ç»„åˆæœ€ç»ˆé…ç½® ---
        # ä½¿ç”¨æ¥è‡ª UI çš„ç­–ç•¥æ›´æ–° settings.documentï¼Œä½¿å…¶æˆä¸ºæ–‡æ¡£å¤„ç†çš„å•ä¸€äº‹å®æ¥æº
        # è¿™æ ·ï¼Œæ‰€æœ‰ä¸‹æ¸¸å‡½æ•°éƒ½å¯ä»¥é€šè¿‡ settings å¯¹è±¡è®¿é—®åˆ°æœ€ç»ˆçš„ã€æœ‰æ•ˆçš„é…ç½®

        
        # ç°åœ¨åªåŒ…å«ç¿»è¯‘æ¨¡å¼ç›¸å…³ä¿¡æ¯
        translation_mode_config = selected_mode.model_dump()

        # --- 4. ç»Ÿä¸€å¤„ç†æµç¨‹ ---
        # ä¼ å…¥æ›´æ–°åçš„ settings å¯¹è±¡
        process_document_flow(settings, translation_mode_config)
        logger.info("=" * 60)
        logger.info("ğŸ‰ ç¿»è¯‘ä»»åŠ¡æˆåŠŸå®Œæˆï¼")
        logger.info("=" * 60)

    except TranslationError as e:
        logger.critical(f"ğŸ’¥ ç¿»è¯‘é”™è¯¯: {e}", exc_info=True)
        sys.exit(1)
    except APIError as e:
        logger.critical(f"ğŸ’¥ API é”™è¯¯: {e}", exc_info=True)
        sys.exit(1)
    except APITimeoutError as e:
        logger.critical(f"ğŸ’¥ API è¶…æ—¶: {e}", exc_info=True)
        sys.exit(1)
    except JSONParseError as e:
        logger.critical(f"ğŸ’¥ JSON è§£æé”™è¯¯: {e}", exc_info=True)
        sys.exit(1)
    except Exception as e:
        logger.critical(f"ğŸ’¥ å‘ç”Ÿæœªé¢„æœŸçš„ä¸¥é‡é”™è¯¯: {e}", exc_info=True)
        import traceback
        logger.critical(traceback.format_exc())
        sys.exit(1)
    finally:
        logger.info("ç³»ç»Ÿå…³é—­ã€‚")


if __name__ == "__main__":
    main()