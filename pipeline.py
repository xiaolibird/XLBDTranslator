import os
import json
import re
from abc import ABC, abstractmethod
import csv

# ç¬¬ä¸‰æ–¹åº“
import fitz  # PyMuPDF
import spacy
from ebooklib import epub

# æœ¬åœ°å·¥å…· 
import utils

# ==============================================================================
# 0. å…¨å±€èµ„æºåˆå§‹åŒ–
# ==============================================================================

print("âš™ï¸ Loading NLP Model (Spacy)...")
try:
    nlp = spacy.load("en_core_web_sm")
except:
    print("âš ï¸ æœªæ£€æµ‹åˆ° Spacy æ¨¡å‹ï¼Œè¯·è¿è¡Œ: python -m spacy download en_core_web_sm")
    nlp = None

# ==============================================================================
# 1. åŸºç±»ï¼šå®šä¹‰æµæ°´çº¿éª¨æ¶
# ==============================================================================

class BaseDocPipeline(ABC):
    def __init__(self, file_path, cache_path):
        self.file_path = file_path
        self.cache_path = cache_path
        
        # å†…éƒ¨çŠ¶æ€
        self.all_segments = []
        self.global_id_counter = 0
        self.rolling_buffer = ""
        self.BATCH_SIZE = 1200  # ç¼“å†²åŒºé˜ˆå€¼
        
        # ç« èŠ‚æ˜ å°„è¡¨
        self.chapter_map = {} 

    def run(self):
        """ä¸»æµç¨‹ (Template Method)"""
        print(f"âš™ï¸ [{self.__class__.__name__}] Start: {os.path.basename(self.file_path)}")
        
        # 1. åŠ è½½å…ƒæ•°æ®
        self._load_metadata()
        
        # 2. åˆå§‹åŒ–å®¹å™¨
        self.all_segments = []
        self.rolling_buffer = ""  # ç¡®ä¿è¿™é‡Œåˆå§‹åŒ–äº†
        
        # 3. å¼€å§‹éå†
        for unit_key, text in self._iter_content_units():
            if not text or not text.strip(): continue
            
            # =========================================================
            # ğŸŸ¢ ã€ä¿®å¤æ ¸å¿ƒã€‘Vision Mode ç›´é€šé€»è¾‘
            # =========================================================
            if text.strip().startswith("<<IMAGE_PATH"):
                print(f"   ğŸ“¸ [Page {unit_key}] Image Token detected.")

                # A. å…³é”®ä¿®æ­£ï¼šæ£€æŸ¥ self.rolling_buffer è€Œä¸æ˜¯ current_buffer
                # å¦‚æœç¼“å†²åŒºé‡Œæœ‰ä¹‹å‰æ”’ä¸‹çš„æ™®é€šæ–‡å­—ï¼Œå…ˆè®©å®ƒä»¬â€œè½è¢‹ä¸ºå®‰â€
                if self.rolling_buffer:
                    print(f"      ğŸ’¨ Flushing text buffer before image...")
                    self._flush_buffer() 
                
                # B. å°†å›¾ç‰‡ Token ç›´æ¥å­˜ä¸ºä¸€ä¸ªç‹¬ç«‹çš„ Segment
                # ä¿æŒ ID çš„è¿ç»­æ€§ (1-based)
                self.all_segments.append({
                    "id": len(self.all_segments) + 1,
                    "text": text.strip()
                })
                
                # C. ç›´æ¥è·³è¿‡æœ¬æ¬¡å¾ªç¯ï¼Œä¸èµ°ä¸‹é¢çš„æ–‡æœ¬å¤„ç†é€»è¾‘
                continue
            # =========================================================

            # --- ä¸‹é¢æ˜¯ Text æ¨¡å¼çš„å¸¸è§„é€»è¾‘ ---

            # è¯­ä¹‰æ³¨å…¥
            chapter_title = self.chapter_map.get(unit_key)
            is_new_chapter = (chapter_title is not None)
            
            # å¼ºåˆ¶åˆ·æ–°ç¼“å†²åŒº (ç« èŠ‚éš”ç¦»)
            # è¿™é‡Œçš„ self.rolling_buffer ä¹Ÿè¦å¯¹åº”ä¿®æ”¹
            if is_new_chapter and self.rolling_buffer:
                print(f"   âœ‚ï¸ Boundary detected: [{chapter_title}]. Flushing buffer...")
                self._flush_buffer()

            # æ³¨å…¥ç»“æ„æ ‡è®°
            if is_new_chapter:
                # ä¿®å¤ï¼šæ ‡é¢˜å‰ååŠ åŒæ¢è¡Œ
                marker = f"\n\n## [Chapter: {chapter_title}]\n\n"
                print(f"   ğŸ“ Locate: {chapter_title}")
            else:
                # å¼±æ ‡è®°ï¼šåªæ˜¯ Section (é¡µç )
                marker = f"\n\n## [Section: {unit_key}]\n\n"
                
            self.rolling_buffer += marker + text
            
            # å®¹é‡æ£€æŸ¥
            if len(self.rolling_buffer) >= self.BATCH_SIZE:
                self._flush_buffer()
        
        # 4. æ”¶å°¾ï¼šå¾ªç¯ç»“æŸï¼ŒæŠŠè‚šå­é‡Œå‰©ä¸‹çš„åå‡ºæ¥
        self._flush_buffer()
        
        # 5. ä¿å­˜
        self._save_cache()
        
        return self.all_segments

    def _flush_buffer(self):
        if not self.rolling_buffer: return
        
        new_segs = self._semantic_chunking(self.rolling_buffer, max_chars=1200)
        
        for seg in new_segs:
            seg['id'] = self.global_id_counter
            self.all_segments.append(seg)
            self.global_id_counter += 1
            
        self.rolling_buffer = ""

    def _save_cache(self):
        print(f"   ğŸ’¾ Freezing structure to: {os.path.basename(self.cache_path)}")
        with open(self.cache_path, "w", encoding="utf-8") as f:
            json.dump(self.all_segments, f, ensure_ascii=False, indent=2)

    @staticmethod
    def _semantic_chunking(text, max_chars=1500, method="paragraph_aware"):
        """[NLP Tool] é€šç”¨å­—ç¬¦ä¸²åˆ‡åˆ†é€»è¾‘"""
        
        # =========== ğŸŸ¢ æ ¸å¿ƒä¿®å¤åŒºåŸŸ ===========
        # 1. ä¿®å¤å­—é¢è½¬ä¹‰ç¬¦ï¼šæŠŠ "\n" (ä¸¤ä¸ªå­—ç¬¦) å˜æˆçœŸæ­£çš„æ¢è¡Œ
        text = text.replace('\\n', '\n')
        # 2. ç»Ÿä¸€ Windows/Unix æ¢è¡Œ
        text = text.replace('\r\n', '\n')
        # 3. ã€æ–°é€»è¾‘ã€‘ä½¿ç”¨æ­£åˆ™ "æŒ¤å‹" æ¢è¡Œç¬¦
        # å«ä¹‰ï¼šæ— è®ºåŸæœ¬æ˜¯ 1 ä¸ªã€2 ä¸ªè¿˜æ˜¯ 10 ä¸ªæ¢è¡Œç¬¦ï¼Œç»Ÿç»Ÿå˜æˆæ ‡å‡†çš„ Markdown åˆ†æ®µ (\n\n)
        text = re.sub(r'\n+', '\n\n', text)
        # =====================================

        if not nlp:
             return [{"text": text}]

        if method == "paragraph_aware":
            paragraphs = re.split(r'\n\s*\n+', text)
            paragraphs = [p.strip() for p in paragraphs if p.strip()]
            
            segments = []
            current_chunk = []
            current_len = 0
            
            for para in paragraphs:
                para_len = len(para)
                
                # æƒ…å†µ A: è¶…é•¿æ®µè½ï¼ŒæŒ‰å¥å­åˆ‡
                if para_len > max_chars:
                    if current_chunk:
                        segments.append({"text": "\n\n".join(current_chunk)})
                        current_chunk = []
                        current_len = 0
                    
                    doc = nlp(para[:100000]) 
                    sents = [s.text.strip() for s in doc.sents if s.text.strip()]
                    
                    temp_chunk = []
                    temp_len = 0
                    for sent in sents:
                        sl = len(sent)
                        if temp_len + sl > max_chars and temp_chunk:
                            segments.append({"text": " ".join(temp_chunk)})
                            temp_chunk = [sent]
                            temp_len = sl
                        else:
                            temp_chunk.append(sent)
                            temp_len += sl
                    if temp_chunk:
                        segments.append({"text": " ".join(temp_chunk)})
                
                # æƒ…å†µ B: æ™®é€šæ®µè½ï¼Œåˆå¹¶
                else:
                    if current_len + para_len + 2 > max_chars and current_chunk:
                        segments.append({"text": "\n\n".join(current_chunk)})
                        current_chunk = [para]
                        current_len = para_len
                    else:
                        current_chunk.append(para)
                        current_len += para_len + 2
            
            if current_chunk:
                segments.append({"text": "\n\n".join(current_chunk)})
            
            return segments
        
        return [{"text": text}]

    @abstractmethod
    def _load_metadata(self):
        pass

    @abstractmethod
    def _iter_content_units(self):
        pass


# ==============================================================================
# 2. EPUB å®ç°ç±»
# ==============================================================================

class EPUBPipeline(BaseDocPipeline):
    def _load_metadata(self):
        print("   ğŸ“– Parsing EPUB TOC...")
        self.book = epub.read_epub(self.file_path)
        self.chapter_map = utils.flatten_toc(self.book.toc)
        print(f"   ğŸ—ºï¸  Mapped {len(self.chapter_map)} TOC entries.")

    def _iter_content_units(self):
            for item_id, linear in self.book.spine:
                item = self.book.get_item_with_id(item_id)
                if item:
                    # ğŸŸ¢ æ­£ç¡®ä»£ç ï¼šè°ƒç”¨ HTML æ¸…æ´—å‡½æ•°
                    raw_text = utils.extract_text_from_epub_item(item)
                    file_name = item.get_name()
                    yield file_name, raw_text


# ==============================================================================
# 3. PDF å®ç°ç±» 
# ==============================================================================

class PDFPipeline(BaseDocPipeline):
    """
    åŸºäº PyMuPDF çš„åŸç”Ÿ PDF æå–å™¨ã€‚
    ç»§æ‰¿è‡ª BaseDocPipelineã€‚
    ä¿ç•™äº†é«˜çº§å¸ƒå±€åˆ†æåŠŸèƒ½ï¼šè¾¹è·åˆ‡é™¤ã€æ®µè½é—´è·åˆ¤å®šã€è¿å­—ç¬¦ä¿®å¤ã€‚
    """
    
    def __init__(self, file_path, cache_path, extra_config=None):
        super().__init__(file_path, cache_path)
        self.doc = None
        # æ¥æ”¶å¤–éƒ¨é…ç½® (main.py ä¼ è¿›æ¥çš„ PDF_CONFIG)
        self.config = extra_config or {}
        self.custom_toc_path = extra_config.get("custom_toc_path") # ä¿å­˜è‡ªå®šä¹‰ç« èŠ‚ä¿¡æ¯è·¯å¾„
        """        
        Page,Title,Level
        1,Title1,1
        5,Title2,1
        5,Subtitle2-1,2
        20,Title3,1
        """
    
    def _load_metadata(self):
        """[å¿…é¡»å®ç°] æ‰“å¼€ PDF å¹¶è§£æç›®å½•"""
        print(f"   ğŸ“• Opening PDF: {os.path.basename(self.file_path)}")
        self.doc = fitz.open(self.file_path)
        
        # ğŸŸ¢ 2. ä¼˜å…ˆè¯»å– CSV TOC
        if self.custom_toc_path and os.path.exists(self.custom_toc_path):
            print(f"   ğŸ“¥ Loading Custom TOC from: {self.custom_toc_path}")
            try:
                with open(self.custom_toc_path, 'r', encoding='utf-8') as f:
                    reader = csv.DictReader(f)
                    for row in reader:
                        # CSV Page 1 -> PDF Index 0
                        p_idx = int(row['Page']) - 1 
                        title = row['Title']
                        # level = row['Level'] # æš‚æ—¶æ²¡ç”¨åˆ°å±‚çº§ï¼Œåç»­å¯æ‰©å±•
                        self.chapter_map[p_idx] = title
                print(f"   âœ… Overloaded {len(self.chapter_map)} chapters from CSV.")
                return # æˆåŠŸè¯»å–åï¼Œç›´æ¥è¿”å›ï¼Œä¸å†è¯» PDF å†…ç½®ç›®å½•
            except Exception as e:
                print(f"   âš ï¸ Failed to load CSV TOC: {e}")

        # å°è¯•æå–ä¹¦ç­¾ä½œä¸ºç« èŠ‚æ˜ å°„
        try:
            toc = self.doc.get_toc(simple=False)
            for item in toc:
                lvl, title, page_num = item[0], item[1], item[2]
                if page_num > 0:
                    # PDF é¡µç æ˜¯ä» 1 å¼€å§‹çš„ï¼Œè½¬æ¢ä¸º 0-based ç´¢å¼•
                    self.chapter_map[page_num - 1] = title
            print(f"   ğŸ—ºï¸  Mapped {len(self.chapter_map)} bookmarks from PDF TOC.")
        except Exception as e:
            print(f"   âš ï¸  TOC extraction failed: {e}")

    def _iter_content_units(self):
        """[å¿…é¡»å®ç°] æŒ‰é¡µé¢éå†å†…å®¹ (å¸¦è°ƒè¯•æ—¥å¿—)"""
        
        # 1. å¤„ç†é¡µé¢èŒƒå›´
        start_page = 0
        end_page = len(self.doc)
        
        if "page_range" in self.config and self.config["page_range"]:
            r_start, r_end = self.config["page_range"]
            start_page = max(0, r_start)
            end_page = min(len(self.doc), r_end)
            print(f"   ğŸ“„ [Scope] Processing specific range: {start_page} to {end_page}")
        else:
            print(f"   ğŸ“„ [Scope] Processing ALL pages: 0 to {end_page}")

        # 2. å¾ªç¯éå†
        for i in range(start_page, end_page):
            page = self.doc[i]
            
            # è°ƒç”¨æ ¸å¿ƒæå–é€»è¾‘
            # ğŸ’¡ è¿™é‡Œçš„ page_text å¯èƒ½æ˜¯æ–‡æœ¬ï¼Œä¹Ÿå¯èƒ½æ˜¯ "<<IMAGE_PATH...>>"
            page_text = self._extract_text_from_page(page, page_idx=i)
            
            # ğŸ›¡ï¸ [é˜²å‘†ä¿®æ­£] é˜²æ­¢è¿”å› None å¯¼è‡´åç»­æŠ¥é”™
            if page_text is None:
                page_text = ""
                
            # ğŸ•µï¸â€â™‚ï¸ [è°ƒè¯•æ—¥å¿—] è¿™ä¸€æ­¥èƒ½æ•‘å‘½ï¼
            # è®©ä½ åœ¨ç»ˆç«¯ç›´æ¥çœ‹åˆ°æ¯ä¸€é¡µåˆ°åº•æå–åˆ°äº†ä»€ä¹ˆ
            content_preview = page_text[:50].replace('\n', '\\n')
            if not page_text.strip():
                print(f"      âš ï¸ [Page {i}] Extracted EMPTY content! (Check Config/OCR)")
            elif "<<IMAGE_PATH" in page_text:
                print(f"      ğŸ“¸ [Page {i}] Vision Token: {content_preview}...")
            else:
                print(f"      ğŸ“ [Page {i}] Text Length: {len(page_text)} chars")

            # Yield å‡ºå»
            yield i, page_text

    def _extract_text_from_page(self, page, page_idx=0):
        """
        [æ ¸å¿ƒé€»è¾‘ç§»æ¤] å•é¡µå¸ƒå±€åˆ†æä¸æ–‡æœ¬æå–ã€‚
        """

        if self.config.get("use_vision_mode", False):
            # ç›´æ¥è·³å»å¤„ç†å›¾ç‰‡ï¼Œä¸å†å¾€ä¸‹æ‰§è¡Œæ–‡æœ¬æå–
            return self._extract_image_for_vision(page, page_idx)   

        h = page.rect.height
        
        # 1. è·å–é…ç½® (ä¼˜å…ˆç»å¯¹å€¼ï¼Œæ²¡æœ‰åˆ™ç”¨ç™¾åˆ†æ¯”)
        cfg_top = self.config.get("margin_top", None)
        cfg_bottom = self.config.get("margin_bottom", None)
        cfg_top_pct = self.config.get("margin_top_pct", 0.08)
        cfg_bottom_pct = self.config.get("margin_bottom_pct", 0.08)
        min_gap = self.config.get("min_gap", 8.0) # æ®µè½é—´è·é˜ˆå€¼

        # è®¡ç®—åˆ‡é™¤é˜ˆå€¼
        m_top = cfg_top if (cfg_top is not None and cfg_top > 1) else h * cfg_top_pct
        m_bottom = cfg_bottom if (cfg_bottom is not None and cfg_bottom > 1) else h * cfg_bottom_pct
        
        # (å¯é€‰) æ‰“å°ç¬¬ä¸€é¡µçš„å‚æ•°ä¾›è°ƒè¯•
        if page_idx == 0:
            print(f"   ğŸ“ Layout Config: H={h:.1f} | Top Cut={m_top:.1f} | Bottom Cut={m_bottom:.1f}")

        # 2. è·å–æ‰€æœ‰æ–‡æœ¬å—
        blocks = page.get_text("dict").get("blocks", [])
        
        page_paragraphs = []     # å­˜æ”¾æœ¬é¡µæå–å‡ºçš„æ‰€æœ‰å®Œæ•´æ®µè½
        current_para_lines = []  # æ­£åœ¨æ‹¼æ¥çš„å½“å‰æ®µè½è¡Œ
        prev_block_bottom = None # ä¸Šä¸€ä¸ªå—çš„åº•è¾¹ä½ç½®
        
        for block in blocks:
            # è¿‡æ»¤éæ–‡æœ¬å— (å¦‚å›¾ç‰‡)
            if "lines" not in block: continue
            
            bbox = block.get("bbox", [0,0,0,0]) # [x0, y0, x1, y1]
            
            # A. æ ¸å¿ƒè¿‡æ»¤ï¼šåˆ‡é™¤é¡µçœ‰é¡µè„š
            if bbox[3] < m_top or bbox[1] > (h - m_bottom): 
                continue
            
            # B. æå–æ–‡æœ¬å†…å®¹
            block_text = ""
            for line in block["lines"]:
                for span in line.get("spans", []):
                    block_text += span.get("text", "") + " "
            block_text = block_text.strip()
            
            # C. è¿‡æ»¤å™ªç‚¹ (é¡µç ã€å¤ªçŸ­çš„ä¹±ç )
            if not block_text or (block_text.isdigit() and len(block_text) < 5) or len(block_text) < 3:
                continue
            
            # D. æ®µè½åˆ¤å®š (åŸºäºå‚ç›´é—´è· min_gap)
            block_top = bbox[1]
            is_new_para = False
            
            if prev_block_bottom is not None:
                # å¦‚æœå½“å‰å—çš„é¡¶éƒ¨ - ä¸Šä¸€ä¸ªå—çš„åº•éƒ¨ > é˜ˆå€¼ï¼Œè®¤ä¸ºæ˜¯æ–°æ®µè½
                if (block_top - prev_block_bottom) > min_gap:
                    is_new_para = True
            
            # å¦‚æœæ˜¯æ–°æ®µè½ï¼Œå…ˆç»“ç®—ä¸Šä¸€æ®µ
            if is_new_para and current_para_lines:
                full_para = " ".join(current_para_lines).strip()
                # E. è¿å­—ä¿®å¤ (Hyphenation Repair)
                full_para = self._repair_hyphenation(full_para)
                if full_para: page_paragraphs.append(full_para)
                current_para_lines = []
            
            # åŠ å…¥å½“å‰ç´¯ç§¯
            current_para_lines.append(block_text)
            prev_block_bottom = bbox[3]
            
        # å¤„ç†æœ¬é¡µæœ€åå‰©ä¸‹çš„æ®µè½ç¼“å­˜
        if current_para_lines:
            full_para = " ".join(current_para_lines).strip()
            full_para = self._repair_hyphenation(full_para)
            if full_para: page_paragraphs.append(full_para)
            
        # è¿”å›æœ¬é¡µæ–‡æœ¬ï¼Œæ®µè½ä¹‹é—´ç”¨åŒæ¢è¡Œéš”å¼€
        return "\n\n".join(page_paragraphs)

    def _extract_image_for_vision(self, page, page_idx):
        """
        ğŸ“¸ [Vision Mode Core]
        å°†å½“å‰ PDF é¡µé¢æ¸²æŸ“ä¸ºé«˜åˆ†è¾¨ç‡å›¾ç‰‡ï¼Œä¿å­˜åˆ°ç¼“å­˜ç›®å½•ï¼Œå¹¶è¿”å›è·¯å¾„æš—å·ã€‚
        """
        try:
            # 1. æ¸²æŸ“å›¾ç‰‡ (DPI=200 æ˜¯æ€§ä»·æ¯”ä¹‹é€‰ï¼ŒGemini çœ‹å¾—æ¸…ä¸”ä½“ç§¯ä¸è¿‡å¤§)
            # matrix=fitz.Matrix(2, 2) ç­‰æ•ˆäº zoom=2
            pix = page.get_pixmap(dpi=200)
            
            # 2. å‡†å¤‡å­˜æ”¾å›¾ç‰‡çš„æ–‡ä»¶å¤¹
            # å­˜æ”¾åœ¨è·Ÿ structure_map.json åŒä¸€çº§çš„ "page_images" æ–‡ä»¶å¤¹é‡Œ
            base_dir = os.path.dirname(self.cache_path)
            img_dir = os.path.join(base_dir, "page_images")
            os.makedirs(img_dir, exist_ok=True)
            
            # 3. æ„é€ æ–‡ä»¶å (æŒ‰é¡µç æ’åºï¼Œæ–¹ä¾¿æŸ¥æ‰¾)
            img_filename = f"page_{page_idx:04d}.jpg"
            img_path = os.path.join(img_dir, img_filename)
            
            # 4. ä¿å­˜åˆ°ç¡¬ç›˜
            pix.save(img_path)
            print(f"      ğŸ“¸ Vision Capture: Page {page_idx} saved.")
            
            # 5. ã€å…³é”®ã€‘è¿”å›â€œæš—å·â€
            # è¿™ä¸ªæ ¼å¼å¿…é¡»å’Œ translator.py é‡Œè¯†åˆ«çš„æ ¼å¼å®Œå…¨ä¸€è‡´ï¼
            return f"<<IMAGE_PATH::{img_path}>>"
            
        except Exception as e:
            print(f"      âŒ Vision Capture Failed on Page {page_idx}: {e}")
            return "" # è¿”å›ç©ºå­—ç¬¦ä¸²ï¼Œè·³è¿‡æ­¤é¡µ

    @staticmethod
    def _repair_hyphenation(text):
        """è¾…åŠ©ï¼šä¿®å¤è·¨è¡Œè¿å­—ç¬¦ (ex- ample -> example)"""
        # åŒ¹é…é€»è¾‘ï¼šå•è¯ + è¿å­—ç¬¦ + ç©ºæ ¼/æ¢è¡Œ + å°å†™å­—æ¯
        return re.sub(r'-\s*\n?\s*([a-z])', r'\1', text)


# ==============================================================================
# 4. å·¥å‚å…¥å£
# ==============================================================================

def compile_structure(file_path, cache_path, project_config):
    """
    æ™ºèƒ½å·¥å‚ï¼šå…¨è‡ªåŠ¨å†³ç­–ä¸­å¿ƒã€‚
    å†³å®šæ˜¯ç”¨ PDFPipeline è¿˜æ˜¯ EPUBPipelineï¼Œæ˜¯ Vision æ¨¡å¼è¿˜æ˜¯ Native æ¨¡å¼ã€‚
    """
    
    ext = os.path.splitext(file_path)[1].lower()
    
    # å‡†å¤‡é…ç½®å®¹å™¨ (å¦‚æœç”¨æˆ·æ²¡ä¼ ï¼Œå°±æ–°å»ºä¸€ä¸ªç©ºå­—å…¸)
    # final_config = project_config.copy() if project_config else {}
    if project_config is None: project_config = {}
    final_config = project_config 
    pipeline = None

    if ext == '.epub':
        pipeline = EPUBPipeline(file_path, cache_path)

    elif ext == '.pdf':
        # =========== ğŸ•µï¸â€â™€ï¸ ä¾¦æ¢é€»è¾‘ (è‡ªåŠ¨å†³ç­–) ===========
        
        # 1. å†³ç­–ï¼šæ˜¯å¦å¼€å¯ Vision æ¨¡å¼ï¼Ÿ
        # åªæœ‰å½“ç”¨æˆ·æ²¡æœ‰æ˜¾å¼æŒ‡å®šæ—¶ï¼Œæˆ‘ä»¬æ‰å»è‡ªåŠ¨æ£€æµ‹
        if "use_vision_mode" not in final_config:
            print("   ğŸ” Auto-detecting PDF type...")
            try:
                # è°ƒç”¨ utils è¿›è¡Œè¯Šæ–­
                pdf_type = utils.detect_pdf_type(file_path)
                
                if pdf_type == "image_only":
                    print("   âš ï¸ Diagnosis: Image-only/Scanned PDF. ğŸŸ¢ Switching to VISION mode.")
                    final_config["use_vision_mode"] = True
                else:
                    print("   âœ… Diagnosis: Native Text PDF. ğŸ”µ Using TEXT extraction.")
                    final_config["use_vision_mode"] = False
            except Exception as e:
                print(f"   âš ï¸ Detection failed ({e}). Defaulting to Text mode.")
                final_config["use_vision_mode"] = False

        # 2. å†³ç­–ï¼šå¦‚æœæ˜¯ Native æ¨¡å¼ï¼Œéœ€è¦åˆ‡è¾¹è·å—ï¼Ÿ
        # æ— è®ºæ˜¯å¦æ˜¯ Vision æ¨¡å¼ï¼Œå¦‚æœé…ç½®ä¸­æ²¡æœ‰è¾¹è·ï¼Œéƒ½å°è¯•è‡ªåŠ¨æ‰«æ
        if final_config.get("margin_top") is None:
            print("   ğŸ” Auto-scanning layout margins...")
            try:
                margins = utils.analyze_pdf_margins_by_scan(file_path)
                final_config.update({
                    "margin_top": margins.get("suggested_margin_top", 0),
                    "margin_bottom": margins.get("suggested_margin_bottom", 0)
                })
                print(f"      âœ… Margins set: Top={final_config['margin_top']} / Bottom={final_config['margin_bottom']}")
            except:
                print("      âš ï¸ Margin scan failed. Using defaults.")

        # =========== ğŸ­ å®ä¾‹åŒ– ===========
        pipeline = PDFPipeline(
            file_path, 
            cache_path, 
            extra_config=final_config
        )

    else:
        raise ValueError(f"âŒ Unsupported file format: {ext}")
        
    return pipeline.run()